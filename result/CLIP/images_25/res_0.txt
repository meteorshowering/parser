{"type": "text", "bbox": [1228, 1756, 2169, 2827], "res": [{"text": "A related idea to CLIP is webly supervised learning. This", "confidence": 0.9904075264930725, "text_region": [[1224.0, 1759.0], [2172.0, 1759.0], [2172.0, 1805.0], [1224.0, 1805.0]]}, {"text": "line of work queries image search engines to build image", "confidence": 0.9793672561645508, "text_region": [[1221.0, 1802.0], [2169.0, 1802.0], [2169.0, 1848.0], [1221.0, 1848.0]]}, {"text": "datasets by querying for terms and uses the queries as the", "confidence": 0.9826430082321167, "text_region": [[1224.0, 1855.0], [2169.0, 1855.0], [2169.0, 1901.0], [1224.0, 1901.0]]}, {"text": "labels for the returned images (Fergus et al., 2005). Classi-", "confidence": 0.9947260022163391, "text_region": [[1224.0, 1901.0], [2175.0, 1901.0], [2175.0, 1947.0], [1224.0, 1947.0]]}, {"text": "fiers trained on these large but noisily labeled datasets can", "confidence": 0.9927846193313599, "text_region": [[1224.0, 1950.0], [2175.0, 1950.0], [2175.0, 1996.0], [1224.0, 1996.0]]}, {"text": " be competitive with those trained on smaller carefully la-", "confidence": 0.972672700881958, "text_region": [[1221.0, 1993.0], [2169.0, 1993.0], [2169.0, 2039.0], [1221.0, 2039.0]]}, {"text": "beled datasets. These image-query pairs are also often used", "confidence": 0.9736149311065674, "text_region": [[1221.0, 2036.0], [2175.0, 2039.0], [2175.0, 2096.0], [1221.0, 2092.0]]}, {"text": " to improve performance on standard datasets as additional", "confidence": 0.9865145683288574, "text_region": [[1217.0, 2089.0], [2175.0, 2086.0], [2175.0, 2142.0], [1217.0, 2145.0]]}, {"text": " training data (Chen & Gupta, 2015). CLIP also uses search", "confidence": 0.9893574714660645, "text_region": [[1217.0, 2135.0], [2175.0, 2132.0], [2175.0, 2188.0], [1217.0, 2191.0]]}, {"text": "queries as part of its dataset creation process. However", "confidence": 0.9800649285316467, "text_region": [[1221.0, 2181.0], [2172.0, 2185.0], [2172.0, 2241.0], [1221.0, 2237.0]]}, {"text": " CLIP only uses full text sequences co-occuring with images", "confidence": 0.9816258549690247, "text_region": [[1221.0, 2227.0], [2172.0, 2231.0], [2172.0, 2287.0], [1221.0, 2284.0]]}, {"text": " as supervision rather than just the queries, which are often", "confidence": 0.9874644875526428, "text_region": [[1221.0, 2284.0], [2169.0, 2280.0], [2169.0, 2326.0], [1221.0, 2330.0]]}, {"text": " only a single word or short n-gram. We also restrict this step", "confidence": 0.9833313822746277, "text_region": [[1221.0, 2326.0], [2175.0, 2326.0], [2175.0, 2383.0], [1221.0, 2383.0]]}, {"text": "in CLIP to text only querying for sub-string matches while", "confidence": 0.9890638589859009, "text_region": [[1221.0, 2376.0], [2172.0, 2379.0], [2172.0, 2426.0], [1221.0, 2422.0]]}, {"text": "most webly supervised work uses standard image search", "confidence": 0.9988423585891724, "text_region": [[1227.0, 2429.0], [2169.0, 2429.0], [2169.0, 2475.0], [1227.0, 2475.0]]}, {"text": "engines which have their own complex retrieval and filter-", "confidence": 0.985684335231781, "text_region": [[1227.0, 2475.0], [2172.0, 2475.0], [2172.0, 2521.0], [1227.0, 2521.0]]}, {"text": "ing pipelines that often involve computer vision systems.", "confidence": 0.9943759441375732, "text_region": [[1224.0, 2524.0], [2172.0, 2524.0], [2172.0, 2571.0], [1224.0, 2571.0]]}, {"text": "Of this line of work, Learning Everything about Anything:", "confidence": 0.9938758611679077, "text_region": [[1227.0, 2571.0], [2172.0, 2571.0], [2172.0, 2617.0], [1227.0, 2617.0]]}, {"text": "Webly-Supervised Visual Concept Learning (Divvala et al.,", "confidence": 0.9991973042488098, "text_region": [[1224.0, 2620.0], [2175.0, 2620.0], [2175.0, 2666.0], [1224.0, 2666.0]]}, {"text": "2014) has a notably similar ambition and goal as CLIP.", "confidence": 0.9990226626396179, "text_region": [[1227.0, 2666.0], [2109.0, 2666.0], [2109.0, 2713.0], [1227.0, 2713.0]]}, {"text": " Finally, CLIP is related to a recent burst of activity on learn-", "confidence": 0.986332356929779, "text_region": [[1221.0, 2736.0], [2169.0, 2739.0], [2169.0, 2785.0], [1221.0, 2782.0]]}, {"text": "ing joint models of vision and language (Lu et al., 2019; Tan", "confidence": 0.9963193535804749, "text_region": [[1221.0, 2789.0], [2169.0, 2785.0], [2169.0, 2831.0], [1221.0, 2835.0]]}], "img_idx": 0, "score": 0.9935232996940613}
{"type": "text", "bbox": [217, 832, 1161, 2056], "res": [{"text": "CLIP is an example of using natural language as a training", "confidence": 0.990180492401123, "text_region": [[216.0, 822.0], [1161.0, 828.0], [1161.0, 875.0], [216.0, 868.0]]}, {"text": "signal for learning about a domain other than language. In", "confidence": 0.9789493083953857, "text_region": [[220.0, 874.0], [1164.0, 874.0], [1164.0, 921.0], [220.0, 921.0]]}, {"text": "this context, the earliest use of the term natural language", "confidence": 0.9983477592468262, "text_region": [[216.0, 917.0], [1164.0, 924.0], [1164.0, 970.0], [216.0, 964.0]]}, {"text": " supervision that we are aware of is the work of Ramanathan", "confidence": 0.987454354763031, "text_region": [[213.0, 964.0], [1164.0, 960.0], [1164.0, 1016.0], [213.0, 1020.0]]}, {"text": "et al. (2013) which showed that natural language descrip-", "confidence": 0.981967568397522, "text_region": [[216.0, 1016.0], [1164.0, 1016.0], [1164.0, 1063.0], [216.0, 1063.0]]}, {"text": " tions could be used along side other sources of supervision", "confidence": 0.9860675930976868, "text_region": [[210.0, 1056.0], [1164.0, 1059.0], [1164.0, 1115.0], [209.0, 1112.0]]}, {"text": "to improve performance on the task of video event under-", "confidence": 0.9966278076171875, "text_region": [[216.0, 1115.0], [1167.0, 1115.0], [1167.0, 1158.0], [216.0, 1158.0]]}, {"text": "standing. However, as mentioned in the introduction and", "confidence": 0.9873738884925842, "text_region": [[216.0, 1162.0], [1164.0, 1162.0], [1164.0, 1208.0], [216.0, 1208.0]]}, {"text": "approach section, methods of leveraging natural language", "confidence": 0.9800270795822144, "text_region": [[220.0, 1211.0], [1161.0, 1211.0], [1161.0, 1257.0], [220.0, 1257.0]]}, {"text": "descriptions in computer vision well predate the use of this", "confidence": 0.9970619082450867, "text_region": [[216.0, 1251.0], [1164.0, 1251.0], [1164.0, 1307.0], [216.0, 1307.0]]}, {"text": "specific term, especially for image retrieval (Mori et al.,", "confidence": 0.9994508028030396, "text_region": [[220.0, 1307.0], [1161.0, 1307.0], [1161.0, 1350.0], [220.0, 1350.0]]}, {"text": "1999) and object classification (Wang et al., 2009). Other", "confidence": 0.9876573085784912, "text_region": [[220.0, 1346.0], [1167.0, 1346.0], [1167.0, 1402.0], [220.0, 1402.0]]}, {"text": "early work leveraged tags (but not natural language) asso-", "confidence": 0.9955464005470276, "text_region": [[216.0, 1399.0], [1167.0, 1399.0], [1167.0, 1445.0], [216.0, 1445.0]]}, {"text": "ciated with images for the task of semantic segmentation", "confidence": 0.996139407157898, "text_region": [[220.0, 1449.0], [1164.0, 1449.0], [1164.0, 1495.0], [220.0, 1495.0]]}, {"text": "(Barnard et al., 2003). More recently, He & Peng (2017)", "confidence": 0.9791662693023682, "text_region": [[216.0, 1495.0], [1167.0, 1495.0], [1167.0, 1541.0], [216.0, 1541.0]]}, {"text": " and Liang et al. (2020) demonstrated using natural language ", "confidence": 0.9807377457618713, "text_region": [[210.0, 1534.0], [1168.0, 1538.0], [1167.0, 1594.0], [209.0, 1591.0]]}, {"text": "descriptions and explanations to improve fine-grained vi-", "confidence": 0.9977155327796936, "text_region": [[216.0, 1591.0], [1164.0, 1591.0], [1164.0, 1637.0], [216.0, 1637.0]]}, {"text": "sual classification of birds. Others have investigated how", "confidence": 0.99571692943573, "text_region": [[213.0, 1630.0], [1168.0, 1634.0], [1167.0, 1690.0], [213.0, 1686.0]]}, {"text": "grounded language can be used to improve visual represen-", "confidence": 0.9886359572410583, "text_region": [[220.0, 1690.0], [1164.0, 1690.0], [1164.0, 1736.0], [220.0, 1736.0]]}, {"text": "tations and classifiers on the Shape World dataset (Kuhnle", "confidence": 0.9921373128890991, "text_region": [[220.0, 1732.0], [1161.0, 1732.0], [1161.0, 1779.0], [220.0, 1779.0]]}, {"text": "& Copestake, 2017; Andreas et al., 2017; Mu et al., 2019).", "confidence": 0.9924377799034119, "text_region": [[220.0, 1782.0], [1164.0, 1782.0], [1164.0, 1828.0], [220.0, 1828.0]]}, {"text": " Finally, techniques which combine natural language with", "confidence": 0.9851034283638, "text_region": [[213.0, 1825.0], [1167.0, 1825.0], [1167.0, 1881.0], [213.0, 1881.0]]}, {"text": "reinforcement learning environments (Narasimhan et al.,", "confidence": 0.9990740418434143, "text_region": [[220.0, 1878.0], [1167.0, 1878.0], [1167.0, 1924.0], [220.0, 1924.0]]}, {"text": " 2015) have demonstrated exciting emergent behaviors such", "confidence": 0.9732918739318848, "text_region": [[213.0, 1917.0], [1168.0, 1921.0], [1167.0, 1977.0], [213.0, 1973.0]]}, {"text": "as systematically accomplishing zero-shot tasks (Hill et al.,", "confidence": 0.9892244935035706, "text_region": [[216.0, 1973.0], [1164.0, 1973.0], [1164.0, 2020.0], [216.0, 2020.0]]}, {"text": "2019).", "confidence": 0.9988552927970886, "text_region": [[220.0, 2020.0], [329.0, 2020.0], [329.0, 2066.0], [220.0, 2066.0]]}], "img_idx": 0, "score": 0.9928520917892456}
{"type": "text", "bbox": [221, 281, 1157, 794], "res": [{"text": "been explored in many creative and advanced ways. Dialog", "confidence": 0.9920259714126587, "text_region": [[220.0, 277.0], [1164.0, 277.0], [1164.0, 320.0], [220.0, 320.0]]}, {"text": "based learning (Weston, 2016; Li et al., 2016; Hancock et al.", "confidence": 0.9871081709861755, "text_region": [[220.0, 323.0], [1164.0, 323.0], [1164.0, 370.0], [220.0, 370.0]]}, {"text": "2019) develops techniques to learn from interactive natural", "confidence": 0.9992787837982178, "text_region": [[223.0, 373.0], [1164.0, 373.0], [1164.0, 419.0], [223.0, 419.0]]}, {"text": "language feedback in dialog. Several papers have leveraged", "confidence": 0.9932543039321899, "text_region": [[220.0, 422.0], [1161.0, 422.0], [1161.0, 469.0], [220.0, 469.0]]}, {"text": "semantic parsing to convert natural language explanations", "confidence": 0.9900174736976624, "text_region": [[220.0, 469.0], [1161.0, 469.0], [1161.0, 515.0], [220.0, 515.0]]}, {"text": "into features (Srivastava et al., 2017) or additional training", "confidence": 0.9915987849235535, "text_region": [[216.0, 511.0], [1161.0, 515.0], [1161.0, 561.0], [216.0, 558.0]]}, {"text": "labels (Hancock et al., 2018). More recently, ExpBERT", "confidence": 0.9978572130203247, "text_region": [[220.0, 561.0], [1161.0, 561.0], [1161.0, 607.0], [220.0, 607.0]]}, {"text": "(Murty et al., 2020) uses feature representations produced", "confidence": 0.9926533102989197, "text_region": [[216.0, 610.0], [1164.0, 610.0], [1164.0, 657.0], [216.0, 657.0]]}, {"text": "by conditioning a deep contextual language model on nat-", "confidence": 0.986542284488678, "text_region": [[220.0, 660.0], [1167.0, 660.0], [1167.0, 706.0], [220.0, 706.0]]}, {"text": "ural language explanations and descriptions of relations to", "confidence": 0.9874058961868286, "text_region": [[220.0, 706.0], [1161.0, 706.0], [1161.0, 752.0], [220.0, 752.0]]}, {"text": "improve performance on the task of relation extraction.", "confidence": 0.9848811626434326, "text_region": [[220.0, 756.0], [1101.0, 756.0], [1101.0, 802.0], [220.0, 802.0]]}], "img_idx": 0, "score": 0.99234539270401}
{"type": "text", "bbox": [1227, 282, 2165, 793], "res": [{"text": " large scale representation learning by training a system to", "confidence": 0.9866450428962708, "text_region": [[1217.0, 267.0], [2175.0, 271.0], [2175.0, 327.0], [1217.0, 323.0]]}, {"text": "pair descriptive text with videos instead of images. Several", "confidence": 0.9845383763313293, "text_region": [[1227.0, 323.0], [2172.0, 323.0], [2172.0, 370.0], [1227.0, 370.0]]}, {"text": "works have explored using dense spoken natural language", "confidence": 0.9982778429985046, "text_region": [[1224.0, 373.0], [2165.0, 373.0], [2165.0, 419.0], [1224.0, 419.0]]}, {"text": "supervision for videos (Miech et al., 2019; 2020b). When", "confidence": 0.992738664150238, "text_region": [[1221.0, 416.0], [2175.0, 412.0], [2175.0, 469.0], [1221.0, 472.0]]}, {"text": "considered together with CLIP, these works suggest that", "confidence": 0.9958943724632263, "text_region": [[1227.0, 469.0], [2172.0, 469.0], [2172.0, 515.0], [1227.0, 515.0]]}, {"text": "large scale natural language supervision is a promising way", "confidence": 0.9987426400184631, "text_region": [[1224.0, 515.0], [2169.0, 515.0], [2169.0, 561.0], [1224.0, 561.0]]}, {"text": "to learn high quality perceptual systems for many domains.", "confidence": 0.9873138666152954, "text_region": [[1227.0, 564.0], [2172.0, 564.0], [2172.0, 610.0], [1227.0, 610.0]]}, {"text": "Alayrac et al. (2020) extended this line of work to an addi-", "confidence": 0.9991200566291809, "text_region": [[1224.0, 607.0], [2172.0, 607.0], [2172.0, 653.0], [1224.0, 653.0]]}, {"text": "tional modality by adding raw audio as an additional super-", "confidence": 0.986266553401947, "text_region": [[1224.0, 660.0], [2175.0, 660.0], [2175.0, 706.0], [1224.0, 706.0]]}, {"text": "vision source and demonstrated benefits from combining all", "confidence": 0.9934637546539307, "text_region": [[1224.0, 706.0], [2172.0, 706.0], [2172.0, 752.0], [1224.0, 752.0]]}, {"text": "three sources of supervision.", "confidence": 0.9974026679992676, "text_region": [[1221.0, 746.0], [1687.0, 749.0], [1686.0, 805.0], [1221.0, 802.0]]}], "img_idx": 0, "score": 0.991755485534668}
{"type": "text", "bbox": [219, 2096, 1159, 2816], "res": [{"text": " CLIP's pre-training task optimizes for text-image retrieval.", "confidence": 0.9912246465682983, "text_region": [[213.0, 2089.0], [1168.0, 2092.0], [1167.0, 2138.0], [213.0, 2135.0]]}, {"text": "This areas of research dates back to the mid-90s with the", "confidence": 0.985927164554596, "text_region": [[220.0, 2142.0], [1164.0, 2142.0], [1164.0, 2185.0], [220.0, 2185.0]]}, {"text": "previously mentioned Mori et al. (1999) as representative of", "confidence": 0.9986802935600281, "text_region": [[220.0, 2188.0], [1164.0, 2188.0], [1164.0, 2234.0], [220.0, 2234.0]]}, {"text": " early work. While initial efforts focused primarily on predic-", "confidence": 0.9815804362297058, "text_region": [[213.0, 2227.0], [1168.0, 2231.0], [1167.0, 2287.0], [213.0, 2284.0]]}, {"text": "tive objectives over time research shifted towards learning", "confidence": 0.9852608442306519, "text_region": [[210.0, 2277.0], [1168.0, 2280.0], [1167.0, 2336.0], [209.0, 2333.0]]}, {"text": "joint multi-modal embedding spaces with techniques like ", "confidence": 0.988439679145813, "text_region": [[216.0, 2326.0], [1167.0, 2326.0], [1167.0, 2383.0], [216.0, 2383.0]]}, {"text": "kernel Canonical Correlation Analysis and various ranking", "confidence": 0.9980409145355225, "text_region": [[216.0, 2376.0], [1161.0, 2383.0], [1161.0, 2429.0], [216.0, 2422.0]]}, {"text": "objectives (Weston et al., 2010; Socher & Fei-Fei, 2010;", "confidence": 0.9849481582641602, "text_region": [[220.0, 2426.0], [1164.0, 2426.0], [1164.0, 2472.0], [220.0, 2472.0]]}, {"text": "Hodosh et al., 2013). Over time work explored many combi-", "confidence": 0.9944027662277222, "text_region": [[220.0, 2472.0], [1164.0, 2472.0], [1164.0, 2518.0], [220.0, 2518.0]]}, {"text": "nations of training objective, transfer, and more expressive", "confidence": 0.9916801452636719, "text_region": [[220.0, 2524.0], [1164.0, 2524.0], [1164.0, 2571.0], [220.0, 2571.0]]}, {"text": "models and steadily improved performance (Frome et al.,", "confidence": 0.9758416414260864, "text_region": [[220.0, 2571.0], [1164.0, 2571.0], [1164.0, 2617.0], [220.0, 2617.0]]}, {"text": "2013; Socher et al., 2014; Karpathy et al., 2014; Kiros et al.,", "confidence": 0.9935402274131775, "text_region": [[220.0, 2617.0], [1164.0, 2617.0], [1164.0, 2663.0], [220.0, 2663.0]]}, {"text": "2014; Faghri et al., 2017).", "confidence": 0.9844799637794495, "text_region": [[220.0, 2666.0], [642.0, 2666.0], [642.0, 2713.0], [220.0, 2713.0]]}, {"text": "Other work has leveraged natural language supervision for", "confidence": 0.9968639612197876, "text_region": [[220.0, 2739.0], [1164.0, 2739.0], [1164.0, 2785.0], [220.0, 2785.0]]}, {"text": "domains other than images. Stroud et al. (2020) explores", "confidence": 0.9991983771324158, "text_region": [[220.0, 2788.0], [1161.0, 2788.0], [1161.0, 2835.0], [220.0, 2835.0]]}], "img_idx": 0, "score": 0.9886510968208313}
{"type": "text", "bbox": [1227, 830, 2170, 1724], "res": [{"text": "As part of our work on CLIP we also construct a new dataset", "confidence": 0.9961374402046204, "text_region": [[1224.0, 825.0], [2172.0, 825.0], [2172.0, 871.0], [1224.0, 871.0]]}, {"text": " of image-text pairs. Modern work on image-text retrieval", "confidence": 0.9912797808647156, "text_region": [[1221.0, 871.0], [2172.0, 871.0], [2172.0, 917.0], [1221.0, 917.0]]}, {"text": "has relied on a set of crowd-sourced sentence level im-", "confidence": 0.996670663356781, "text_region": [[1224.0, 921.0], [2179.0, 921.0], [2179.0, 967.0], [1224.0, 967.0]]}, {"text": " age caption evaluation datasets like Pascal1K (Rashtchian", "confidence": 0.9777568578720093, "text_region": [[1221.0, 964.0], [2172.0, 960.0], [2172.0, 1016.0], [1221.0, 1020.0]]}, {"text": "et al., 2010), Flickr8K (Hodosh et al., 2013), and Flickr30K", "confidence": 0.980696976184845, "text_region": [[1227.0, 1016.0], [2172.0, 1016.0], [2172.0, 1059.0], [1227.0, 1059.0]]}, {"text": "(Young et al., 2014). However, these datasets are still rel-", "confidence": 0.9871001839637756, "text_region": [[1224.0, 1059.0], [2179.0, 1059.0], [2179.0, 1115.0], [1224.0, 1115.0]]}, {"text": "atively small and limit achievable performance. Several", "confidence": 0.993216872215271, "text_region": [[1224.0, 1112.0], [2175.0, 1112.0], [2175.0, 1158.0], [1224.0, 1158.0]]}, {"text": "methods have been proposed to create larger datasets au-", "confidence": 0.9994942545890808, "text_region": [[1224.0, 1158.0], [2175.0, 1158.0], [2175.0, 1204.0], [1224.0, 1204.0]]}, {"text": "tomatically with Ordonez et al. (2011) as a notable early", "confidence": 0.9973121285438538, "text_region": [[1224.0, 1208.0], [2169.0, 1208.0], [2169.0, 1254.0], [1224.0, 1254.0]]}, {"text": "example. In the deep learning era, Mithun et al. (2018)", "confidence": 0.9874796867370605, "text_region": [[1224.0, 1251.0], [2175.0, 1251.0], [2175.0, 1307.0], [1224.0, 1307.0]]}, {"text": "demonstrated an additional set of (image, text) pairs col-", "confidence": 0.995542585849762, "text_region": [[1227.0, 1304.0], [2175.0, 1304.0], [2175.0, 1350.0], [1227.0, 1350.0]]}, {"text": "lected from the internet could improve retrieval performance", "confidence": 0.999618649482727, "text_region": [[1224.0, 1353.0], [2172.0, 1353.0], [2172.0, 1399.0], [1224.0, 1399.0]]}, {"text": "and several new automatically constructed datasets such as", "confidence": 0.9979895949363708, "text_region": [[1224.0, 1399.0], [2172.0, 1399.0], [2172.0, 1445.0], [1224.0, 1445.0]]}, {"text": " Conceptual Captions (Sharma et al., 2018), LAIT (Qi et al.,", "confidence": 0.9855039715766907, "text_region": [[1217.0, 1442.0], [2175.0, 1439.0], [2175.0, 1495.0], [1217.0, 1498.0]]}, {"text": "2020), and OCR-CC (Yang et al., 2020) have been created.", "confidence": 0.9761220812797546, "text_region": [[1227.0, 1495.0], [2175.0, 1495.0], [2175.0, 1541.0], [1227.0, 1541.0]]}, {"text": "However, these datasets still use significantly more aggres-", "confidence": 0.9973456263542175, "text_region": [[1221.0, 1534.0], [2175.0, 1545.0], [2175.0, 1591.0], [1220.0, 1581.0]]}, {"text": "sive filtering or are designed for a specific task such as OCR", "confidence": 0.9714547991752625, "text_region": [[1224.0, 1591.0], [2172.0, 1591.0], [2172.0, 1637.0], [1224.0, 1637.0]]}, {"text": "and as a result are still much smaller than WIT with between", "confidence": 0.9790530800819397, "text_region": [[1224.0, 1637.0], [2172.0, 1637.0], [2172.0, 1683.0], [1224.0, 1683.0]]}, {"text": "1 and 10 million training examples.", "confidence": 0.9855683445930481, "text_region": [[1218.0, 1680.0], [1793.0, 1683.0], [1793.0, 1739.0], [1217.0, 1736.0]]}], "img_idx": 0, "score": 0.984871506690979}
{"type": "header", "bbox": [2129, 191, 2165, 216], "res": [{"text": "26", "confidence": 0.9996418952941895, "text_region": [[2122.0, 188.0], [2172.0, 188.0], [2172.0, 228.0], [2122.0, 228.0]]}], "img_idx": 0, "score": 0.9083175659179688}
{"type": "header", "bbox": [292, 193, 1189, 225], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9977167248725891, "text_region": [[220.0, 188.0], [1370.0, 188.0], [1370.0, 231.0], [220.0, 231.0]]}], "img_idx": 0, "score": 0.7132073044776917}
