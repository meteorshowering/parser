{"type": "text", "bbox": [1228, 1616, 2168, 2708], "res": [{"text": " SST-2 is a sentence level NLP dataset which we render into", "confidence": 0.986824631690979, "text_region": [[1221.0, 1614.0], [2169.0, 1614.0], [2169.0, 1660.0], [1221.0, 1660.0]]}, {"text": "images. We include SST-2 in order to check whether CLIP", "confidence": 0.9973477721214294, "text_region": [[1224.0, 1663.0], [2169.0, 1663.0], [2169.0, 1709.0], [1224.0, 1709.0]]}, {"text": "is able to convert low level OCR capability into a higher", "confidence": 0.9938545823097229, "text_region": [[1224.0, 1709.0], [2169.0, 1709.0], [2169.0, 1756.0], [1224.0, 1756.0]]}, {"text": "level representation. Fitting a linear classifier on CLIP's rep-", "confidence": 0.9844503402709961, "text_region": [[1224.0, 1759.0], [2169.0, 1759.0], [2169.0, 1805.0], [1224.0, 1805.0]]}, {"text": "resentation of rendered sentences achives 80.5% accuracy.", "confidence": 0.9952536225318909, "text_region": [[1224.0, 1805.0], [2172.0, 1808.0], [2172.0, 1855.0], [1224.0, 1851.0]]}, {"text": " This is on par with the 80% accuracy of a continuous bag", "confidence": 0.984369695186615, "text_region": [[1217.0, 1848.0], [2175.0, 1851.0], [2175.0, 1907.0], [1217.0, 1904.0]]}, {"text": "of words baseline using GloVe word vectors pre-trained on", "confidence": 0.9989197254180908, "text_region": [[1224.0, 1904.0], [2172.0, 1904.0], [2172.0, 1950.0], [1224.0, 1950.0]]}, {"text": "840 billion tokens (Pennington et al., 2014). While this is a", "confidence": 0.994888186454773, "text_region": [[1227.0, 1950.0], [2172.0, 1950.0], [2172.0, 1996.0], [1227.0, 1996.0]]}, {"text": "simple NLP baseline by today's standard, and well below", "confidence": 0.988656222820282, "text_region": [[1227.0, 2000.0], [2169.0, 2000.0], [2169.0, 2046.0], [1227.0, 2046.0]]}, {"text": "the 97.5% of the current SOTA, it is encouraging to see", "confidence": 0.9888163208961487, "text_region": [[1221.0, 2043.0], [2169.0, 2046.0], [2169.0, 2092.0], [1221.0, 2089.0]]}, {"text": "that CLIP is able to turn an image of rendered text into a", "confidence": 0.9924273490905762, "text_region": [[1224.0, 2096.0], [2169.0, 2096.0], [2169.0, 2138.0], [1224.0, 2138.0]]}, {"text": "non-trivial sentence level representation. Fully supervised", "confidence": 0.994861900806427, "text_region": [[1224.0, 2142.0], [2175.0, 2142.0], [2175.0, 2188.0], [1224.0, 2188.0]]}, {"text": "CLIP is also surprisingly strong on Hateful Meme detec-", "confidence": 0.9992544651031494, "text_region": [[1227.0, 2191.0], [2172.0, 2191.0], [2172.0, 2237.0], [1227.0, 2237.0]]}, {"text": "tion, where CLIP is only O.7 points behind the current single", "confidence": 0.9788907766342163, "text_region": [[1227.0, 2237.0], [2169.0, 2237.0], [2169.0, 2284.0], [1227.0, 2284.0]]}, {"text": "model SOTA and several points above the best baseline from", "confidence": 0.9964597821235657, "text_region": [[1231.0, 2287.0], [2169.0, 2287.0], [2169.0, 2333.0], [1231.0, 2333.0]]}, {"text": "the original paper. Similar to SST-2, these other results on", "confidence": 0.9785943627357483, "text_region": [[1227.0, 2333.0], [2169.0, 2333.0], [2169.0, 2379.0], [1227.0, 2379.0]]}, {"text": "Hateful Memes use the ground truth text which CLIP does", "confidence": 0.9815885424613953, "text_region": [[1224.0, 2383.0], [2169.0, 2383.0], [2169.0, 2426.0], [1224.0, 2426.0]]}, {"text": " not have access to. Finally, we note that zero-shot CLIP", "confidence": 0.99020916223526, "text_region": [[1221.0, 2429.0], [2169.0, 2425.0], [2169.0, 2472.0], [1221.0, 2475.0]]}, {"text": "outperforms the best results using fully supervised linear", "confidence": 0.989460825920105, "text_region": [[1227.0, 2478.0], [2165.0, 2478.0], [2165.0, 2524.0], [1227.0, 2524.0]]}, {"text": "probes across all other 56 models included in our evaluation", "confidence": 0.9871176481246948, "text_region": [[1224.0, 2524.0], [2165.0, 2524.0], [2165.0, 2571.0], [1224.0, 2571.0]]}, {"text": "suite. This suggests CLIP's OCR capability is at least some", "confidence": 0.992170512676239, "text_region": [[1224.0, 2574.0], [2162.0, 2574.0], [2162.0, 2620.0], [1224.0, 2620.0]]}, {"text": "what unique compared to existing work on self-supervised", "confidence": 0.9916350245475769, "text_region": [[1221.0, 2620.0], [2165.0, 2620.0], [2165.0, 2666.0], [1221.0, 2666.0]]}, {"text": "and supervised representation learning.", "confidence": 0.983793318271637, "text_region": [[1224.0, 2670.0], [1856.0, 2670.0], [1856.0, 2716.0], [1224.0, 2716.0]]}], "img_idx": 0, "score": 0.995814859867096}
{"type": "text", "bbox": [1227, 686, 2168, 1583], "res": [{"text": "CLIP's performance is still highly variable and appears to", "confidence": 0.9951504468917847, "text_region": [[1224.0, 680.0], [2169.0, 686.0], [2168.0, 733.0], [1224.0, 726.0]]}, {"text": "be sensitive to some combination of the domain (rendered or", "confidence": 0.9848001599311829, "text_region": [[1227.0, 733.0], [2169.0, 733.0], [2169.0, 776.0], [1227.0, 776.0]]}, {"text": "natural images) and the type of text to be recognized (num-", "confidence": 0.9944092631340027, "text_region": [[1227.0, 779.0], [2172.0, 779.0], [2172.0, 825.0], [1227.0, 825.0]]}, {"text": "bers or words). CLIP's OCR performance is strongest Hate-", "confidence": 0.9987830519676208, "text_region": [[1224.0, 825.0], [2172.0, 825.0], [2172.0, 871.0], [1224.0, 871.0]]}, {"text": "ful Memes and SST-2 - datasets where the text is digitally", "confidence": 0.9897872805595398, "text_region": [[1221.0, 874.0], [2172.0, 874.0], [2172.0, 921.0], [1221.0, 921.0]]}, {"text": "rendered and consists mostly of words. On IHIT5K, which", "confidence": 0.9840583205223083, "text_region": [[1224.0, 924.0], [2169.0, 924.0], [2169.0, 967.0], [1224.0, 967.0]]}, {"text": "is natural images of individually cropped words, zero-shot", "confidence": 0.9894384741783142, "text_region": [[1224.0, 970.0], [2169.0, 970.0], [2169.0, 1016.0], [1224.0, 1016.0]]}, {"text": "CLIP performs a bit more respectively and its performance", "confidence": 0.988333523273468, "text_region": [[1227.0, 1020.0], [2165.0, 1020.0], [2165.0, 1066.0], [1227.0, 1066.0]]}, {"text": "is similar to Jaderberg et al. (2014) early work combining", "confidence": 0.9878041744232178, "text_region": [[1224.0, 1066.0], [2169.0, 1066.0], [2169.0, 1112.0], [1224.0, 1112.0]]}, {"text": "deep learning and structured prediction to perform open-", "confidence": 0.9975501298904419, "text_region": [[1227.0, 1115.0], [2172.0, 1115.0], [2172.0, 1162.0], [1227.0, 1162.0]]}, {"text": "vocabulary OCR. However, performance is noticeably lower", "confidence": 0.9869468808174133, "text_region": [[1227.0, 1162.0], [2169.0, 1162.0], [2169.0, 1208.0], [1227.0, 1208.0]]}, {"text": "on two datasets involving recognition of hand written and", "confidence": 0.9894386529922485, "text_region": [[1231.0, 1211.0], [2169.0, 1211.0], [2169.0, 1257.0], [1231.0, 1257.0]]}, {"text": "street view numbers. CLIP's 51% accuracy on full number", "confidence": 0.99351966381073, "text_region": [[1227.0, 1257.0], [2169.0, 1257.0], [2169.0, 1304.0], [1227.0, 1304.0]]}, {"text": "SVHN is well below any published results. Inspection sug-", "confidence": 0.9955493807792664, "text_region": [[1224.0, 1307.0], [2169.0, 1307.0], [2169.0, 1353.0], [1224.0, 1353.0]]}, {"text": " gests CLIP struggles with repeated characters as well as the", "confidence": 0.9865428805351257, "text_region": [[1221.0, 1350.0], [2172.0, 1343.0], [2172.0, 1399.0], [1221.0, 1406.0]]}, {"text": "low resolution and blurry images of SVHN. CLIP's zero-", "confidence": 0.9832782745361328, "text_region": [[1224.0, 1402.0], [2172.0, 1402.0], [2172.0, 1449.0], [1224.0, 1449.0]]}, {"text": "shot MNIST performance is also poor and is outperformed", "confidence": 0.9896507859230042, "text_region": [[1227.0, 1449.0], [2172.0, 1449.0], [2172.0, 1495.0], [1227.0, 1495.0]]}, {"text": "by supervised logistic regression on raw pixels, one of the", "confidence": 0.9744366407394409, "text_region": [[1224.0, 1498.0], [2172.0, 1498.0], [2172.0, 1544.0], [1224.0, 1544.0]]}, {"text": "simplest possible machine learning baselines.", "confidence": 0.9955762028694153, "text_region": [[1224.0, 1544.0], [1959.0, 1544.0], [1959.0, 1591.0], [1224.0, 1591.0]]}], "img_idx": 0, "score": 0.9956700801849365}
{"type": "text", "bbox": [218, 1172, 1159, 2165], "res": [{"text": " CLIP pre-trains for the task of image-text retrieval on our", "confidence": 0.9794456958770752, "text_region": [[210.0, 1162.0], [1164.0, 1165.0], [1164.0, 1221.0], [209.0, 1218.0]]}, {"text": " noisy web-scale dataset. Although the focus of this paper", "confidence": 0.9705188274383545, "text_region": [[210.0, 1211.0], [1164.0, 1214.0], [1164.0, 1271.0], [209.0, 1267.0]]}, {"text": "is on representation learning and task learning for the pur-", "confidence": 0.992216169834137, "text_region": [[220.0, 1267.0], [1167.0, 1267.0], [1167.0, 1313.0], [220.0, 1313.0]]}, {"text": "pose of transfer to a wide variety of downstream datasets,", "confidence": 0.989840567111969, "text_region": [[220.0, 1313.0], [1164.0, 1313.0], [1164.0, 1360.0], [220.0, 1360.0]]}, {"text": "validating that CLIP is able to achieve high transfer perfor-", "confidence": 0.9731696248054504, "text_region": [[213.0, 1353.0], [1164.0, 1356.0], [1164.0, 1412.0], [213.0, 1409.0]]}, {"text": "mance transfer on exactly what it is pre-trained for is an", "confidence": 0.9902437925338745, "text_region": [[220.0, 1409.0], [1161.0, 1409.0], [1161.0, 1455.0], [220.0, 1455.0]]}, {"text": "important sanity check / proof of concept. In Table 13 we", "confidence": 0.9919677972793579, "text_region": [[220.0, 1459.0], [1161.0, 1459.0], [1161.0, 1505.0], [220.0, 1505.0]]}, {"text": "check the zero-shot transfer performance of CLIP for both", "confidence": 0.9990330934524536, "text_region": [[220.0, 1505.0], [1157.0, 1505.0], [1157.0, 1551.0], [220.0, 1551.0]]}, {"text": "text and image retrieval on the Flickr30k and MSCOCO", "confidence": 0.9793649911880493, "text_region": [[220.0, 1554.0], [1161.0, 1554.0], [1161.0, 1600.0], [220.0, 1600.0]]}, {"text": "datsets. Zero-shot CLIP matches or outperforms all prior", "confidence": 0.9839922785758972, "text_region": [[220.0, 1600.0], [1164.0, 1600.0], [1164.0, 1647.0], [220.0, 1647.0]]}, {"text": "zero-shot results on these two datasets. Zero-shot CLIP is", "confidence": 0.9991997480392456, "text_region": [[216.0, 1650.0], [1161.0, 1650.0], [1161.0, 1693.0], [216.0, 1693.0]]}, {"text": "also competitive with the current overall SOTA for the task", "confidence": 0.9906618595123291, "text_region": [[216.0, 1696.0], [1161.0, 1696.0], [1161.0, 1742.0], [216.0, 1742.0]]}, {"text": "of text retrieval on Flickr30k. On image retrieval, CLIP's", "confidence": 0.9843762516975403, "text_region": [[216.0, 1746.0], [1161.0, 1746.0], [1161.0, 1792.0], [216.0, 1792.0]]}, {"text": "performance relative to the overall state of the art is notice-", "confidence": 0.9909141659736633, "text_region": [[220.0, 1792.0], [1161.0, 1792.0], [1161.0, 1838.0], [220.0, 1838.0]]}, {"text": "ably lower. However, zero-shot CLIP is still competitive", "confidence": 0.9911175966262817, "text_region": [[220.0, 1841.0], [1161.0, 1841.0], [1161.0, 1888.0], [220.0, 1888.0]]}, {"text": "with a fine-tuned Unicoder-VL. On the larger MS-COCO", "confidence": 0.9937466979026794, "text_region": [[213.0, 1878.0], [1168.0, 1881.0], [1167.0, 1937.0], [213.0, 1934.0]]}, {"text": "dataset fine-tuning improves performance significantly and", "confidence": 0.9937934279441833, "text_region": [[220.0, 1937.0], [1161.0, 1937.0], [1161.0, 1980.0], [220.0, 1980.0]]}, {"text": "zero-shot CLIP is not competitive with the most recent work.", "confidence": 0.9970811605453491, "text_region": [[220.0, 1983.0], [1167.0, 1983.0], [1167.0, 2030.0], [220.0, 2030.0]]}, {"text": "For both these datasets we prepend the prompt \u201ca phot0", "confidence": 0.9864283204078674, "text_region": [[220.0, 2033.0], [1161.0, 2033.0], [1161.0, 2079.0], [220.0, 2079.0]]}, {"text": "Of\" to the description of each image which we found boosts", "confidence": 0.9907661080360413, "text_region": [[220.0, 2079.0], [1161.0, 2079.0], [1161.0, 2125.0], [220.0, 2125.0]]}, {"text": "CLIP's zero-shot R@1 performance between 1 and 2 points.", "confidence": 0.9937390089035034, "text_region": [[216.0, 2122.0], [1164.0, 2125.0], [1164.0, 2171.0], [216.0, 2168.0]]}], "img_idx": 0, "score": 0.9952796101570129}
{"type": "text", "bbox": [220, 2302, 1158, 2864], "res": [{"text": "Although visualizations have shown that ImageNet models", "confidence": 0.9914848208427429, "text_region": [[216.0, 2297.0], [1161.0, 2300.0], [1161.0, 2346.0], [216.0, 2343.0]]}, {"text": "contain features that respond to the presence of text in an", "confidence": 0.9937131404876709, "text_region": [[220.0, 2346.0], [1157.0, 2346.0], [1157.0, 2392.0], [220.0, 2392.0]]}, {"text": "image (Zeiler & Fergus, 2014), these representations are", "confidence": 0.9859355092048645, "text_region": [[220.0, 2396.0], [1161.0, 2396.0], [1161.0, 2442.0], [220.0, 2442.0]]}, {"text": "not sufficiently fine-grained to use for the task of optical", "confidence": 0.9738416075706482, "text_region": [[220.0, 2445.0], [1164.0, 2445.0], [1164.0, 2492.0], [220.0, 2492.0]]}, {"text": "character recognition (OCR). To compensate, models are", "confidence": 0.9828910827636719, "text_region": [[220.0, 2495.0], [1164.0, 2495.0], [1164.0, 2541.0], [220.0, 2541.0]]}, {"text": "augmented with the outputs of custom OCR engines and", "confidence": 0.9855102300643921, "text_region": [[216.0, 2541.0], [1164.0, 2541.0], [1164.0, 2587.0], [216.0, 2587.0]]}, {"text": "features to boost performance on tasks where this capability", "confidence": 0.9966402053833008, "text_region": [[220.0, 2590.0], [1164.0, 2590.0], [1164.0, 2633.0], [220.0, 2633.0]]}, {"text": "is required (Singh et al., 2019; Yang et al., 2020). Early dur-", "confidence": 0.9833547472953796, "text_region": [[223.0, 2637.0], [1167.0, 2637.0], [1167.0, 2683.0], [223.0, 2683.0]]}, {"text": "ing the development of CLIP, we noticed that CLIP began to", "confidence": 0.9988715052604675, "text_region": [[213.0, 2676.0], [1164.0, 2680.0], [1164.0, 2736.0], [213.0, 2732.0]]}, {"text": "learn primitive OCR capabilities which appeared to steadily", "confidence": 0.9936432242393494, "text_region": [[220.0, 2732.0], [1161.0, 2732.0], [1161.0, 2779.0], [220.0, 2779.0]]}, {"text": "improve over the course of the project. To evaluate this", "confidence": 0.9977945685386658, "text_region": [[220.0, 2782.0], [1161.0, 2782.0], [1161.0, 2828.0], [220.0, 2828.0]]}, {"text": "qualitatively noticed behavior, we measured performance", "confidence": 0.9960811138153076, "text_region": [[216.0, 2828.0], [1161.0, 2828.0], [1161.0, 2874.0], [216.0, 2874.0]]}], "img_idx": 0, "score": 0.9942286610603333}
{"type": "text", "bbox": [219, 280, 1160, 644], "res": [{"text": "Finally, we caution that WIT includes this filtered subset ", "confidence": 0.9889026284217834, "text_region": [[220.0, 277.0], [1174.0, 277.0], [1174.0, 320.0], [220.0, 320.0]]}, {"text": "of YFCC100M. This could result in our ablation under-", "confidence": 0.975732684135437, "text_region": [[220.0, 323.0], [1234.0, 323.0], [1234.0, 370.0], [220.0, 370.0]]}, {"text": "estimating the size of performance differences between ", "confidence": 0.9876599907875061, "text_region": [[220.0, 373.0], [1177.0, 373.0], [1177.0, 419.0], [220.0, 419.0]]}, {"text": "YFCC1ooM and the rest of WIT. We do not think this is", "confidence": 0.965011477470398, "text_region": [[220.0, 422.0], [1177.0, 422.0], [1177.0, 465.0], [220.0, 465.0]]}, {"text": "likely as YFCC100M is only 3.7% of the overall WIT data", "confidence": 0.9740235805511475, "text_region": [[220.0, 469.0], [1167.0, 469.0], [1167.0, 511.0], [220.0, 511.0]]}, {"text": "blend and it did not noticeably change the performance of ", "confidence": 0.9891327023506165, "text_region": [[216.0, 511.0], [1171.0, 515.0], [1171.0, 561.0], [216.0, 558.0]]}, {"text": " models when it was added to the existing data blend during", "confidence": 0.9858940839767456, "text_region": [[213.0, 554.0], [1168.0, 561.0], [1167.0, 617.0], [213.0, 610.0]]}, {"text": "the creation of WIT.", "confidence": 0.9992790222167969, "text_region": [[216.0, 614.0], [545.0, 614.0], [545.0, 657.0], [216.0, 657.0]]}], "img_idx": 0, "score": 0.9928714036941528}
{"type": "text", "bbox": [1227, 280, 2167, 633], "res": [{"text": " on 5 datasets requiring the direct and indirect use of OCR.", "confidence": 0.9910494089126587, "text_region": [[1217.0, 277.0], [2172.0, 277.0], [2172.0, 320.0], [1217.0, 320.0]]}, {"text": "of YFCC100M. This could result in our ablation under-", "confidence": 0.975732684135437, "text_region": [[220.0, 323.0], [1234.0, 323.0], [1234.0, 370.0], [220.0, 370.0]]}, {"text": " Three of these datasets MNIST (LeCun), SVHN (Netzer", "confidence": 0.9850603342056274, "text_region": [[1214.0, 323.0], [2165.0, 323.0], [2165.0, 370.0], [1214.0, 370.0]]}, {"text": " et al., 2011), and IHIT5K (Mishra et al., 2012) directly check", "confidence": 0.9774405360221863, "text_region": [[1214.0, 373.0], [2169.0, 373.0], [2169.0, 416.0], [1214.0, 416.0]]}, {"text": " the ability of a model to perform low-level character and", "confidence": 0.9811797142028809, "text_region": [[1217.0, 422.0], [2172.0, 422.0], [2172.0, 465.0], [1217.0, 465.0]]}, {"text": "word recognition, while Hateful Memes (Kiela et al., 2020)", "confidence": 0.99334317445755, "text_region": [[1221.0, 469.0], [2172.0, 469.0], [2172.0, 515.0], [1221.0, 515.0]]}, {"text": " and SST-2 (Socher et al., 2013) check the ability of a model", "confidence": 0.9888988733291626, "text_region": [[1221.0, 515.0], [2172.0, 515.0], [2172.0, 561.0], [1221.0, 561.0]]}, {"text": "to use OCR to perform a semantic task. Results are reported", "confidence": 0.986603319644928, "text_region": [[1224.0, 564.0], [2172.0, 564.0], [2172.0, 610.0], [1224.0, 610.0]]}, {"text": "in Table 14.", "confidence": 0.9981668591499329, "text_region": [[1224.0, 610.0], [1420.0, 610.0], [1420.0, 657.0], [1224.0, 657.0]]}], "img_idx": 0, "score": 0.9900891184806824}
{"type": "text", "bbox": [219, 806, 1161, 1033], "res": [{"text": "Due to the large variety of datasets and experiments consid-", "confidence": 0.9967541098594666, "text_region": [[216.0, 799.0], [1171.0, 799.0], [1171.0, 855.0], [216.0, 855.0]]}, {"text": "ered in this work, the main body focuses on summarizing", "confidence": 0.9889408349990845, "text_region": [[216.0, 848.0], [1161.0, 851.0], [1161.0, 898.0], [216.0, 894.0]]}, {"text": "and analyzing overall results. In the following subsections", "confidence": 0.994988203048706, "text_region": [[220.0, 901.0], [1164.0, 901.0], [1164.0, 944.0], [220.0, 944.0]]}, {"text": "we report details of performance for specific groups of tasks,", "confidence": 0.9985101819038391, "text_region": [[220.0, 947.0], [1167.0, 947.0], [1167.0, 993.0], [220.0, 993.0]]}, {"text": "datasets, and evaluation settings.", "confidence": 0.9975647926330566, "text_region": [[216.0, 993.0], [742.0, 997.0], [742.0, 1043.0], [216.0, 1039.0]]}], "img_idx": 0, "score": 0.9886435866355896}
{"type": "title", "bbox": [221, 724, 968, 760], "res": [{"text": "E. Selected Task and Dataset Results", "confidence": 0.9964617490768433, "text_region": [[213.0, 713.0], [975.0, 716.0], [974.0, 772.0], [213.0, 769.0]]}], "img_idx": 0, "score": 0.9554627537727356}
{"type": "title", "bbox": [220, 1097, 726, 1131], "res": [{"text": "E.1. Image and Text Retrieval", "confidence": 0.999800980091095, "text_region": [[220.0, 1096.0], [732.0, 1096.0], [732.0, 1142.0], [220.0, 1142.0]]}], "img_idx": 0, "score": 0.9543454051017761}
{"type": "title", "bbox": [220, 2228, 823, 2266], "res": [{"text": "E.2. Optical Character Recognition", "confidence": 0.9993672966957092, "text_region": [[220.0, 2228.0], [828.0, 2228.0], [828.0, 2274.0], [220.0, 2274.0]]}], "img_idx": 0, "score": 0.9513882398605347}
{"type": "header", "bbox": [2126, 192, 2163, 217], "res": [{"text": "45", "confidence": 0.9998701810836792, "text_region": [[2122.0, 188.0], [2169.0, 188.0], [2169.0, 228.0], [2122.0, 228.0]]}], "img_idx": 0, "score": 0.9007408618927002}
{"type": "header", "bbox": [292, 193, 1189, 224], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9983819127082825, "text_region": [[220.0, 191.0], [1367.0, 191.0], [1367.0, 234.0], [220.0, 234.0]]}], "img_idx": 0, "score": 0.7191386818885803}
