{"type": "text", "bbox": [1227, 778, 2168, 2250], "res": [{"text": "CLIP is pre-trained to predict if an image and a text snippet", "confidence": 0.9942383170127869, "text_region": [[1224.0, 775.0], [2172.0, 782.0], [2172.0, 828.0], [1224.0, 822.0]]}, {"text": "are paired together in its dataset. To perform zero-shot clas-", "confidence": 0.9971666932106018, "text_region": [[1224.0, 828.0], [2172.0, 828.0], [2172.0, 874.0], [1224.0, 874.0]]}, {"text": "sification, we reuse this capability. For each dataset, we use", "confidence": 0.9817132353782654, "text_region": [[1224.0, 878.0], [2169.0, 878.0], [2169.0, 924.0], [1224.0, 924.0]]}, {"text": "the names of all the classes in the dataset as the set of poten-", "confidence": 0.9891899228096008, "text_region": [[1224.0, 924.0], [2172.0, 924.0], [2172.0, 970.0], [1224.0, 970.0]]}, {"text": "tial text pairings and predict the most probable (image, text)", "confidence": 0.9868524074554443, "text_region": [[1224.0, 973.0], [2172.0, 973.0], [2172.0, 1020.0], [1224.0, 1020.0]]}, {"text": "pair according to CLIP. In a bit more detail, we first compute", "confidence": 0.9900975227355957, "text_region": [[1224.0, 1020.0], [2169.0, 1020.0], [2169.0, 1066.0], [1224.0, 1066.0]]}, {"text": " the feature embedding of the image and the feature embed-", "confidence": 0.9841318726539612, "text_region": [[1217.0, 1063.0], [2178.0, 1059.0], [2179.0, 1115.0], [1217.0, 1119.0]]}, {"text": "ding of the set of possible texts by their respective encoders.", "confidence": 0.9919537305831909, "text_region": [[1227.0, 1115.0], [2172.0, 1115.0], [2172.0, 1162.0], [1227.0, 1162.0]]}, {"text": "The cosine similarity of these embeddings is then calculated,", "confidence": 0.9877688884735107, "text_region": [[1227.0, 1165.0], [2175.0, 1165.0], [2175.0, 1208.0], [1227.0, 1208.0]]}, {"text": "scaled by a temperature parameter T, and normalized into a", "confidence": 0.9958529472351074, "text_region": [[1224.0, 1208.0], [2175.0, 1204.0], [2175.0, 1261.0], [1224.0, 1264.0]]}, {"text": "probability distribution via a softmax. Note that this predic-", "confidence": 0.9945182204246521, "text_region": [[1227.0, 1261.0], [2172.0, 1261.0], [2172.0, 1304.0], [1227.0, 1304.0]]}, {"text": "tion layer is a multinomial logistic regression classifier with", "confidence": 0.9905620217323303, "text_region": [[1227.0, 1307.0], [2169.0, 1307.0], [2169.0, 1353.0], [1227.0, 1353.0]]}, {"text": " L2-normalized inputs, L2-normalized weights, no bias, and", "confidence": 0.9857515096664429, "text_region": [[1221.0, 1350.0], [2172.0, 1353.0], [2172.0, 1399.0], [1221.0, 1396.0]]}, {"text": "temperature scaling. When interpreted this way, the image", "confidence": 0.9974749088287354, "text_region": [[1224.0, 1402.0], [2169.0, 1402.0], [2169.0, 1449.0], [1224.0, 1449.0]]}, {"text": "encoder is the computer vision backbone which computes a", "confidence": 0.9868852496147156, "text_region": [[1224.0, 1452.0], [2172.0, 1452.0], [2172.0, 1498.0], [1224.0, 1498.0]]}, {"text": "feature representation for the image and the text encoder is a", "confidence": 0.9935912489891052, "text_region": [[1221.0, 1498.0], [2169.0, 1498.0], [2169.0, 1544.0], [1221.0, 1544.0]]}, {"text": "hypernetwork (Ha et al., 2016) which generates the weights", "confidence": 0.9877865314483643, "text_region": [[1224.0, 1544.0], [2172.0, 1544.0], [2172.0, 1591.0], [1224.0, 1591.0]]}, {"text": "of a linear classifier based on the text specifying the visual", "confidence": 0.9959636926651001, "text_region": [[1224.0, 1594.0], [2172.0, 1594.0], [2172.0, 1640.0], [1224.0, 1640.0]]}, {"text": "concepts that the classes represent. Lei Ba et al. (2015) first", "confidence": 0.9746598601341248, "text_region": [[1224.0, 1640.0], [2175.0, 1640.0], [2175.0, 1686.0], [1224.0, 1686.0]]}, {"text": "introduced a zero-shot image classifier of this form while", "confidence": 0.9887341260910034, "text_region": [[1227.0, 1690.0], [2169.0, 1690.0], [2169.0, 1736.0], [1227.0, 1736.0]]}, {"text": "the idea of generating a classifier from natural language", "confidence": 0.9975029230117798, "text_region": [[1224.0, 1739.0], [2169.0, 1739.0], [2169.0, 1785.0], [1224.0, 1785.0]]}, {"text": " dates back to at least Elhoseiny et al. (2013). Continuing", "confidence": 0.9863527417182922, "text_region": [[1221.0, 1772.0], [2172.0, 1779.0], [2172.0, 1835.0], [1221.0, 1828.0]]}, {"text": "with this interpretation, every step of CLIP pre-training can", "confidence": 0.988780677318573, "text_region": [[1221.0, 1828.0], [2175.0, 1825.0], [2175.0, 1881.0], [1221.0, 1884.0]]}, {"text": "be viewed as optimizing the performance of a randomly", "confidence": 0.9912803173065186, "text_region": [[1227.0, 1881.0], [2169.0, 1881.0], [2169.0, 1927.0], [1227.0, 1927.0]]}, {"text": "created proxy to a computer vision dataset which contains 1", "confidence": 0.9935089945793152, "text_region": [[1227.0, 1927.0], [2175.0, 1927.0], [2175.0, 1973.0], [1227.0, 1973.0]]}, {"text": "example per class and has 32,768 total classes defined via", "confidence": 0.9859168529510498, "text_region": [[1224.0, 1970.0], [2175.0, 1967.0], [2175.0, 2023.0], [1224.0, 2026.0]]}, {"text": "natural language descriptions. For zero-shot evaluation, we", "confidence": 0.981412947177887, "text_region": [[1221.0, 2020.0], [2172.0, 2016.0], [2172.0, 2072.0], [1221.0, 2076.0]]}, {"text": "cache the zero-shot classifier once it has been computed by", "confidence": 0.9878666400909424, "text_region": [[1221.0, 2062.0], [2172.0, 2066.0], [2172.0, 2122.0], [1221.0, 2119.0]]}, {"text": "the text encoder and reuse it for all subsequent predictions.", "confidence": 0.9817788600921631, "text_region": [[1224.0, 2122.0], [2172.0, 2122.0], [2172.0, 2165.0], [1224.0, 2165.0]]}, {"text": "This allows the cost of generating it to be amortized across", "confidence": 0.9923508763313293, "text_region": [[1227.0, 2168.0], [2172.0, 2168.0], [2172.0, 2214.0], [1227.0, 2214.0]]}, {"text": "all the predictions in a dataset.", "confidence": 0.996917724609375, "text_region": [[1227.0, 2218.0], [1716.0, 2218.0], [1716.0, 2260.0], [1227.0, 2260.0]]}], "img_idx": 0, "score": 0.9949864745140076}
{"type": "text", "bbox": [1228, 281, 2166, 646], "res": [{"text": "training as a transfer learning method to improve supervised", "confidence": 0.9969517588615417, "text_region": [[1227.0, 277.0], [2169.0, 277.0], [2169.0, 320.0], [1227.0, 320.0]]}, {"text": "fine-tuning, it also included an ablation study demonstrat-", "confidence": 0.9942860007286072, "text_region": [[1227.0, 323.0], [2172.0, 323.0], [2172.0, 370.0], [1227.0, 370.0]]}, {"text": "ing that the performance of four heuristic zero-shot transfer", "confidence": 0.9871297478675842, "text_region": [[1224.0, 373.0], [2172.0, 373.0], [2172.0, 419.0], [1224.0, 419.0]]}, {"text": "methods improved steadily over the course of pre-training,", "confidence": 0.9899519085884094, "text_region": [[1224.0, 419.0], [2172.0, 422.0], [2172.0, 469.0], [1224.0, 465.0]]}, {"text": "without any supervised adaption. This analysis served as the", "confidence": 0.9898859262466431, "text_region": [[1221.0, 462.0], [2172.0, 459.0], [2172.0, 515.0], [1221.0, 518.0]]}, {"text": "basis for GPT-2 (Radford et al., 2019) which focused exclu-", "confidence": 0.9892024397850037, "text_region": [[1224.0, 511.0], [2169.0, 515.0], [2168.0, 561.0], [1224.0, 558.0]]}, {"text": "sively on studying the task-learning capabilities of language", "confidence": 0.9940305352210999, "text_region": [[1224.0, 558.0], [2172.0, 558.0], [2172.0, 614.0], [1224.0, 614.0]]}, {"text": "models via zero-shot transfer.", "confidence": 0.9995644688606262, "text_region": [[1224.0, 614.0], [1703.0, 614.0], [1703.0, 657.0], [1224.0, 657.0]]}], "img_idx": 0, "score": 0.9923937320709229}
{"type": "text", "bbox": [1227, 2384, 2167, 2853], "res": [{"text": " In Table 1 we compare Visual N-Grams to CLIP. The best", "confidence": 0.9938727021217346, "text_region": [[1221.0, 2379.0], [2169.0, 2379.0], [2169.0, 2426.0], [1221.0, 2426.0]]}, {"text": "CLIP model improves accuracy on ImageNet from a proof", "confidence": 0.9885263442993164, "text_region": [[1224.0, 2429.0], [2175.0, 2432.0], [2175.0, 2478.0], [1224.0, 2475.0]]}, {"text": "of concept 11.5% to 76.2% and matches the performance", "confidence": 0.9889996647834778, "text_region": [[1227.0, 2482.0], [2172.0, 2482.0], [2172.0, 2528.0], [1227.0, 2528.0]]}, {"text": "of the original ResNet-50 despite using none of the 1.28", "confidence": 0.986456573009491, "text_region": [[1224.0, 2528.0], [2172.0, 2528.0], [2172.0, 2574.0], [1224.0, 2574.0]]}, {"text": "million crowd-labeled training examples available for this", "confidence": 0.9905453324317932, "text_region": [[1224.0, 2577.0], [2169.0, 2577.0], [2169.0, 2624.0], [1224.0, 2624.0]]}, {"text": "dataset. Additionally, the top-5 accuracy of CLIP models", "confidence": 0.9898279905319214, "text_region": [[1227.0, 2624.0], [2169.0, 2624.0], [2169.0, 2670.0], [1227.0, 2670.0]]}, {"text": "are noticeably higher than their top-1, and this model has a", "confidence": 0.9882070422172546, "text_region": [[1224.0, 2673.0], [2169.0, 2673.0], [2169.0, 2716.0], [1224.0, 2716.0]]}, {"text": "95% top-5 accuracy, matching Inception-V4 (Szegedy et al.,", "confidence": 0.9977394342422485, "text_region": [[1224.0, 2719.0], [2172.0, 2719.0], [2172.0, 2765.0], [1224.0, 2765.0]]}, {"text": " 2016). The ability to match the performance of a strong,", "confidence": 0.9909107089042664, "text_region": [[1221.0, 2759.0], [2179.0, 2762.0], [2178.0, 2818.0], [1221.0, 2815.0]]}, {"text": "fully supervised baselines in a zero-shot setting suggests", "confidence": 0.9922776222229004, "text_region": [[1221.0, 2811.0], [2169.0, 2818.0], [2168.0, 2865.0], [1221.0, 2858.0]]}], "img_idx": 0, "score": 0.9923532605171204}
{"type": "text", "bbox": [219, 1766, 1162, 2839], "res": [{"text": "To our knowledge, Visual N-Grams (Li et al., 2017) first", "confidence": 0.9865107536315918, "text_region": [[216.0, 1769.0], [1164.0, 1769.0], [1164.0, 1815.0], [216.0, 1815.0]]}, {"text": "studied zero-shot transfer to existing image classification", "confidence": 0.99074387550354, "text_region": [[216.0, 1815.0], [1161.0, 1815.0], [1161.0, 1861.0], [216.0, 1861.0]]}, {"text": "datasets in the manner described above. It is also the only", "confidence": 0.982617199420929, "text_region": [[216.0, 1865.0], [1161.0, 1865.0], [1161.0, 1911.0], [216.0, 1911.0]]}, {"text": "other work we are aware of that has studied zero-shot trans-", "confidence": 0.9761461019515991, "text_region": [[220.0, 1914.0], [1161.0, 1914.0], [1161.0, 1960.0], [220.0, 1960.0]]}, {"text": "fer to standard image classification datasets using a gener-", "confidence": 0.9811599254608154, "text_region": [[213.0, 1954.0], [1164.0, 1957.0], [1164.0, 2013.0], [213.0, 2010.0]]}, {"text": "ically pre-trained model and serves as the best reference", "confidence": 0.9857955574989319, "text_region": [[216.0, 2010.0], [1161.0, 2010.0], [1161.0, 2056.0], [216.0, 2056.0]]}, {"text": "point for contextualizing CLIP. Their approach learns the", "confidence": 0.9943535327911377, "text_region": [[220.0, 2059.0], [1161.0, 2059.0], [1161.0, 2105.0], [220.0, 2105.0]]}, {"text": "parameters of a dictionary of 142,806 visual n-grams (span-", "confidence": 0.9939500093460083, "text_region": [[220.0, 2105.0], [1161.0, 2105.0], [1161.0, 2152.0], [220.0, 2152.0]]}, {"text": "ning 1- to 5- grams) and optimizes these n-grams using a", "confidence": 0.9867625832557678, "text_region": [[220.0, 2155.0], [1161.0, 2155.0], [1161.0, 2201.0], [220.0, 2201.0]]}, {"text": " differential version of Jelinek-Mercer smoothing to maxi-", "confidence": 0.9839584827423096, "text_region": [[213.0, 2191.0], [1171.0, 2195.0], [1171.0, 2251.0], [213.0, 2247.0]]}, {"text": "mize the probability of all text n-grams for a given image.", "confidence": 0.9867566823959351, "text_region": [[216.0, 2247.0], [1164.0, 2251.0], [1164.0, 2297.0], [216.0, 2293.0]]}, {"text": "In order to perform zero-shot transfer, they first convert the", "confidence": 0.9965760111808777, "text_region": [[220.0, 2297.0], [1161.0, 2297.0], [1161.0, 2343.0], [220.0, 2343.0]]}, {"text": "text of each of the dataset's class names into its n-gram", "confidence": 0.9840883612632751, "text_region": [[220.0, 2346.0], [1161.0, 2346.0], [1161.0, 2389.0], [220.0, 2389.0]]}, {"text": "representation and then compute its probability according", "confidence": 0.9811119437217712, "text_region": [[216.0, 2392.0], [1164.0, 2392.0], [1164.0, 2439.0], [216.0, 2439.0]]}, {"text": "to their model, predicting the one with the highest score.", "confidence": 0.9934769868850708, "text_region": [[213.0, 2435.0], [1124.0, 2439.0], [1124.0, 2485.0], [213.0, 2482.0]]}, {"text": " Our focus on studying zero-shot transfer as an evaluation of ", "confidence": 0.9827958941459656, "text_region": [[216.0, 2511.0], [1167.0, 2511.0], [1167.0, 2558.0], [216.0, 2558.0]]}, {"text": "task learning is inspired by work demonstrating task learn-", "confidence": 0.9798444509506226, "text_region": [[220.0, 2561.0], [1167.0, 2561.0], [1167.0, 2607.0], [220.0, 2607.0]]}, {"text": "ing in the field of NLP. To our knowledge Liu et al. (2018)", "confidence": 0.9971057176589966, "text_region": [[220.0, 2607.0], [1167.0, 2607.0], [1167.0, 2653.0], [220.0, 2653.0]]}, {"text": "first identified task learning as an \u201cunexpected side-effect\u201d\"", "confidence": 0.9730411767959595, "text_region": [[220.0, 2656.0], [1167.0, 2656.0], [1167.0, 2703.0], [220.0, 2703.0]]}, {"text": "when a language model trained to generate Wikipedia ar-", "confidence": 0.983055055141449, "text_region": [[220.0, 2706.0], [1164.0, 2706.0], [1164.0, 2752.0], [220.0, 2752.0]]}, {"text": "ticles learned to reliably transliterate names between lan-", "confidence": 0.9943556785583496, "text_region": [[216.0, 2752.0], [1167.0, 2752.0], [1167.0, 2795.0], [216.0, 2795.0]]}, {"text": " guages. While GPT-1 (Radford et al., 2018) focused on pre-", "confidence": 0.9865189790725708, "text_region": [[209.0, 2795.0], [1167.0, 2792.0], [1168.0, 2848.0], [210.0, 2851.0]]}], "img_idx": 0, "score": 0.9914604425430298}
{"type": "text", "bbox": [218, 507, 1161, 1736], "res": [{"text": " In computer vision, zero-shot learning usually refers to the", "confidence": 0.9892777800559998, "text_region": [[210.0, 495.0], [1168.0, 498.0], [1167.0, 554.0], [209.0, 551.0]]}, {"text": " study of generalizing to unseen object categories in image", "confidence": 0.9861074090003967, "text_region": [[210.0, 544.0], [1168.0, 548.0], [1167.0, 604.0], [209.0, 601.0]]}, {"text": "classification (Lampert et al., 2009). We instead use the", "confidence": 0.9906384944915771, "text_region": [[220.0, 594.0], [1157.0, 594.0], [1157.0, 640.0], [220.0, 640.0]]}, {"text": "term in a broader sense and study generalization to unseen", "confidence": 0.9851871728897095, "text_region": [[220.0, 647.0], [1161.0, 647.0], [1161.0, 693.0], [220.0, 693.0]]}, {"text": " datasets. We motivate this as a proxy for performing un-", "confidence": 0.9908167719841003, "text_region": [[210.0, 686.0], [1168.0, 690.0], [1167.0, 746.0], [209.0, 742.0]]}, {"text": "seen tasks, as aspired to in the zero-data learning paper of", "confidence": 0.9889587163925171, "text_region": [[216.0, 742.0], [1167.0, 742.0], [1167.0, 789.0], [216.0, 789.0]]}, {"text": "Larochelle et al. (2008). While much research in the field of", "confidence": 0.9938908815383911, "text_region": [[220.0, 792.0], [1164.0, 792.0], [1164.0, 835.0], [220.0, 835.0]]}, {"text": "unsupervised learning focuses on the representation learn-", "confidence": 0.9894421100616455, "text_region": [[220.0, 838.0], [1157.0, 838.0], [1157.0, 884.0], [220.0, 884.0]]}, {"text": "ing capabilities of machine learning systems, we motivate", "confidence": 0.9909592866897583, "text_region": [[220.0, 888.0], [1161.0, 888.0], [1161.0, 934.0], [220.0, 934.0]]}, {"text": "studying zero-shot transfer as a way of measuring the task-", "confidence": 0.9918097257614136, "text_region": [[220.0, 931.0], [1161.0, 931.0], [1161.0, 977.0], [220.0, 977.0]]}, {"text": "learning capabilities of machine learning systems. In this", "confidence": 0.9900689125061035, "text_region": [[216.0, 983.0], [1164.0, 983.0], [1164.0, 1030.0], [216.0, 1030.0]]}, {"text": "view, a dataset evaluates performance on a task on a spe-", "confidence": 0.9957701563835144, "text_region": [[213.0, 1023.0], [1164.0, 1026.0], [1164.0, 1082.0], [213.0, 1079.0]]}, {"text": "cific distribution. However, many popular computer vision", "confidence": 0.9993996024131775, "text_region": [[220.0, 1079.0], [1161.0, 1079.0], [1161.0, 1125.0], [220.0, 1125.0]]}, {"text": "datasets were created by the research community primarily", "confidence": 0.9883525371551514, "text_region": [[216.0, 1125.0], [1161.0, 1125.0], [1161.0, 1172.0], [216.0, 1172.0]]}, {"text": "as benchmarks to guide the development of generic image", "confidence": 0.9859463572502136, "text_region": [[216.0, 1171.0], [1164.0, 1175.0], [1164.0, 1221.0], [216.0, 1218.0]]}, {"text": "classification methods rather than measuring performance", "confidence": 0.9880738854408264, "text_region": [[216.0, 1221.0], [1161.0, 1221.0], [1161.0, 1267.0], [216.0, 1267.0]]}, {"text": "on a specific task. While it is reasonable to say that the", "confidence": 0.9926739931106567, "text_region": [[216.0, 1271.0], [1164.0, 1267.0], [1164.0, 1313.0], [216.0, 1317.0]]}, {"text": "SVHN dataset measures the task of street number transcrip-", "confidence": 0.999591588973999, "text_region": [[220.0, 1317.0], [1167.0, 1317.0], [1167.0, 1363.0], [220.0, 1363.0]]}, {"text": "tion on the distribution of Google Street View photos, it is", "confidence": 0.9932331442832947, "text_region": [[220.0, 1363.0], [1161.0, 1363.0], [1161.0, 1409.0], [220.0, 1409.0]]}, {"text": "unclear what \u201creal\u2019 task the CIFAR-10 dataset measures.", "confidence": 0.97867751121521, "text_region": [[220.0, 1409.0], [1164.0, 1409.0], [1164.0, 1455.0], [220.0, 1455.0]]}, {"text": "It is clear, however, what distribution CIFAR-10 is drawn", "confidence": 0.9868576526641846, "text_region": [[220.0, 1459.0], [1161.0, 1459.0], [1161.0, 1505.0], [220.0, 1505.0]]}, {"text": "from - TinyImages (Torralba et al., 2008). On these kinds of", "confidence": 0.981660008430481, "text_region": [[213.0, 1498.0], [1171.0, 1502.0], [1171.0, 1558.0], [213.0, 1554.0]]}, {"text": "datasets, zero-shot transfer is more an evaluation of CLIP's", "confidence": 0.9968697428703308, "text_region": [[216.0, 1558.0], [1161.0, 1558.0], [1161.0, 1600.0], [216.0, 1600.0]]}, {"text": "robustness to distribution shift and domain generalization", "confidence": 0.9850014448165894, "text_region": [[216.0, 1604.0], [1164.0, 1604.0], [1164.0, 1650.0], [216.0, 1650.0]]}, {"text": "rather than task generalization. Please see Section 3.3 for", "confidence": 0.9951127767562866, "text_region": [[216.0, 1650.0], [1164.0, 1647.0], [1164.0, 1693.0], [216.0, 1696.0]]}, {"text": " analysis focused on this.", "confidence": 0.9773591756820679, "text_region": [[213.0, 1693.0], [609.0, 1693.0], [609.0, 1739.0], [213.0, 1739.0]]}], "img_idx": 0, "score": 0.9806025624275208}
{"type": "text", "bbox": [1231, 709, 2041, 741], "res": [{"text": "3.1.2. USING CLIP FOR ZERO-SHOT TRANSFER", "confidence": 0.9828680753707886, "text_region": [[1224.0, 703.0], [2046.0, 706.0], [2045.0, 752.0], [1224.0, 749.0]]}], "img_idx": 0, "score": 0.932685375213623}
{"type": "text", "bbox": [1232, 2314, 2098, 2345], "res": [{"text": "3.1.3. INITIAL COMPARISON TO VISUAL N-GRAMS", "confidence": 0.9844905734062195, "text_region": [[1224.0, 2307.0], [2102.0, 2310.0], [2102.0, 2356.0], [1224.0, 2353.0]]}], "img_idx": 0, "score": 0.8506537675857544}
{"type": "title", "bbox": [223, 359, 620, 389], "res": [{"text": "3.1. Zero-Shot Transfer", "confidence": 0.9912851452827454, "text_region": [[216.0, 353.0], [622.0, 353.0], [622.0, 396.0], [216.0, 396.0]]}], "img_idx": 0, "score": 0.9563093185424805}
{"type": "title", "bbox": [221, 274, 528, 315], "res": [{"text": "3. Experiments", "confidence": 0.9987382292747498, "text_region": [[213.0, 267.0], [536.0, 267.0], [536.0, 323.0], [213.0, 323.0]]}], "img_idx": 0, "score": 0.9520664215087891}
{"type": "title", "bbox": [218, 432, 550, 461], "res": [{"text": "3.1.1. MOTIVATION", "confidence": 0.9867697358131409, "text_region": [[220.0, 426.0], [559.0, 426.0], [559.0, 472.0], [220.0, 472.0]]}], "img_idx": 0, "score": 0.9414438009262085}
{"type": "header", "bbox": [2143, 193, 2165, 221], "res": [], "img_idx": 0, "score": 0.7186014652252197}
{"type": "header", "bbox": [228, 193, 1124, 224], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9886844754219055, "text_region": [[220.0, 188.0], [1370.0, 188.0], [1370.0, 234.0], [220.0, 234.0]]}], "img_idx": 0, "score": 0.707044780254364}
