{"type": "text", "bbox": [218, 828, 1161, 1583], "res": [{"text": "Our methodology has several significant limitations. De-", "confidence": 0.98005610704422, "text_region": [[216.0, 825.0], [1164.0, 825.0], [1164.0, 871.0], [216.0, 871.0]]}, {"text": "spite our focus on zero-shot transfer, we repeatedly queried", "confidence": 0.997809648513794, "text_region": [[216.0, 874.0], [1164.0, 874.0], [1164.0, 921.0], [216.0, 921.0]]}, {"text": "performance on full validation sets to guide the develop-", "confidence": 0.9749549627304077, "text_region": [[216.0, 917.0], [1171.0, 917.0], [1171.0, 973.0], [216.0, 973.0]]}, {"text": "ment of CLIP. These validation sets often have thousands", "confidence": 0.9909505248069763, "text_region": [[220.0, 967.0], [1164.0, 967.0], [1164.0, 1013.0], [220.0, 1013.0]]}, {"text": "of examples, which is unrealistic for true zero-shot sce-", "confidence": 0.9918200373649597, "text_region": [[216.0, 1013.0], [1164.0, 1013.0], [1164.0, 1059.0], [216.0, 1059.0]]}, {"text": "narios. Similar concerns have been raised in the field of", "confidence": 0.9731835722923279, "text_region": [[220.0, 1063.0], [1164.0, 1063.0], [1164.0, 1106.0], [220.0, 1106.0]]}, {"text": "semi-supervised learning (Oliver et al., 2018). Another po-", "confidence": 0.9871217012405396, "text_region": [[216.0, 1112.0], [1171.0, 1112.0], [1171.0, 1158.0], [216.0, 1158.0]]}, {"text": "tential issue is our selection of evaluation datasets. While", "confidence": 0.9872402548789978, "text_region": [[220.0, 1162.0], [1164.0, 1162.0], [1164.0, 1204.0], [220.0, 1204.0]]}, {"text": "we have reported results on Kornblith et al. (2019)'s 12", "confidence": 0.9937490820884705, "text_region": [[213.0, 1208.0], [1167.0, 1201.0], [1168.0, 1254.0], [213.0, 1261.0]]}, {"text": "dataset evaluation suite as a standardized collection, our", "confidence": 0.9971974492073059, "text_region": [[220.0, 1254.0], [1164.0, 1254.0], [1164.0, 1300.0], [220.0, 1300.0]]}, {"text": "main results use a somewhat haphazardly assembled col-", "confidence": 0.9941118955612183, "text_region": [[220.0, 1307.0], [1164.0, 1307.0], [1164.0, 1350.0], [220.0, 1350.0]]}, {"text": "lection of 27 datasets that is undeniably co-adapted with", "confidence": 0.9927941560745239, "text_region": [[220.0, 1353.0], [1164.0, 1353.0], [1164.0, 1396.0], [220.0, 1396.0]]}, {"text": "the development and capabilities of CLIP. Creating a new", "confidence": 0.9817463755607605, "text_region": [[220.0, 1399.0], [1164.0, 1399.0], [1164.0, 1445.0], [220.0, 1445.0]]}, {"text": "benchmark of tasks designed explicitly to evaluate broad", "confidence": 0.9934529662132263, "text_region": [[213.0, 1439.0], [1164.0, 1442.0], [1164.0, 1498.0], [213.0, 1495.0]]}, {"text": " zero-shot transfer capabilities, rather than re-using existing", "confidence": 0.987332820892334, "text_region": [[213.0, 1491.0], [1161.0, 1498.0], [1161.0, 1545.0], [213.0, 1538.0]]}, {"text": " supervised datasets, would help address these issues.", "confidence": 0.9888951778411865, "text_region": [[209.0, 1538.0], [1068.0, 1534.0], [1068.0, 1591.0], [210.0, 1594.0]]}], "img_idx": 0, "score": 0.9939371347427368}
{"type": "text", "bbox": [1228, 2343, 2169, 2852], "res": [{"text": "We have also sought to characterize the social biases inher-", "confidence": 0.984668493270874, "text_region": [[1224.0, 2333.0], [2172.0, 2333.0], [2172.0, 2379.0], [1224.0, 2379.0]]}, {"text": "ent to the model. Our bias tests represent our initial efforts", "confidence": 0.9991903901100159, "text_region": [[1227.0, 2386.0], [2172.0, 2386.0], [2172.0, 2432.0], [1227.0, 2432.0]]}, {"text": "to probe aspects of how the model responds in different sce-", "confidence": 0.9887276887893677, "text_region": [[1224.0, 2435.0], [2179.0, 2435.0], [2179.0, 2482.0], [1224.0, 2482.0]]}, {"text": "narios, and are by nature limited in scope. CLIP and models ", "confidence": 0.9848390817642212, "text_region": [[1224.0, 2485.0], [2172.0, 2485.0], [2172.0, 2531.0], [1224.0, 2531.0]]}, {"text": "like it will need to be analyzed in relation to their specific", "confidence": 0.9951428771018982, "text_region": [[1221.0, 2524.0], [2169.0, 2528.0], [2169.0, 2577.0], [1221.0, 2574.0]]}, {"text": "deployments to understand how bias manifests and iden-", "confidence": 0.9932300448417664, "text_region": [[1227.0, 2577.0], [2169.0, 2577.0], [2169.0, 2624.0], [1227.0, 2624.0]]}, {"text": "tify potential interventions. Further community exploration", "confidence": 0.9986540079116821, "text_region": [[1227.0, 2627.0], [2169.0, 2627.0], [2169.0, 2673.0], [1227.0, 2673.0]]}, {"text": "will be required to develop broader, more contextual, and", "confidence": 0.9922558069229126, "text_region": [[1221.0, 2673.0], [2165.0, 2673.0], [2165.0, 2719.0], [1221.0, 2719.0]]}, {"text": "more robust testing schemes so that AI developers can bet-", "confidence": 0.9967579245567322, "text_region": [[1224.0, 2722.0], [2175.0, 2722.0], [2175.0, 2769.0], [1224.0, 2769.0]]}, {"text": "ter characterize biases in general purpose computer vision", "confidence": 0.9974238872528076, "text_region": [[1224.0, 2772.0], [2172.0, 2772.0], [2172.0, 2818.0], [1224.0, 2818.0]]}, {"text": "models.", "confidence": 0.9978767037391663, "text_region": [[1227.0, 2822.0], [1350.0, 2822.0], [1350.0, 2858.0], [1227.0, 2858.0]]}], "img_idx": 0, "score": 0.992621123790741}
{"type": "text", "bbox": [219, 280, 1158, 796], "res": [{"text": "CLIP also does not address the poor data efficiency of deep", "confidence": 0.9951746463775635, "text_region": [[216.0, 271.0], [1161.0, 274.0], [1161.0, 320.0], [216.0, 317.0]]}, {"text": "learning. Instead CLIP compensates by using a source of", "confidence": 0.9950425624847412, "text_region": [[220.0, 323.0], [1164.0, 323.0], [1164.0, 370.0], [220.0, 370.0]]}, {"text": " supervision that can be scaled to hundreds of millions of", "confidence": 0.9787046909332275, "text_region": [[213.0, 373.0], [1167.0, 370.0], [1168.0, 416.0], [213.0, 419.0]]}, {"text": "training examples. If every image seen during training of", "confidence": 0.9983183741569519, "text_region": [[216.0, 422.0], [1164.0, 422.0], [1164.0, 469.0], [216.0, 469.0]]}, {"text": "a CLIP model was presented at a rate of one per second,", "confidence": 0.9938995242118835, "text_region": [[210.0, 459.0], [1168.0, 462.0], [1167.0, 518.0], [209.0, 515.0]]}, {"text": "it would take 405 years to iterate through the 12.8 billion", "confidence": 0.9906637072563171, "text_region": [[220.0, 515.0], [1164.0, 515.0], [1164.0, 561.0], [220.0, 561.0]]}, {"text": "images seen over 32 training epochs. Combining CLIP", "confidence": 0.9805237054824829, "text_region": [[216.0, 558.0], [1164.0, 558.0], [1164.0, 614.0], [216.0, 614.0]]}, {"text": "with self-supervision (Henaff, 2020; Chen et al., 2020c) and", "confidence": 0.9910643100738525, "text_region": [[216.0, 607.0], [1161.0, 607.0], [1161.0, 653.0], [216.0, 653.0]]}, {"text": "self-training (Lee; Xie et al., 2020) methods is a promising", "confidence": 0.9933510422706604, "text_region": [[216.0, 660.0], [1167.0, 660.0], [1167.0, 706.0], [216.0, 706.0]]}, {"text": "direction given their demonstrated ability to improve data", "confidence": 0.9927381277084351, "text_region": [[220.0, 706.0], [1164.0, 706.0], [1164.0, 752.0], [220.0, 752.0]]}, {"text": " efficiency over standard supervised learning.", "confidence": 0.982052206993103, "text_region": [[210.0, 746.0], [938.0, 753.0], [938.0, 809.0], [209.0, 802.0]]}], "img_idx": 0, "score": 0.9922227263450623}
{"type": "text", "bbox": [1227, 1649, 2168, 2300], "res": [{"text": "In addition to the more than 30 datasets studied in earlier", "confidence": 0.9990132451057434, "text_region": [[1224.0, 1643.0], [2169.0, 1643.0], [2169.0, 1690.0], [1224.0, 1690.0]]}, {"text": "sections of this paper, we evaluate CLIP's performance on", "confidence": 0.999146044254303, "text_region": [[1227.0, 1693.0], [2172.0, 1693.0], [2172.0, 1739.0], [1227.0, 1739.0]]}, {"text": "the FairFace benchmark and undertake exploratory bias", "confidence": 0.9865133762359619, "text_region": [[1217.0, 1732.0], [2175.0, 1736.0], [2175.0, 1792.0], [1217.0, 1789.0]]}, {"text": "probes. We then characterize the model's performance in", "confidence": 0.9937760829925537, "text_region": [[1224.0, 1789.0], [2172.0, 1789.0], [2172.0, 1835.0], [1224.0, 1835.0]]}, {"text": "a downstream task, surveillance, and discuss its usefulness", "confidence": 0.9883247017860413, "text_region": [[1224.0, 1835.0], [2169.0, 1835.0], [2169.0, 1881.0], [1224.0, 1881.0]]}, {"text": "as compared with other available systems. Many of CLIP's", "confidence": 0.9830083250999451, "text_region": [[1224.0, 1884.0], [2169.0, 1884.0], [2169.0, 1930.0], [1224.0, 1930.0]]}, {"text": " capabilities are omni-use in nature (e.g. OCR can be used", "confidence": 0.9896102547645569, "text_region": [[1217.0, 1924.0], [2179.0, 1927.0], [2178.0, 1983.0], [1217.0, 1980.0]]}, {"text": "to make scanned documents searchable, to power screen", "confidence": 0.995608925819397, "text_region": [[1221.0, 1977.0], [2172.0, 1980.0], [2172.0, 2026.0], [1221.0, 2023.0]]}, {"text": "reading technologies, or to read license plates). Several", "confidence": 0.9893546104431152, "text_region": [[1224.0, 2026.0], [2172.0, 2026.0], [2172.0, 2072.0], [1224.0, 2072.0]]}, {"text": "of the capabilities measured, from action recognition, ob-", "confidence": 0.9885759949684143, "text_region": [[1224.0, 2076.0], [2175.0, 2076.0], [2175.0, 2119.0], [1224.0, 2119.0]]}, {"text": "ject classification, and geo-localization, to facial emotion", "confidence": 0.9950487017631531, "text_region": [[1224.0, 2122.0], [2172.0, 2122.0], [2172.0, 2168.0], [1224.0, 2168.0]]}, {"text": "recognition, can be used in surveillance. Given its social", "confidence": 0.9881979823112488, "text_region": [[1224.0, 2171.0], [2172.0, 2171.0], [2172.0, 2218.0], [1224.0, 2218.0]]}, {"text": "implications, we address this domain of use specifically in", "confidence": 0.9848740696907043, "text_region": [[1224.0, 2221.0], [2172.0, 2221.0], [2172.0, 2264.0], [1224.0, 2264.0]]}, {"text": "the Surveillance section.", "confidence": 0.9973436594009399, "text_region": [[1224.0, 2264.0], [1620.0, 2264.0], [1620.0, 2310.0], [1224.0, 2310.0]]}], "img_idx": 0, "score": 0.9913240671157837}
{"type": "text", "bbox": [217, 1977, 1160, 2632], "res": [{"text": "While we have emphasized throughout this work that speci-", "confidence": 0.9982261657714844, "text_region": [[220.0, 1973.0], [1161.0, 1973.0], [1161.0, 2020.0], [220.0, 2020.0]]}, {"text": "fying image classifiers through natural language is a fexible", "confidence": 0.9943174123764038, "text_region": [[220.0, 2023.0], [1164.0, 2023.0], [1164.0, 2069.0], [220.0, 2069.0]]}, {"text": "and general interface, it has its own limitations. Many com-", "confidence": 0.9941129684448242, "text_region": [[220.0, 2069.0], [1164.0, 2069.0], [1164.0, 2115.0], [220.0, 2115.0]]}, {"text": "plex tasks and visual concepts can be difficult to specify", "confidence": 0.9995629787445068, "text_region": [[216.0, 2119.0], [1161.0, 2119.0], [1161.0, 2165.0], [216.0, 2165.0]]}, {"text": "just through text. Actual training examples are undeniably", "confidence": 0.9879269003868103, "text_region": [[220.0, 2168.0], [1161.0, 2168.0], [1161.0, 2211.0], [220.0, 2211.0]]}, {"text": "useful but CLIP does not optimize for few-shot performance", "confidence": 0.9901048541069031, "text_region": [[220.0, 2214.0], [1161.0, 2214.0], [1161.0, 2260.0], [220.0, 2260.0]]}, {"text": "directly. In our work, we fall back to fitting linear classifiers", "confidence": 0.9955182671546936, "text_region": [[220.0, 2260.0], [1161.0, 2260.0], [1161.0, 2307.0], [220.0, 2307.0]]}, {"text": "on top of CLIP's features. This results in a counter-intuitive", "confidence": 0.9903762340545654, "text_region": [[220.0, 2307.0], [1164.0, 2307.0], [1164.0, 2353.0], [220.0, 2353.0]]}, {"text": "drop in performance when transitioning from a zero-shot", "confidence": 0.9749703407287598, "text_region": [[216.0, 2353.0], [1164.0, 2356.0], [1164.0, 2402.0], [216.0, 2399.0]]}, {"text": "to a few-shot setting. As discussed in Section 4, this is", "confidence": 0.9904218316078186, "text_region": [[216.0, 2406.0], [1164.0, 2406.0], [1164.0, 2452.0], [216.0, 2452.0]]}, {"text": "notably different from human performance which shows a", "confidence": 0.9976730346679688, "text_region": [[220.0, 2452.0], [1164.0, 2452.0], [1164.0, 2498.0], [220.0, 2498.0]]}, {"text": "large increase from a zero to a one shot setting. Future work", "confidence": 0.9949163794517517, "text_region": [[220.0, 2501.0], [1161.0, 2501.0], [1161.0, 2548.0], [220.0, 2548.0]]}, {"text": "is needed to develop methods that combine CLIP's strong", "confidence": 0.984291672706604, "text_region": [[220.0, 2548.0], [1161.0, 2548.0], [1161.0, 2594.0], [220.0, 2594.0]]}, {"text": "zero-shot performance with efficient few-shot learning", "confidence": 0.9990813136100769, "text_region": [[220.0, 2597.0], [1094.0, 2597.0], [1094.0, 2643.0], [220.0, 2643.0]]}], "img_idx": 0, "score": 0.9785077571868896}
{"type": "text", "bbox": [1227, 1197, 2168, 1614], "res": [{"text": "Our studies of CLIP in a zero-shot setting show that the", "confidence": 0.9814709424972534, "text_region": [[1224.0, 1188.0], [2172.0, 1188.0], [2172.0, 1234.0], [1224.0, 1234.0]]}, {"text": " model displays significant promise for widely-applicable", "confidence": 0.9950599074363708, "text_region": [[1221.0, 1238.0], [2169.0, 1234.0], [2169.0, 1280.0], [1221.0, 1284.0]]}, {"text": "tasks like image retrieval or search. For example, it can find", "confidence": 0.9850360155105591, "text_region": [[1221.0, 1280.0], [2175.0, 1280.0], [2175.0, 1336.0], [1221.0, 1336.0]]}, {"text": "relevant images in a database given text, or relevant text", "confidence": 0.9957234859466553, "text_region": [[1227.0, 1333.0], [2172.0, 1333.0], [2172.0, 1379.0], [1227.0, 1379.0]]}, {"text": "given an image. Further, the relative ease of steering CLIP", "confidence": 0.9906845092773438, "text_region": [[1227.0, 1383.0], [2169.0, 1383.0], [2169.0, 1429.0], [1227.0, 1429.0]]}, {"text": "toward bespoke applications with little or no additional data", "confidence": 0.9852468371391296, "text_region": [[1224.0, 1429.0], [2172.0, 1429.0], [2172.0, 1475.0], [1224.0, 1475.0]]}, {"text": "or training could unlock a variety of novel applications that", "confidence": 0.9911954998970032, "text_region": [[1224.0, 1478.0], [2172.0, 1478.0], [2172.0, 1525.0], [1224.0, 1525.0]]}, {"text": " are hard for us to envision today, as has occurred with large", "confidence": 0.9954398274421692, "text_region": [[1217.0, 1518.0], [2172.0, 1521.0], [2172.0, 1577.0], [1217.0, 1574.0]]}, {"text": "language models over the past few years.", "confidence": 0.9987141489982605, "text_region": [[1227.0, 1574.0], [1883.0, 1574.0], [1883.0, 1620.0], [1227.0, 1620.0]]}], "img_idx": 0, "score": 0.9751672744750977}
{"type": "text", "bbox": [219, 1617, 1160, 1939], "res": [{"text": " CLIP is trained on text paired with images on the internet.", "confidence": 0.9714696407318115, "text_region": [[210.0, 1604.0], [1168.0, 1611.0], [1167.0, 1667.0], [209.0, 1660.0]]}, {"text": "These image-text pairs are unfiltered and uncurated and", "confidence": 0.9952529072761536, "text_region": [[220.0, 1663.0], [1161.0, 1663.0], [1161.0, 1709.0], [220.0, 1709.0]]}, {"text": "result in CLIP models learning many social biases. This", "confidence": 0.9831376075744629, "text_region": [[216.0, 1706.0], [1164.0, 1706.0], [1164.0, 1762.0], [216.0, 1762.0]]}, {"text": "has been previously demonstrated for image caption models", "confidence": 0.9981285929679871, "text_region": [[220.0, 1759.0], [1164.0, 1759.0], [1164.0, 1805.0], [220.0, 1805.0]]}, {"text": "(Bhargava & Forsyth, 2019). We refer readers to Section 7", "confidence": 0.9898914098739624, "text_region": [[220.0, 1805.0], [1164.0, 1805.0], [1164.0, 1851.0], [220.0, 1851.0]]}, {"text": "for detailed analysis and quantification of these behaviors for", "confidence": 0.9886621832847595, "text_region": [[216.0, 1855.0], [1164.0, 1855.0], [1164.0, 1901.0], [216.0, 1901.0]]}, {"text": " CLIP as well as discussion of potential mitigation strategies.", "confidence": 0.9853172898292542, "text_region": [[213.0, 1897.0], [1168.0, 1904.0], [1167.0, 1950.0], [213.0, 1944.0]]}], "img_idx": 0, "score": 0.9606506824493408}
{"type": "text", "bbox": [1225, 358, 2170, 1157], "res": [{"text": "CLIP has a wide range of capabilities due to its ability to", "confidence": 0.9957885146141052, "text_region": [[1227.0, 353.0], [2172.0, 353.0], [2172.0, 399.0], [1227.0, 399.0]]}, {"text": "carry out arbitrary image classification tasks. One can give", "confidence": 0.9964764714241028, "text_region": [[1227.0, 403.0], [2169.0, 403.0], [2169.0, 449.0], [1227.0, 449.0]]}, {"text": "it images of cats and dogs and ask it to classify cats, or give", "confidence": 0.9943737387657166, "text_region": [[1224.0, 449.0], [2172.0, 449.0], [2172.0, 495.0], [1224.0, 495.0]]}, {"text": "it images taken in a department store and ask it to classify", "confidence": 0.9863768219947815, "text_region": [[1221.0, 492.0], [2172.0, 492.0], [2172.0, 548.0], [1221.0, 548.0]]}, {"text": "shoplifters-a task with significant social implications and", "confidence": 0.9902751445770264, "text_region": [[1224.0, 544.0], [2172.0, 544.0], [2172.0, 591.0], [1224.0, 591.0]]}, {"text": " for which AI may be unfit. Like any image classification", "confidence": 0.9767658114433289, "text_region": [[1217.0, 584.0], [2175.0, 587.0], [2175.0, 644.0], [1217.0, 640.0]]}, {"text": "system, CLIP's performance and fitness for purpose need to", "confidence": 0.9848851561546326, "text_region": [[1221.0, 637.0], [2175.0, 634.0], [2175.0, 690.0], [1221.0, 693.0]]}, {"text": " be evaluated, and its broader impacts analyzed in context.", "confidence": 0.9919065237045288, "text_region": [[1217.0, 680.0], [2179.0, 683.0], [2178.0, 739.0], [1217.0, 736.0]]}, {"text": "CLIP also introduces a capability that will magnify and alter", "confidence": 0.9951915144920349, "text_region": [[1224.0, 736.0], [2172.0, 736.0], [2172.0, 782.0], [1224.0, 782.0]]}, {"text": "such issues: CLIP makes it possible to easily create your", "confidence": 0.9936556220054626, "text_region": [[1221.0, 775.0], [2172.0, 779.0], [2172.0, 835.0], [1221.0, 832.0]]}, {"text": "own classes for categorization (to \u2018roll your own classifier')", "confidence": 0.977928638458252, "text_region": [[1227.0, 832.0], [2172.0, 832.0], [2172.0, 878.0], [1227.0, 878.0]]}, {"text": "without a need for re-training. This capability introduces", "confidence": 0.9993379712104797, "text_region": [[1221.0, 875.0], [2172.0, 871.0], [2172.0, 927.0], [1221.0, 931.0]]}, {"text": "challenges similar to those found in characterizing other,", "confidence": 0.9963867664337158, "text_region": [[1224.0, 927.0], [2175.0, 927.0], [2175.0, 973.0], [1224.0, 973.0]]}, {"text": " large-scale generative models like GPT-3 (Brown et al.,", "confidence": 0.9887629151344299, "text_region": [[1217.0, 970.0], [2178.0, 967.0], [2179.0, 1023.0], [1217.0, 1026.0]]}, {"text": "2020); models that exhibit non-trivial zero-shot (or few-", "confidence": 0.9870491027832031, "text_region": [[1224.0, 1020.0], [2172.0, 1020.0], [2172.0, 1066.0], [1224.0, 1066.0]]}, {"text": " shot) generalization can have a vast range of capabilities,", "confidence": 0.9906122088432312, "text_region": [[1217.0, 1063.0], [2179.0, 1066.0], [2178.0, 1122.0], [1217.0, 1119.0]]}, {"text": "many of which are made clear only after testing for them.", "confidence": 0.9921576976776123, "text_region": [[1224.0, 1119.0], [2149.0, 1119.0], [2149.0, 1165.0], [1224.0, 1165.0]]}], "img_idx": 0, "score": 0.9560129642486572}
{"type": "title", "bbox": [1229, 275, 1623, 315], "res": [{"text": "7. Broader Impacts", "confidence": 0.9958523511886597, "text_region": [[1221.0, 260.0], [1630.0, 268.0], [1629.0, 324.0], [1220.0, 316.0]]}], "img_idx": 0, "score": 0.9475932717323303}
{"type": "header", "bbox": [2128, 192, 2166, 217], "res": [{"text": "20", "confidence": 0.9997494220733643, "text_region": [[2125.0, 191.0], [2169.0, 191.0], [2169.0, 221.0], [2125.0, 221.0]]}], "img_idx": 0, "score": 0.9087081551551819}
{"type": "header", "bbox": [292, 193, 1189, 225], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9977167248725891, "text_region": [[220.0, 188.0], [1370.0, 188.0], [1370.0, 231.0], [220.0, 231.0]]}], "img_idx": 0, "score": 0.7358250617980957}
