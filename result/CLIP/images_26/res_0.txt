{"type": "text", "bbox": [219, 1981, 1161, 2731], "res": [{"text": "We'd like to thank the millions of people involved in creating", "confidence": 0.9893544316291809, "text_region": [[213.0, 1967.0], [1164.0, 1970.0], [1164.0, 2026.0], [213.0, 2023.0]]}, {"text": "the data CLIP is trained on. We'd also like to thank Susan", "confidence": 0.9878236055374146, "text_region": [[220.0, 2023.0], [1157.0, 2023.0], [1157.0, 2069.0], [220.0, 2069.0]]}, {"text": "Zhang for her work on image conditional language models", "confidence": 0.9994333386421204, "text_region": [[220.0, 2069.0], [1157.0, 2069.0], [1157.0, 2115.0], [220.0, 2115.0]]}, {"text": "while at OpenAI, Ishaan Gulrajani for catching an error in", "confidence": 0.9826033115386963, "text_region": [[213.0, 2112.0], [1161.0, 2115.0], [1161.0, 2171.0], [213.0, 2168.0]]}, {"text": "the pseudocode, and Irene Solaiman, Miles Brundage, and", "confidence": 0.9973816871643066, "text_region": [[220.0, 2168.0], [1161.0, 2168.0], [1161.0, 2214.0], [220.0, 2214.0]]}, {"text": "Gillian Hadfield for their thoughtful feedback on the broader", "confidence": 0.9994959831237793, "text_region": [[220.0, 2218.0], [1161.0, 2218.0], [1161.0, 2260.0], [220.0, 2260.0]]}, {"text": "impacts section of the paper. We are also grateful to the", "confidence": 0.9896339178085327, "text_region": [[220.0, 2264.0], [1154.0, 2264.0], [1154.0, 2310.0], [220.0, 2310.0]]}, {"text": "Acceleration and Supercomputing teams at OpenAI for their", "confidence": 0.9871423840522766, "text_region": [[220.0, 2313.0], [1154.0, 2313.0], [1154.0, 2360.0], [220.0, 2360.0]]}, {"text": "critical work on software and hardware infrastructure this", "confidence": 0.9994615316390991, "text_region": [[220.0, 2360.0], [1157.0, 2360.0], [1157.0, 2406.0], [220.0, 2406.0]]}, {"text": "project used. Finally, we'd also like to thank the developers", "confidence": 0.9922091364860535, "text_region": [[216.0, 2409.0], [1157.0, 2409.0], [1157.0, 2455.0], [216.0, 2455.0]]}, {"text": "of the many software packages used throughout this project", "confidence": 0.987766444683075, "text_region": [[216.0, 2455.0], [1161.0, 2455.0], [1161.0, 2501.0], [216.0, 2501.0]]}, {"text": "including, but not limited, to Numpy (Harris et al., 2020),", "confidence": 0.9949377775192261, "text_region": [[220.0, 2498.0], [1161.0, 2498.0], [1161.0, 2544.0], [220.0, 2544.0]]}, {"text": "SciPy (Virtanen et al., 2020), ftfy (Speer, 2019), Tensor-", "confidence": 0.9895171523094177, "text_region": [[216.0, 2544.0], [1164.0, 2551.0], [1164.0, 2597.0], [216.0, 2590.0]]}, {"text": "Flow (Abadi et al., 2016), PyTorch (Paszke et al., 2019),", "confidence": 0.9914703369140625, "text_region": [[220.0, 2597.0], [1164.0, 2597.0], [1164.0, 2643.0], [220.0, 2643.0]]}, {"text": "pandas (pandas development team, 2020), and scikit-learn", "confidence": 0.9990611672401428, "text_region": [[216.0, 2647.0], [1164.0, 2647.0], [1164.0, 2693.0], [216.0, 2693.0]]}, {"text": "(Pedregosa et al., 2011).", "confidence": 0.9908855557441711, "text_region": [[216.0, 2696.0], [609.0, 2696.0], [609.0, 2739.0], [216.0, 2739.0]]}], "img_idx": 0, "score": 0.9943848848342896}
{"type": "text", "bbox": [218, 279, 1159, 1125], "res": [{"text": "& Bansal, 2019; Chen et al., 2019; Li et al., 2020b; Yu et al.", "confidence": 0.9806774258613586, "text_region": [[220.0, 274.0], [1161.0, 274.0], [1161.0, 317.0], [220.0, 317.0]]}, {"text": "2020). This line of work focuses on richly connecting vision", "confidence": 0.9909336566925049, "text_region": [[220.0, 323.0], [1161.0, 323.0], [1161.0, 370.0], [220.0, 370.0]]}, {"text": "and language in order to solve complex downstream tasks", "confidence": 0.9957127571105957, "text_region": [[220.0, 373.0], [1157.0, 373.0], [1157.0, 419.0], [220.0, 419.0]]}, {"text": "such as visual question answering, visual commonsense", "confidence": 0.9993438124656677, "text_region": [[220.0, 419.0], [1154.0, 419.0], [1154.0, 465.0], [220.0, 465.0]]}, {"text": "reasoning, or multimodal entailment. These approaches", "confidence": 0.9891602993011475, "text_region": [[220.0, 465.0], [1157.0, 465.0], [1157.0, 521.0], [220.0, 521.0]]}, {"text": "leverage impressively engineered models which combine 3", "confidence": 0.9996797442436218, "text_region": [[223.0, 515.0], [1154.0, 515.0], [1154.0, 561.0], [223.0, 561.0]]}, {"text": "(or more) pre-trained subsystems, typically an image feature", "confidence": 0.989344596862793, "text_region": [[220.0, 564.0], [1157.0, 564.0], [1157.0, 610.0], [220.0, 610.0]]}, {"text": "model, a region proposal / object detection model, and a", "confidence": 0.9957875609397888, "text_region": [[220.0, 614.0], [1157.0, 614.0], [1157.0, 657.0], [220.0, 657.0]]}, {"text": "pre-trained masked language model such as BERT. These", "confidence": 0.9986651539802551, "text_region": [[220.0, 660.0], [1154.0, 660.0], [1154.0, 706.0], [220.0, 706.0]]}, {"text": " systems are then jointly fine-tuned via various training objec-", "confidence": 0.986727237701416, "text_region": [[209.0, 703.0], [1161.0, 700.0], [1161.0, 756.0], [210.0, 759.0]]}, {"text": "tives on image-text pairs and applied to the aforementioned", "confidence": 0.9917052984237671, "text_region": [[220.0, 756.0], [1161.0, 756.0], [1161.0, 802.0], [220.0, 802.0]]}, {"text": "tasks and achieve impressive results. CLIP is instead fo-", "confidence": 0.9864778518676758, "text_region": [[220.0, 805.0], [1157.0, 805.0], [1157.0, 848.0], [220.0, 848.0]]}, {"text": "cused on learning visual models from scratch via natural", "confidence": 0.988287091255188, "text_region": [[220.0, 851.0], [1161.0, 851.0], [1161.0, 898.0], [220.0, 898.0]]}, {"text": "language supervision and does not densely connect the two", "confidence": 0.9962485432624817, "text_region": [[220.0, 901.0], [1157.0, 901.0], [1157.0, 947.0], [220.0, 947.0]]}, {"text": "domains with a joint attention model. The only interaction", "confidence": 0.993618369102478, "text_region": [[216.0, 944.0], [1158.0, 947.0], [1157.0, 993.0], [216.0, 990.0]]}, {"text": "in a CLIP model between the image and text domain is a", "confidence": 0.9989323616027832, "text_region": [[220.0, 993.0], [1157.0, 993.0], [1157.0, 1040.0], [220.0, 1040.0]]}, {"text": "single dot product in a learned joint embedding space. We", "confidence": 0.9853726625442505, "text_region": [[216.0, 1040.0], [1164.0, 1040.0], [1164.0, 1096.0], [216.0, 1096.0]]}, {"text": "are excited to see CLIP hybridized with this line of work.", "confidence": 0.988875150680542, "text_region": [[216.0, 1089.0], [1134.0, 1089.0], [1134.0, 1135.0], [216.0, 1135.0]]}], "img_idx": 0, "score": 0.9941378235816956}
{"type": "text", "bbox": [220, 1286, 1161, 1849], "res": [{"text": "We have investigated whether it is possible to transfer the", "confidence": 0.9874541759490967, "text_region": [[216.0, 1280.0], [1161.0, 1280.0], [1161.0, 1327.0], [216.0, 1327.0]]}, {"text": "success of task-agnostic web-scale pre-training in NLP to", "confidence": 0.9876377582550049, "text_region": [[220.0, 1330.0], [1161.0, 1330.0], [1161.0, 1376.0], [220.0, 1376.0]]}, {"text": "another domain. We find that adopting this formula re-", "confidence": 0.983543336391449, "text_region": [[216.0, 1376.0], [1161.0, 1376.0], [1161.0, 1422.0], [216.0, 1422.0]]}, {"text": "sults in similar behaviors emerging in the field of computer", "confidence": 0.9811177849769592, "text_region": [[220.0, 1426.0], [1161.0, 1426.0], [1161.0, 1472.0], [220.0, 1472.0]]}, {"text": "vision and discuss the social implications of this line of", "confidence": 0.9967788457870483, "text_region": [[220.0, 1475.0], [1164.0, 1475.0], [1164.0, 1518.0], [220.0, 1518.0]]}, {"text": "research. In order to optimize their training objective, CLIP", "confidence": 0.9914189577102661, "text_region": [[223.0, 1521.0], [1161.0, 1521.0], [1161.0, 1568.0], [223.0, 1568.0]]}, {"text": "models learn to perform a wide variety of tasks during pre-", "confidence": 0.9849077463150024, "text_region": [[220.0, 1567.0], [1158.0, 1571.0], [1157.0, 1617.0], [219.0, 1614.0]]}, {"text": "training. This task learning can then be leveraged via natural", "confidence": 0.9922477006912231, "text_region": [[220.0, 1617.0], [1157.0, 1617.0], [1157.0, 1663.0], [220.0, 1663.0]]}, {"text": "language prompting to enable zero-shot transfer to many", "confidence": 0.9977491497993469, "text_region": [[220.0, 1660.0], [1164.0, 1660.0], [1164.0, 1716.0], [220.0, 1716.0]]}, {"text": "existing datasets. At sufficient scale, the performance of this", "confidence": 0.9960458278656006, "text_region": [[220.0, 1713.0], [1161.0, 1713.0], [1161.0, 1756.0], [220.0, 1756.0]]}, {"text": "approach can be competitive with task-specific supervised", "confidence": 0.9931820034980774, "text_region": [[213.0, 1756.0], [1161.0, 1752.0], [1161.0, 1808.0], [213.0, 1812.0]]}, {"text": "models although there is still room for much improvement.", "confidence": 0.9945785999298096, "text_region": [[220.0, 1808.0], [1161.0, 1808.0], [1161.0, 1855.0], [220.0, 1855.0]]}], "img_idx": 0, "score": 0.9931976795196533}
{"type": "text", "bbox": [1241, 1705, 2167, 1931], "res": [{"text": "Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gut-", "confidence": 0.9669766426086426, "text_region": [[1227.0, 1703.0], [2172.0, 1703.0], [2172.0, 1746.0], [1227.0, 1746.0]]}, {"text": "freund, D., Tenenbaum, J., and Katz, B. Objectnet: A", "confidence": 0.9925917387008667, "text_region": [[1267.0, 1746.0], [2172.0, 1746.0], [2172.0, 1792.0], [1267.0, 1792.0]]}, {"text": "large-scale bias-controlled dataset for pushing the lim-", "confidence": 0.9889166951179504, "text_region": [[1267.0, 1795.0], [2172.0, 1795.0], [2172.0, 1841.0], [1267.0, 1841.0]]}, {"text": "its of object recognition models. In Advances in Neural", "confidence": 0.9906740784645081, "text_region": [[1267.0, 1848.0], [2165.0, 1848.0], [2165.0, 1891.0], [1267.0, 1891.0]]}, {"text": "Information Processing Systems, pp. 9453-9463, 2019.", "confidence": 0.9854932427406311, "text_region": [[1264.0, 1891.0], [2152.0, 1891.0], [2152.0, 1947.0], [1264.0, 1947.0]]}], "img_idx": 0, "score": 0.9898062944412231}
{"type": "text", "bbox": [1233, 864, 2169, 1093], "res": [{"text": "Alcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-", "confidence": 0.9910597205162048, "text_region": [[1231.0, 865.0], [2172.0, 865.0], [2172.0, 908.0], [1231.0, 908.0]]}, {"text": "S., and Nguyen, A. Strike (with) a pose: Neural networks", "confidence": 0.9933924078941345, "text_region": [[1264.0, 914.0], [2169.0, 914.0], [2169.0, 960.0], [1264.0, 960.0]]}, {"text": "are easily fooled by strange poses of familiar objects. In", "confidence": 0.9910445809364319, "text_region": [[1264.0, 957.0], [2172.0, 957.0], [2172.0, 1013.0], [1264.0, 1013.0]]}, {"text": "Proceedings of the IEEE Conference on Computer Vision", "confidence": 0.9873583316802979, "text_region": [[1267.0, 1003.0], [2169.0, 1003.0], [2169.0, 1049.0], [1267.0, 1049.0]]}, {"text": "and Pattern Rec0gnition, pp. 4845-4854, 2019.", "confidence": 0.9834998250007629, "text_region": [[1267.0, 1056.0], [2026.0, 1056.0], [2026.0, 1102.0], [1267.0, 1102.0]]}], "img_idx": 0, "score": 0.9865231513977051}
{"type": "text", "bbox": [1240, 1473, 2166, 1650], "res": [{"text": "Bachman, P., Hjelm, R. D., and Buchwalter, W. Learning", "confidence": 0.9940594434738159, "text_region": [[1224.0, 1465.0], [2169.0, 1469.0], [2168.0, 1515.0], [1224.0, 1511.0]]}, {"text": "representations by maximizing mutual information across", "confidence": 0.9970231056213379, "text_region": [[1264.0, 1518.0], [2165.0, 1515.0], [2165.0, 1561.0], [1264.0, 1564.0]]}, {"text": "views. In Advances in Neural Information Processing", "confidence": 0.9844322800636292, "text_region": [[1264.0, 1564.0], [2169.0, 1568.0], [2168.0, 1614.0], [1264.0, 1610.0]]}, {"text": "Systems, pp. 15535-15545, 2019.", "confidence": 0.9850744605064392, "text_region": [[1260.0, 1611.0], [1809.0, 1604.0], [1810.0, 1660.0], [1261.0, 1667.0]]}], "img_idx": 0, "score": 0.9851518273353577}
{"type": "text", "bbox": [1243, 1987, 2170, 2160], "res": [{"text": "Barnard, K., Duygulu, P., Forsyth, D., Freitas, N. d., Blei,", "confidence": 0.9871693253517151, "text_region": [[1227.0, 1983.0], [2169.0, 1983.0], [2169.0, 2030.0], [1227.0, 2030.0]]}, {"text": "D. M., and Jordan, M. I. Matching words and pictures.", "confidence": 0.9867156147956848, "text_region": [[1264.0, 2033.0], [2169.0, 2033.0], [2169.0, 2079.0], [1264.0, 2079.0]]}, {"text": "Journal of machine learning research, 3(Feb):1107-1135,", "confidence": 0.982284426689148, "text_region": [[1264.0, 2079.0], [2172.0, 2079.0], [2172.0, 2125.0], [1264.0, 2125.0]]}, {"text": "2003.", "confidence": 0.9999575614929199, "text_region": [[1264.0, 2125.0], [1364.0, 2125.0], [1364.0, 2171.0], [1264.0, 2171.0]]}], "img_idx": 0, "score": 0.982170581817627}
{"type": "text", "bbox": [1227, 1287, 2165, 1410], "res": [{"text": "Assiri, Y. Stochastic optimization of plain convolutional", "confidence": 0.9886232614517212, "text_region": [[1227.0, 1284.0], [2169.0, 1284.0], [2169.0, 1327.0], [1227.0, 1327.0]]}, {"text": "neural networks with simple methods. arXiv preprint", "confidence": 0.9993211627006531, "text_region": [[1267.0, 1333.0], [2172.0, 1333.0], [2172.0, 1379.0], [1267.0, 1379.0]]}, {"text": "arXiv:2001.08856, 2020.", "confidence": 0.9991798996925354, "text_region": [[1267.0, 1379.0], [1673.0, 1379.0], [1673.0, 1422.0], [1267.0, 1422.0]]}], "img_idx": 0, "score": 0.9769725203514099}
{"type": "text", "bbox": [1235, 2734, 2170, 2865], "res": [{"text": "Bhargava, S. and Forsyth, D. Exposing and correcting the", "confidence": 0.9991011023521423, "text_region": [[1227.0, 2732.0], [2169.0, 2732.0], [2169.0, 2779.0], [1227.0, 2779.0]]}, {"text": "gender bias in image captioning datasets and models.", "confidence": 0.9984818696975708, "text_region": [[1267.0, 2782.0], [2169.0, 2782.0], [2169.0, 2828.0], [1267.0, 2828.0]]}, {"text": "arXiv preprint arXiv:1912.00578, 2019.", "confidence": 0.9997021555900574, "text_region": [[1267.0, 2828.0], [1909.0, 2828.0], [1909.0, 2874.0], [1267.0, 2874.0]]}], "img_idx": 0, "score": 0.9730744361877441}
{"type": "text", "bbox": [1232, 631, 2169, 810], "res": [{"text": "Alayrac, J.-B., Recasens, A., Schneider, R., Arandjelovic,", "confidence": 0.9978374242782593, "text_region": [[1231.0, 630.0], [2169.0, 630.0], [2169.0, 673.0], [1231.0, 673.0]]}, {"text": " R., Ramapuram, J., De Fauw, J., Smaira, L., Dieleman, S.,", "confidence": 0.9751913547515869, "text_region": [[1257.0, 673.0], [2172.0, 670.0], [2172.0, 726.0], [1257.0, 729.0]]}, {"text": "and Zisserman, A. Self-supervised multimodal versatile", "confidence": 0.9826921820640564, "text_region": [[1267.0, 726.0], [2169.0, 726.0], [2169.0, 769.0], [1267.0, 769.0]]}, {"text": "networks. arXiv preprint arXiv:2006.16228, 2020.", "confidence": 0.9922589659690857, "text_region": [[1267.0, 776.0], [2079.0, 776.0], [2079.0, 822.0], [1267.0, 822.0]]}], "img_idx": 0, "score": 0.9685789942741394}
{"type": "text", "bbox": [1230, 1145, 2166, 1231], "res": [{"text": "Andreas, J., Klein, D., and Levine, S. Learning with latent", "confidence": 0.9848595857620239, "text_region": [[1231.0, 1145.0], [2172.0, 1145.0], [2172.0, 1191.0], [1231.0, 1191.0]]}, {"text": "language. arXiv preprint arXiv:1711.00482, 2017.", "confidence": 0.9834585189819336, "text_region": [[1264.0, 1195.0], [2075.0, 1195.0], [2075.0, 1241.0], [1264.0, 1241.0]]}], "img_idx": 0, "score": 0.9491202235221863}
{"type": "text", "bbox": [224, 1908, 590, 1935], "res": [{"text": "ACKNOWLEDGMENTS", "confidence": 0.9991788268089294, "text_region": [[220.0, 1897.0], [596.0, 1901.0], [595.0, 1947.0], [219.0, 1944.0]]}], "img_idx": 0, "score": 0.9194253087043762}
{"type": "text", "bbox": [1230, 2548, 2171, 2678], "res": [{"text": "Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. A", "confidence": 0.9985644221305847, "text_region": [[1227.0, 2548.0], [2172.0, 2548.0], [2172.0, 2594.0], [1227.0, 2594.0]]}, {"text": "neural probabilistic language model. Journal of machine", "confidence": 0.9916127324104309, "text_region": [[1267.0, 2597.0], [2169.0, 2597.0], [2169.0, 2643.0], [1267.0, 2643.0]]}, {"text": "learning research, 3(Feb):1137-1155, 2003.", "confidence": 0.9987362623214722, "text_region": [[1267.0, 2643.0], [1976.0, 2643.0], [1976.0, 2690.0], [1267.0, 2690.0]]}], "img_idx": 0, "score": 0.9179092645645142}
{"type": "text", "bbox": [1232, 350, 2170, 578], "res": [{"text": "Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean", "confidence": 0.968010425567627, "text_region": [[1224.0, 346.0], [2165.0, 346.0], [2165.0, 393.0], [1224.0, 393.0]]}, {"text": "J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.", "confidence": 0.9790857434272766, "text_region": [[1264.0, 396.0], [2172.0, 396.0], [2172.0, 442.0], [1264.0, 442.0]]}, {"text": "Tensorflow: A system for large-scale machine learning. In", "confidence": 0.9991343021392822, "text_region": [[1271.0, 446.0], [2169.0, 446.0], [2169.0, 492.0], [1271.0, 492.0]]}, {"text": "12th {USENIX} symposium on operating systems design", "confidence": 0.992916464805603, "text_region": [[1271.0, 492.0], [2172.0, 492.0], [2172.0, 538.0], [1271.0, 538.0]]}, {"text": "and implementation ({OSD1} 16), pp. 265-283, 2016.", "confidence": 0.9783892035484314, "text_region": [[1264.0, 535.0], [2135.0, 535.0], [2135.0, 591.0], [1264.0, 591.0]]}], "img_idx": 0, "score": 0.9177801609039307}
{"type": "title", "bbox": [219, 1201, 497, 1236], "res": [{"text": "9. Conclusion", "confidence": 0.9994560480117798, "text_region": [[214.0, 1191.0], [503.0, 1198.0], [501.0, 1248.0], [212.0, 1240.0]]}], "img_idx": 0, "score": 0.95635586977005}
{"type": "title", "bbox": [1231, 275, 1452, 310], "res": [{"text": "References", "confidence": 0.9987348318099976, "text_region": [[1223.0, 260.0], [1461.0, 271.0], [1458.0, 328.0], [1220.0, 316.0]]}], "img_idx": 0, "score": 0.9382954835891724}
{"type": "header", "bbox": [2127, 191, 2163, 216], "res": [{"text": "27", "confidence": 0.9998511075973511, "text_region": [[2122.0, 188.0], [2169.0, 188.0], [2169.0, 228.0], [2122.0, 228.0]]}], "img_idx": 0, "score": 0.8962669968605042}
{"type": "header", "bbox": [292, 193, 1189, 224], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9898784756660461, "text_region": [[220.0, 188.0], [1364.0, 188.0], [1364.0, 234.0], [220.0, 234.0]]}], "img_idx": 0, "score": 0.7230684757232666}
{"type": "reference", "bbox": [1236, 2218, 2168, 2490], "res": [{"text": "Bechmann, A. and Bowker, G. C. Unsupervised by any", "confidence": 0.9989014267921448, "text_region": [[1221.0, 2211.0], [2169.0, 2214.0], [2168.0, 2270.0], [1221.0, 2267.0]]}, {"text": "other name: Hidden layers of knowledge production in", "confidence": 0.9974827170372009, "text_region": [[1271.0, 2267.0], [2169.0, 2267.0], [2169.0, 2313.0], [1271.0, 2313.0]]}, {"text": "artificial intelligence on social media. Big Data & Society,", "confidence": 0.9884361624717712, "text_region": [[1271.0, 2313.0], [2169.0, 2313.0], [2169.0, 2360.0], [1271.0, 2360.0]]}, {"text": "6(1):205395171881956, January 2019. doi: 10.1177/", "confidence": 0.9962230324745178, "text_region": [[1267.0, 2360.0], [2172.0, 2360.0], [2172.0, 2406.0], [1267.0, 2406.0]]}, {"text": "2053951718819569. URL https: //doi.org/10.", "confidence": 0.9832678437232971, "text_region": [[1261.0, 2402.0], [2179.0, 2406.0], [2178.0, 2462.0], [1260.0, 2458.0]]}, {"text": "1177/2053951718819569.", "confidence": 0.9967502355575562, "text_region": [[1267.0, 2458.0], [1783.0, 2458.0], [1783.0, 2501.0], [1267.0, 2501.0]]}], "img_idx": 0, "score": 0.964749276638031}
