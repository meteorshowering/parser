{"type": "text", "bbox": [1226, 1810, 2170, 2853], "res": [{"text": "Over 20 years ago Mori et al. (1999) explored improving", "confidence": 0.9952251315116882, "text_region": [[1227.0, 1808.0], [2169.0, 1808.0], [2169.0, 1855.0], [1227.0, 1855.0]]}, {"text": "content based image retrieval by training a model to pre-", "confidence": 0.985940158367157, "text_region": [[1224.0, 1851.0], [2175.0, 1851.0], [2175.0, 1907.0], [1224.0, 1907.0]]}, {"text": "dict the nouns and adjectives in text documents paired with", "confidence": 0.9928416013717651, "text_region": [[1227.0, 1904.0], [2172.0, 1904.0], [2172.0, 1950.0], [1227.0, 1950.0]]}, {"text": "images. Quattoni et al. (2007) demonstrated it was possi-", "confidence": 0.9915444254875183, "text_region": [[1221.0, 1947.0], [2179.0, 1947.0], [2179.0, 2003.0], [1221.0, 2003.0]]}, {"text": "able to learn more data efficient image representations via", "confidence": 0.9899843335151672, "text_region": [[1227.0, 2000.0], [2172.0, 2000.0], [2172.0, 2046.0], [1227.0, 2046.0]]}, {"text": "manifold learning in the weight space of classifiers trained", "confidence": 0.9885883331298828, "text_region": [[1224.0, 2043.0], [2175.0, 2043.0], [2175.0, 2099.0], [1224.0, 2099.0]]}, {"text": "to predict words in captions associated with images. Sri-", "confidence": 0.9922383427619934, "text_region": [[1224.0, 2096.0], [2172.0, 2096.0], [2172.0, 2142.0], [1224.0, 2142.0]]}, {"text": "vastava & Salakhutdinov (2012) explored deep represen-", "confidence": 0.9852249622344971, "text_region": [[1221.0, 2135.0], [2179.0, 2138.0], [2178.0, 2195.0], [1221.0, 2191.0]]}, {"text": "tation learning by training multimodal Deep Boltzmann", "confidence": 0.9757940769195557, "text_region": [[1227.0, 2191.0], [2172.0, 2191.0], [2172.0, 2237.0], [1227.0, 2237.0]]}, {"text": "Machines on top of low-level image and text tag features.", "confidence": 0.995023250579834, "text_region": [[1224.0, 2237.0], [2169.0, 2237.0], [2169.0, 2284.0], [1224.0, 2284.0]]}, {"text": "Joulin et al. (2016) modernized this line of work and demon-", "confidence": 0.9993032217025757, "text_region": [[1227.0, 2287.0], [2169.0, 2287.0], [2169.0, 2330.0], [1227.0, 2330.0]]}, {"text": "strated that CNNs trained to predict words in image cap-", "confidence": 0.9949979186058044, "text_region": [[1221.0, 2326.0], [2179.0, 2330.0], [2178.0, 2386.0], [1221.0, 2383.0]]}, {"text": "tions learn useful image representations. They converted", "confidence": 0.9891763925552368, "text_region": [[1227.0, 2383.0], [2172.0, 2383.0], [2172.0, 2429.0], [1227.0, 2429.0]]}, {"text": "the title, description, and hashtag metadata of images in the", "confidence": 0.9929397106170654, "text_region": [[1221.0, 2422.0], [2175.0, 2426.0], [2175.0, 2482.0], [1221.0, 2478.0]]}, {"text": "YFCC100M dataset (Thomee et al., 2016) into a bag-of-", "confidence": 0.989814043045044, "text_region": [[1231.0, 2478.0], [2172.0, 2478.0], [2172.0, 2524.0], [1231.0, 2524.0]]}, {"text": "words multi-label classification task and showed that pre-", "confidence": 0.9900187253952026, "text_region": [[1221.0, 2514.0], [2175.0, 2521.0], [2175.0, 2577.0], [1221.0, 2571.0]]}, {"text": "training AlexNet (Krizhevsky et al., 2012) to predict these", "confidence": 0.9924575686454773, "text_region": [[1224.0, 2574.0], [2169.0, 2574.0], [2169.0, 2620.0], [1224.0, 2620.0]]}, {"text": " labels learned representations which preformed similarly", "confidence": 0.9860708117485046, "text_region": [[1217.0, 2614.0], [2172.0, 2617.0], [2172.0, 2673.0], [1217.0, 2670.0]]}, {"text": "to ImageNet-based pre-training on transfer tasks. Li et al.", "confidence": 0.9897710680961609, "text_region": [[1224.0, 2670.0], [2175.0, 2670.0], [2175.0, 2716.0], [1224.0, 2716.0]]}, {"text": "(2017) then extended this approach to predicting phrase n-", "confidence": 0.9951037764549255, "text_region": [[1221.0, 2709.0], [2179.0, 2713.0], [2178.0, 2769.0], [1221.0, 2765.0]]}, {"text": " grams in addition to individual words and demonstrated the", "confidence": 0.9879189729690552, "text_region": [[1224.0, 2765.0], [2165.0, 2765.0], [2165.0, 2812.0], [1224.0, 2812.0]]}, {"text": "ability of their system to zero-shot transfer to other image", "confidence": 0.9876234531402588, "text_region": [[1224.0, 2808.0], [2169.0, 2815.0], [2168.0, 2861.0], [1224.0, 2854.0]]}], "img_idx": 0, "score": 0.9968090653419495}
{"type": "text", "bbox": [1226, 1361, 2167, 1776], "res": [{"text": " These results suggest that the aggregate supervision acces-", "confidence": 0.9927460551261902, "text_region": [[1217.0, 1346.0], [2175.0, 1350.0], [2175.0, 1406.0], [1217.0, 1402.0]]}, {"text": "sible to modern pre-training methods within web-scale col-", "confidence": 0.9916654825210571, "text_region": [[1224.0, 1402.0], [2175.0, 1402.0], [2175.0, 1449.0], [1224.0, 1449.0]]}, {"text": "lections of text surpasses that of high-quality crowd-labeled", "confidence": 0.9993525147438049, "text_region": [[1227.0, 1449.0], [2172.0, 1449.0], [2172.0, 1495.0], [1227.0, 1495.0]]}, {"text": "NLP datasets. However, in other fields such as computer", "confidence": 0.9824186563491821, "text_region": [[1224.0, 1491.0], [2169.0, 1498.0], [2168.0, 1545.0], [1224.0, 1538.0]]}, {"text": "vision it is still standard practice to pre-train models on", "confidence": 0.9900741577148438, "text_region": [[1224.0, 1541.0], [2175.0, 1541.0], [2175.0, 1597.0], [1224.0, 1597.0]]}, {"text": "crowd-labeled datasets such as ImageNet (Deng et al., 2009).", "confidence": 0.9907805323600769, "text_region": [[1227.0, 1594.0], [2172.0, 1594.0], [2172.0, 1640.0], [1227.0, 1640.0]]}, {"text": "Could scalable pre-training methods which learn directly", "confidence": 0.9860579371452332, "text_region": [[1224.0, 1640.0], [2165.0, 1640.0], [2165.0, 1686.0], [1224.0, 1686.0]]}, {"text": "from web text result in a similar breakthrough in computer", "confidence": 0.9932636022567749, "text_region": [[1224.0, 1686.0], [2172.0, 1690.0], [2172.0, 1736.0], [1224.0, 1732.0]]}, {"text": "vision? Prior work is encouraging.", "confidence": 0.9989275336265564, "text_region": [[1224.0, 1729.0], [1786.0, 1733.0], [1786.0, 1789.0], [1224.0, 1785.0]]}], "img_idx": 0, "score": 0.9934284090995789}
{"type": "text", "bbox": [1227, 763, 2168, 1319], "res": [{"text": "Task-agnostic objectives such as autoregressive and masked", "confidence": 0.9967090487480164, "text_region": [[1224.0, 756.0], [2169.0, 756.0], [2169.0, 802.0], [1224.0, 802.0]]}, {"text": "language modeling have scaled across many orders of mag-", "confidence": 0.9948943257331848, "text_region": [[1227.0, 805.0], [2169.0, 805.0], [2169.0, 851.0], [1227.0, 851.0]]}, {"text": " nitude in compute, model capacity, and data, steadily im-", "confidence": 0.9713813662528992, "text_region": [[1221.0, 848.0], [2172.0, 848.0], [2172.0, 894.0], [1221.0, 894.0]]}, {"text": "proving capabilities. The development of \u201c\"text-to-text\u201d\u2019 as", "confidence": 0.9615210890769958, "text_region": [[1224.0, 901.0], [2172.0, 901.0], [2172.0, 947.0], [1224.0, 947.0]]}, {"text": " a standardized input-output interface (McCann et al., 2018;", "confidence": 0.9901729226112366, "text_region": [[1221.0, 947.0], [2172.0, 944.0], [2172.0, 990.0], [1221.0, 993.0]]}, {"text": "Radford et al., 2019; Raffel et al., 2019) has enabled task-", "confidence": 0.9793586134910583, "text_region": [[1227.0, 993.0], [2172.0, 993.0], [2172.0, 1040.0], [1227.0, 1040.0]]}, {"text": "agnostic architectures to zero-shot transfer to downstream", "confidence": 0.991847813129425, "text_region": [[1227.0, 1046.0], [2172.0, 1036.0], [2172.0, 1082.0], [1228.0, 1092.0]]}, {"text": "datasets removing the need for specialized output heads on", "confidence": 0.9921756982803345, "text_region": [[1224.0, 1089.0], [2165.0, 1089.0], [2165.0, 1135.0], [1224.0, 1135.0]]}, {"text": "dataset specific customization. Flagship systems like GPT-3", "confidence": 0.997941255569458, "text_region": [[1224.0, 1135.0], [2169.0, 1135.0], [2169.0, 1181.0], [1224.0, 1181.0]]}, {"text": "(Brown et al., 2020) are now competitive across many tasks", "confidence": 0.9739024043083191, "text_region": [[1221.0, 1178.0], [2172.0, 1181.0], [2172.0, 1238.0], [1221.0, 1234.0]]}, {"text": "with bespoke models while requiring little to no dataset", "confidence": 0.9857627749443054, "text_region": [[1224.0, 1231.0], [2169.0, 1231.0], [2169.0, 1277.0], [1224.0, 1277.0]]}, {"text": "specific training data.", "confidence": 0.9994556307792664, "text_region": [[1227.0, 1284.0], [1573.0, 1284.0], [1573.0, 1330.0], [1227.0, 1330.0]]}], "img_idx": 0, "score": 0.992098867893219}
{"type": "text", "bbox": [295, 836, 1083, 2305], "res": [{"text": "State-of-the-art computer  vision systems are", "confidence": 0.9646434187889099, "text_region": [[296.0, 828.0], [1084.0, 828.0], [1084.0, 874.0], [296.0, 874.0]]}, {"text": "trained to predict a fixed set of predetermined", "confidence": 0.989427924156189, "text_region": [[296.0, 878.0], [1084.0, 878.0], [1084.0, 924.0], [296.0, 924.0]]}, {"text": "object categories. This restricted form of super-", "confidence": 0.983558177947998, "text_region": [[296.0, 924.0], [1088.0, 924.0], [1088.0, 970.0], [296.0, 970.0]]}, {"text": "vision limits their generality and usability since", "confidence": 0.9939594864845276, "text_region": [[299.0, 973.0], [1081.0, 973.0], [1081.0, 1020.0], [299.0, 1020.0]]}, {"text": "additional labeled data is needed to specify any", "confidence": 0.9862358570098877, "text_region": [[296.0, 1016.0], [1081.0, 1020.0], [1081.0, 1066.0], [296.0, 1063.0]]}, {"text": "other visual concept. Learning directly from raw", "confidence": 0.991649329662323, "text_region": [[299.0, 1066.0], [1081.0, 1066.0], [1081.0, 1112.0], [299.0, 1112.0]]}, {"text": "text about images is a promising alternative which", "confidence": 0.9951324462890625, "text_region": [[299.0, 1115.0], [1081.0, 1115.0], [1081.0, 1162.0], [299.0, 1162.0]]}, {"text": "leverages a much broader source of supervision.", "confidence": 0.9796020984649658, "text_region": [[293.0, 1155.0], [1088.0, 1158.0], [1087.0, 1214.0], [293.0, 1211.0]]}, {"text": "We demonstrate that the simple pre-training task", "confidence": 0.9828483462333679, "text_region": [[293.0, 1204.0], [1088.0, 1208.0], [1087.0, 1264.0], [293.0, 1261.0]]}, {"text": "of predicting which caption goes with which im-", "confidence": 0.967192530632019, "text_region": [[296.0, 1261.0], [1078.0, 1261.0], [1078.0, 1307.0], [296.0, 1307.0]]}, {"text": "age is an efficient and scalable way to learn SOTA", "confidence": 0.986115038394928, "text_region": [[296.0, 1307.0], [1078.0, 1307.0], [1078.0, 1353.0], [296.0, 1353.0]]}, {"text": "image representations from scratch on a dataset", "confidence": 0.9808353781700134, "text_region": [[299.0, 1356.0], [1078.0, 1356.0], [1078.0, 1402.0], [299.0, 1402.0]]}, {"text": "of 400 million (image, text) pairs collected from", "confidence": 0.9755961894989014, "text_region": [[296.0, 1399.0], [1084.0, 1399.0], [1084.0, 1455.0], [296.0, 1455.0]]}, {"text": "the internet. After pre-training, natural language", "confidence": 0.9980924725532532, "text_region": [[293.0, 1442.0], [1084.0, 1449.0], [1084.0, 1505.0], [293.0, 1498.0]]}, {"text": "is used to reference learned visual concepts (or", "confidence": 0.9837589263916016, "text_region": [[293.0, 1488.0], [1084.0, 1495.0], [1084.0, 1551.0], [293.0, 1544.0]]}, {"text": "describe new ones) enabling zero-shot transfer", "confidence": 0.9985048174858093, "text_region": [[296.0, 1541.0], [1084.0, 1541.0], [1084.0, 1597.0], [296.0, 1597.0]]}, {"text": "of the model to downstream tasks. We study", "confidence": 0.9840975403785706, "text_region": [[299.0, 1594.0], [1081.0, 1594.0], [1081.0, 1640.0], [299.0, 1640.0]]}, {"text": "the performance of this approach by benchmark-", "confidence": 0.9977399706840515, "text_region": [[293.0, 1637.0], [1091.0, 1637.0], [1091.0, 1693.0], [293.0, 1693.0]]}, {"text": "ing on over 30 different existing computer vi-", "confidence": 0.9975202679634094, "text_region": [[299.0, 1686.0], [1081.0, 1686.0], [1081.0, 1732.0], [299.0, 1732.0]]}, {"text": "sion datasets, spanning tasks such as OCR, ac-", "confidence": 0.9899078011512756, "text_region": [[303.0, 1739.0], [1084.0, 1739.0], [1084.0, 1785.0], [303.0, 1785.0]]}, {"text": "tion recognition in videos, geo-localization, and", "confidence": 0.9928894639015198, "text_region": [[296.0, 1782.0], [1078.0, 1782.0], [1078.0, 1828.0], [296.0, 1828.0]]}, {"text": "many types of fine-grained object classification.", "confidence": 0.9864810109138489, "text_region": [[299.0, 1831.0], [1081.0, 1831.0], [1081.0, 1878.0], [299.0, 1878.0]]}, {"text": "The model transfers non-trivially to most tasks", "confidence": 0.9954575300216675, "text_region": [[293.0, 1871.0], [1084.0, 1878.0], [1084.0, 1934.0], [293.0, 1927.0]]}, {"text": "and is often competitive with a fully supervised", "confidence": 0.985362708568573, "text_region": [[296.0, 1924.0], [1084.0, 1924.0], [1084.0, 1980.0], [296.0, 1980.0]]}, {"text": "baseline without the need for any dataset spe-", "confidence": 0.9897746443748474, "text_region": [[296.0, 1970.0], [1081.0, 1977.0], [1081.0, 2026.0], [296.0, 2019.0]]}, {"text": "cific training. For instance, we match the ac-", "confidence": 0.9915297031402588, "text_region": [[296.0, 2020.0], [1088.0, 2020.0], [1088.0, 2076.0], [296.0, 2076.0]]}, {"text": "curacy of the original ResNet-50 on ImageNet", "confidence": 0.9960412979125977, "text_region": [[299.0, 2072.0], [1084.0, 2072.0], [1084.0, 2115.0], [299.0, 2115.0]]}, {"text": "zero-shot without needing to use any of the 1.28", "confidence": 0.9907670617103577, "text_region": [[299.0, 2119.0], [1084.0, 2119.0], [1084.0, 2165.0], [299.0, 2165.0]]}, {"text": "million training examples it was trained on. We", "confidence": 0.9737203121185303, "text_region": [[296.0, 2168.0], [1084.0, 2168.0], [1084.0, 2214.0], [296.0, 2214.0]]}, {"text": "release our code and pre-trained model weights at", "confidence": 0.9990835189819336, "text_region": [[296.0, 2211.0], [1081.0, 2211.0], [1081.0, 2257.0], [296.0, 2257.0]]}, {"text": "https://github.Com/OpenAI/CLIP.", "confidence": 0.9834848642349243, "text_region": [[296.0, 2264.0], [1028.0, 2264.0], [1028.0, 2310.0], [296.0, 2310.0]]}], "img_idx": 0, "score": 0.9911421537399292}
{"type": "text", "bbox": [217, 2494, 1160, 2789], "res": [{"text": "Pre-training methods which learn directly from raw text", "confidence": 0.9820332527160645, "text_region": [[220.0, 2492.0], [1164.0, 2492.0], [1164.0, 2538.0], [220.0, 2538.0]]}, {"text": "have revolutionized NLP over the last few years (Dai &", "confidence": 0.9899368286132812, "text_region": [[220.0, 2538.0], [1167.0, 2538.0], [1167.0, 2584.0], [220.0, 2584.0]]}, {"text": "Le, 2015; Peters et al., 2018; Howard & Ruder, 2018; Rad-", "confidence": 0.9792913198471069, "text_region": [[220.0, 2584.0], [1167.0, 2584.0], [1167.0, 2627.0], [220.0, 2627.0]]}, {"text": "ford et al., 2018; Devlin et al., 2018; Raffel et al., 2019).", "confidence": 0.9959444999694824, "text_region": [[220.0, 2633.0], [1167.0, 2633.0], [1167.0, 2680.0], [220.0, 2680.0]]}, {"text": "Equal contribution OpenAI, San Francisco, CA 94110, USA.", "confidence": 0.9873338937759399, "text_region": [[266.0, 2709.0], [1164.0, 2709.0], [1164.0, 2756.0], [266.0, 2756.0]]}, {"text": "Correspondence to: <{alec, jongwook} @openai.com>.", "confidence": 0.9825336933135986, "text_region": [[216.0, 2752.0], [1044.0, 2752.0], [1044.0, 2798.0], [216.0, 2798.0]]}], "img_idx": 0, "score": 0.9898228645324707}
{"type": "text", "bbox": [285, 570, 2095, 656], "res": [{"text": "Alec Radford * 1  Jong Wook Kim * 1 Chris Hallacy 1  Aditya Ramesh 1 Gabriel Goh 1 Sandhini Agarwal 1", "confidence": 0.9268258810043335, "text_region": [[273.0, 558.0], [2099.0, 561.0], [2099.0, 620.0], [273.0, 617.0]]}, {"text": "Girish Sastry 1  Amanda Askell1  Pamela Mishkin 1  Jack Clark 1  Gretchen Krueger 1  Ilya Sutskever 1", "confidence": 0.9469164609909058, "text_region": [[299.0, 607.0], [2082.0, 607.0], [2082.0, 663.0], [299.0, 663.0]]}], "img_idx": 0, "score": 0.9302420020103455}
{"type": "text", "bbox": [68, 853, 136, 1247], "res": [{"text": "2", "confidence": 0.9992066025733948, "text_region": [[73.0, 888.0], [133.0, 888.0], [133.0, 927.0], [73.0, 927.0]]}, {"text": "0", "confidence": 0.8986237049102783, "text_region": [[73.0, 927.0], [130.0, 927.0], [130.0, 967.0], [73.0, 967.0]]}, {"text": "2", "confidence": 0.993075966835022, "text_region": [[73.0, 967.0], [133.0, 967.0], [133.0, 1003.0], [73.0, 1003.0]]}, {"text": "b", "confidence": 0.9981014132499695, "text_region": [[73.0, 1016.0], [136.0, 1016.0], [136.0, 1072.0], [73.0, 1072.0]]}, {"text": "e", "confidence": 0.8304975032806396, "text_region": [[76.0, 1059.0], [133.0, 1059.0], [133.0, 1102.0], [76.0, 1102.0]]}, {"text": "F", "confidence": 0.9592981934547424, "text_region": [[67.0, 1096.0], [133.0, 1096.0], [133.0, 1148.0], [67.0, 1148.0]]}, {"text": "6", "confidence": 0.9904211759567261, "text_region": [[67.0, 1162.0], [136.0, 1162.0], [136.0, 1214.0], [67.0, 1214.0]]}, {"text": "2", "confidence": 0.9989191293716431, "text_region": [[70.0, 1204.0], [136.0, 1204.0], [136.0, 1244.0], [70.0, 1244.0]]}], "img_idx": 0, "score": 0.655937910079956}
{"type": "text", "bbox": [68, 1595, 136, 2210], "res": [{"text": "2", "confidence": 0.9980120658874512, "text_region": [[70.0, 1690.0], [136.0, 1690.0], [136.0, 1746.0], [70.0, 1746.0]]}, {"text": "000\u00b0", "confidence": 0.8913910984992981, "text_region": [[70.0, 1726.0], [133.0, 1726.0], [133.0, 1868.0], [70.0, 1868.0]]}, {"text": "3", "confidence": 0.785688042640686, "text_region": [[70.0, 1871.0], [136.0, 1871.0], [136.0, 1924.0], [70.0, 1924.0]]}, {"text": "0", "confidence": 0.6573918461799622, "text_region": [[73.0, 1917.0], [130.0, 1917.0], [130.0, 1964.0], [73.0, 1964.0]]}, {"text": "2", "confidence": 0.9994845390319824, "text_region": [[73.0, 1996.0], [133.0, 1996.0], [133.0, 2039.0], [73.0, 2039.0]]}, {"text": "1", "confidence": 0.6972842216491699, "text_region": [[90.0, 2092.0], [120.0, 2092.0], [120.0, 2119.0], [90.0, 2119.0]]}, {"text": "X", "confidence": 0.8207228183746338, "text_region": [[73.0, 2112.0], [133.0, 2112.0], [133.0, 2175.0], [73.0, 2175.0]]}], "img_idx": 0, "score": 0.6481375694274902}
{"type": "title", "bbox": [222, 2409, 980, 2452], "res": [{"text": "1. Introduction and Motivating Work", "confidence": 0.9973352551460266, "text_region": [[220.0, 2409.0], [985.0, 2409.0], [985.0, 2455.0], [220.0, 2455.0]]}], "img_idx": 0, "score": 0.9370141625404358}
{"type": "title", "bbox": [278, 362, 2106, 415], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9959591031074524, "text_region": [[276.0, 353.0], [2115.0, 356.0], [2115.0, 422.0], [276.0, 419.0]]}], "img_idx": 0, "score": 0.8849549293518066}
{"type": "title", "bbox": [603, 758, 776, 789], "res": [{"text": "Abstract", "confidence": 0.9980776309967041, "text_region": [[599.0, 752.0], [788.0, 752.0], [788.0, 799.0], [599.0, 799.0]]}], "img_idx": 0, "score": 0.7652856111526489}
