{"type": "text", "bbox": [219, 1663, 1160, 2461], "res": [{"text": "we see that zero-shot CLIP is quite weak on several spe-", "confidence": 0.9913552403450012, "text_region": [[216.0, 1657.0], [1157.0, 1657.0], [1157.0, 1703.0], [216.0, 1703.0]]}, {"text": "cialized, complex, or abstract tasks such as satellite image", "confidence": 0.9917191863059998, "text_region": [[216.0, 1703.0], [1161.0, 1703.0], [1161.0, 1749.0], [216.0, 1749.0]]}, {"text": "classification (EuroSAT and RESISC45), lymph node tumor", "confidence": 0.9992488622665405, "text_region": [[216.0, 1749.0], [1164.0, 1752.0], [1164.0, 1799.0], [216.0, 1795.0]]}, {"text": "detection (PatchCamelyon), counting objects in synthetic", "confidence": 0.9939594864845276, "text_region": [[216.0, 1802.0], [1161.0, 1802.0], [1161.0, 1845.0], [216.0, 1845.0]]}, {"text": "scenes (CLEVRCounts), self-driving related tasks such as", "confidence": 0.9939241409301758, "text_region": [[216.0, 1848.0], [1164.0, 1848.0], [1164.0, 1894.0], [216.0, 1894.0]]}, {"text": "German traffic sign recognition (GTSRB), recognizing dis-", "confidence": 0.9906893372535706, "text_region": [[216.0, 1891.0], [1167.0, 1891.0], [1167.0, 1947.0], [216.0, 1947.0]]}, {"text": "tance to the nearest car (KITTI Distance). These results", "confidence": 0.999114453792572, "text_region": [[216.0, 1944.0], [1164.0, 1940.0], [1164.0, 1987.0], [216.0, 1990.0]]}, {"text": "highlight the poor capability of zero-shot CLIP on more", "confidence": 0.9949496388435364, "text_region": [[216.0, 1986.0], [1161.0, 1993.0], [1161.0, 2040.0], [216.0, 2033.0]]}, {"text": "complex tasks. By contrast, non-expert humans can robustly", "confidence": 0.9983101487159729, "text_region": [[220.0, 2039.0], [1161.0, 2039.0], [1161.0, 2086.0], [220.0, 2086.0]]}, {"text": "perform several of these tasks, such as counting, satellite", "confidence": 0.9995850324630737, "text_region": [[220.0, 2089.0], [1164.0, 2089.0], [1164.0, 2135.0], [220.0, 2135.0]]}, {"text": "image classification, and traffic sign recognition, suggesting", "confidence": 0.9755213260650635, "text_region": [[213.0, 2128.0], [1168.0, 2132.0], [1167.0, 2188.0], [213.0, 2185.0]]}, {"text": "significant room for improvement. However, we caution", "confidence": 0.9890528917312622, "text_region": [[220.0, 2185.0], [1161.0, 2185.0], [1161.0, 2231.0], [220.0, 2231.0]]}, {"text": "that it is unclear whether measuring zero-shot transfer, as", "confidence": 0.9925650358200073, "text_region": [[220.0, 2231.0], [1164.0, 2231.0], [1164.0, 2277.0], [220.0, 2277.0]]}, {"text": "opposed to few-shot transfer, is a meaningful evaluation for", "confidence": 0.9941880702972412, "text_region": [[216.0, 2280.0], [1161.0, 2274.0], [1161.0, 2320.0], [216.0, 2327.0]]}, {"text": "difficult tasks that a learner has no prior experience with,", "confidence": 0.9912420511245728, "text_region": [[216.0, 2326.0], [1167.0, 2326.0], [1167.0, 2373.0], [216.0, 2373.0]]}, {"text": "such as lymph node tumor classification for almost all hu-", "confidence": 0.9928180575370789, "text_region": [[220.0, 2376.0], [1167.0, 2376.0], [1167.0, 2419.0], [220.0, 2419.0]]}, {"text": "mans (and possibly CLIP).", "confidence": 0.9969695210456848, "text_region": [[220.0, 2422.0], [649.0, 2422.0], [649.0, 2468.0], [220.0, 2468.0]]}], "img_idx": 0, "score": 0.9942312836647034}
{"type": "text", "bbox": [1228, 2216, 2168, 2825], "res": [{"text": " In addition to studying the average performance of zero-shot", "confidence": 0.9936282634735107, "text_region": [[1217.0, 2204.0], [2172.0, 2208.0], [2172.0, 2264.0], [1217.0, 2260.0]]}, {"text": "CLIP and few-shot logistic regression, we also examine", "confidence": 0.9898620843887329, "text_region": [[1224.0, 2257.0], [2169.0, 2261.0], [2169.0, 2307.0], [1224.0, 2303.0]]}, {"text": " performance on individual datasets. In Figure 7, we show", "confidence": 0.9971487522125244, "text_region": [[1217.0, 2303.0], [2172.0, 2300.0], [2172.0, 2356.0], [1217.0, 2360.0]]}, {"text": "estimates for the number of labeled examples per class that", "confidence": 0.9917649626731873, "text_region": [[1224.0, 2356.0], [2172.0, 2356.0], [2172.0, 2402.0], [1224.0, 2402.0]]}, {"text": "a logistic regression classifier on the same feature space", "confidence": 0.9959020018577576, "text_region": [[1224.0, 2406.0], [2172.0, 2406.0], [2172.0, 2452.0], [1224.0, 2452.0]]}, {"text": " requires to match the performance of zero-shot CLIP. Since", "confidence": 0.9763090014457703, "text_region": [[1221.0, 2452.0], [2172.0, 2449.0], [2172.0, 2495.0], [1221.0, 2498.0]]}, {"text": "zero-shot CLIP is also a linear classifier, this estimates the", "confidence": 0.9822725057601929, "text_region": [[1224.0, 2498.0], [2169.0, 2498.0], [2169.0, 2544.0], [1224.0, 2544.0]]}, {"text": "effective data efficiency of zero-shot transfer in this setting.", "confidence": 0.9971175789833069, "text_region": [[1227.0, 2548.0], [2172.0, 2548.0], [2172.0, 2594.0], [1227.0, 2594.0]]}, {"text": "In order to avoid training thousands of linear classifiers,", "confidence": 0.9973875880241394, "text_region": [[1224.0, 2594.0], [2175.0, 2594.0], [2175.0, 2640.0], [1224.0, 2640.0]]}, {"text": "we estimate the effective data efficiency based on a log-", "confidence": 0.9936628937721252, "text_region": [[1224.0, 2643.0], [2172.0, 2643.0], [2172.0, 2690.0], [1224.0, 2690.0]]}, {"text": "linear interpolation of the performance of a 1, 2, 4, 8, 16-", "confidence": 0.9968354105949402, "text_region": [[1224.0, 2693.0], [2172.0, 2693.0], [2172.0, 2739.0], [1224.0, 2739.0]]}, {"text": " shot (when possible), and a fully supervised linear classifier", "confidence": 0.9838147759437561, "text_region": [[1221.0, 2739.0], [2165.0, 2739.0], [2165.0, 2785.0], [1221.0, 2785.0]]}, {"text": "trained on each dataset. We find that zero-shot transfer can", "confidence": 0.9991979002952576, "text_region": [[1224.0, 2788.0], [2169.0, 2788.0], [2169.0, 2831.0], [1224.0, 2831.0]]}], "img_idx": 0, "score": 0.9939529895782471}
{"type": "text", "bbox": [1227, 1619, 2168, 2178], "res": [{"text": "When comparing zero-shot CLIP to few-shot logistic re-", "confidence": 0.972178041934967, "text_region": [[1224.0, 1614.0], [2175.0, 1614.0], [2175.0, 1660.0], [1224.0, 1660.0]]}, {"text": "gression on the features of other models, zero-shot CLIP", "confidence": 0.9929396510124207, "text_region": [[1224.0, 1663.0], [2172.0, 1660.0], [2172.0, 1706.0], [1224.0, 1709.0]]}, {"text": "roughly matches the performance of the best performing", "confidence": 0.9989749789237976, "text_region": [[1221.0, 1703.0], [2175.0, 1706.0], [2175.0, 1762.0], [1221.0, 1759.0]]}, {"text": "16-shot classifier in our evaluation suite, which uses the fea-", "confidence": 0.9813673496246338, "text_region": [[1224.0, 1759.0], [2169.0, 1759.0], [2169.0, 1802.0], [1224.0, 1802.0]]}, {"text": "tures of a BiT-M ResNet-152x2 trained on ImageNet-21K.", "confidence": 0.997738778591156, "text_region": [[1224.0, 1805.0], [2172.0, 1805.0], [2172.0, 1851.0], [1224.0, 1851.0]]}, {"text": "We are certain that a BiT-L model trained on JFT-300M", "confidence": 0.9730022549629211, "text_region": [[1224.0, 1848.0], [2172.0, 1851.0], [2172.0, 1898.0], [1224.0, 1894.0]]}, {"text": "would perform even better but these models have not been", "confidence": 0.9839816689491272, "text_region": [[1227.0, 1901.0], [2172.0, 1901.0], [2172.0, 1947.0], [1227.0, 1947.0]]}, {"text": "publicly released. That a BiT-M ResNet-152x2 performs", "confidence": 0.999570906162262, "text_region": [[1227.0, 1950.0], [2172.0, 1950.0], [2172.0, 1996.0], [1227.0, 1996.0]]}, {"text": " best in a 16-shot setting is somewhat surprising since, as", "confidence": 0.9896575808525085, "text_region": [[1218.0, 1986.0], [2172.0, 1993.0], [2172.0, 2049.0], [1217.0, 2043.0]]}, {"text": "analyzed in Section 3.2, the Noisy Student EfficientNet-L2", "confidence": 0.9917407035827637, "text_region": [[1227.0, 2043.0], [2172.0, 2043.0], [2172.0, 2089.0], [1227.0, 2089.0]]}, {"text": " outperforms it in a fully supervised setting by almost 5% on", "confidence": 0.993503212928772, "text_region": [[1217.0, 2092.0], [2175.0, 2085.0], [2175.0, 2138.0], [1218.0, 2145.0]]}, {"text": "average across 27 datasets.", "confidence": 0.9909067749977112, "text_region": [[1224.0, 2142.0], [1660.0, 2142.0], [1660.0, 2188.0], [1224.0, 2188.0]]}], "img_idx": 0, "score": 0.9916683435440063}
{"type": "text", "bbox": [219, 1223, 1159, 1563], "res": [{"text": "Figure 6. Zero-shot CLIP outperforms few-shot linear probes.", "confidence": 0.9994367957115173, "text_region": [[220.0, 1218.0], [1164.0, 1218.0], [1164.0, 1264.0], [220.0, 1264.0]]}, {"text": "Zero-shot CLIP matches the average performance of a 4-shot linear", "confidence": 0.9978493452072144, "text_region": [[216.0, 1264.0], [1157.0, 1264.0], [1157.0, 1310.0], [216.0, 1310.0]]}, {"text": "classifier trained on the same feature space and nearly matches the", "confidence": 0.9890552163124084, "text_region": [[220.0, 1307.0], [1161.0, 1307.0], [1161.0, 1353.0], [220.0, 1353.0]]}, {"text": "best results of a 16-shot linear classifier across publicly available", "confidence": 0.9860683083534241, "text_region": [[216.0, 1346.0], [1164.0, 1350.0], [1164.0, 1396.0], [216.0, 1393.0]]}, {"text": "models. For both BiT-M and SimCLRv2, the best performing", "confidence": 0.9978700876235962, "text_region": [[220.0, 1396.0], [1164.0, 1396.0], [1164.0, 1442.0], [220.0, 1442.0]]}, {"text": "model is highlighted. Light gray lines are other models in the eval", "confidence": 0.9916796088218689, "text_region": [[220.0, 1439.0], [1164.0, 1439.0], [1164.0, 1485.0], [220.0, 1485.0]]}, {"text": "suite. The 20 datasets with at least 16 examples per class were", "confidence": 0.9823502898216248, "text_region": [[216.0, 1482.0], [1167.0, 1482.0], [1167.0, 1528.0], [216.0, 1528.0]]}, {"text": "used in this analysis.", "confidence": 0.9971733689308167, "text_region": [[216.0, 1528.0], [516.0, 1528.0], [516.0, 1571.0], [216.0, 1571.0]]}], "img_idx": 0, "score": 0.9915624856948853}
{"type": "text", "bbox": [1226, 1070, 2170, 1577], "res": [{"text": "A potential resolution of this discrepancy between zero-", "confidence": 0.9952298402786255, "text_region": [[1224.0, 1066.0], [2175.0, 1066.0], [2175.0, 1112.0], [1224.0, 1112.0]]}, {"text": "shot and few-shot performance is to use CLIP's zero-shot", "confidence": 0.9895578026771545, "text_region": [[1224.0, 1112.0], [2175.0, 1112.0], [2175.0, 1158.0], [1224.0, 1158.0]]}, {"text": "classifier as a prior for the weights of the few-shot classifier.", "confidence": 0.9876265525817871, "text_region": [[1227.0, 1162.0], [2175.0, 1162.0], [2175.0, 1208.0], [1227.0, 1208.0]]}, {"text": " While adding an L2 penalty towards the generated weights", "confidence": 0.9785181879997253, "text_region": [[1217.0, 1201.0], [2172.0, 1205.0], [2172.0, 1261.0], [1217.0, 1257.0]]}, {"text": "is a straightforward implementation of this idea, we found", "confidence": 0.9973632097244263, "text_region": [[1224.0, 1254.0], [2172.0, 1254.0], [2172.0, 1300.0], [1224.0, 1300.0]]}, {"text": "that hyperparameter optimization would often select for", "confidence": 0.9994714260101318, "text_region": [[1224.0, 1307.0], [2172.0, 1307.0], [2172.0, 1353.0], [1224.0, 1353.0]]}, {"text": "such a large value of this regularizer that the resulting few-", "confidence": 0.9870221018791199, "text_region": [[1224.0, 1353.0], [2175.0, 1353.0], [2175.0, 1399.0], [1224.0, 1399.0]]}, {"text": "shot classifier was \u201cjust\u2019 the zero-shot classifier. Research", "confidence": 0.9865606427192688, "text_region": [[1227.0, 1399.0], [2172.0, 1399.0], [2172.0, 1445.0], [1227.0, 1445.0]]}, {"text": "into better methods of combining the strength of zero-shot", "confidence": 0.9987056851387024, "text_region": [[1224.0, 1449.0], [2172.0, 1449.0], [2172.0, 1495.0], [1224.0, 1495.0]]}, {"text": " transfer with fexibility of few-shot learning is a promising", "confidence": 0.9799112677574158, "text_region": [[1217.0, 1488.0], [2172.0, 1492.0], [2172.0, 1548.0], [1217.0, 1544.0]]}, {"text": "direction for future work.", "confidence": 0.9998311996459961, "text_region": [[1227.0, 1541.0], [1636.0, 1541.0], [1636.0, 1587.0], [1227.0, 1587.0]]}], "img_idx": 0, "score": 0.9910256862640381}
{"type": "text", "bbox": [220, 2498, 1161, 2864], "res": [{"text": "While comparing zero-shot performance to fully supervised", "confidence": 0.9892287850379944, "text_region": [[213.0, 2485.0], [1164.0, 2488.0], [1164.0, 2544.0], [213.0, 2541.0]]}, {"text": " models contextualizes the task-learning capabilities of CLIP,", "confidence": 0.9895705580711365, "text_region": [[213.0, 2538.0], [1171.0, 2538.0], [1171.0, 2594.0], [213.0, 2594.0]]}, {"text": "comparing to few-shot methods is a more direct compari-", "confidence": 0.988227128982544, "text_region": [[220.0, 2590.0], [1167.0, 2590.0], [1167.0, 2637.0], [220.0, 2637.0]]}, {"text": "son, since zero-shot is its limit. In Figure 6, we visualize", "confidence": 0.9969216585159302, "text_region": [[216.0, 2640.0], [1164.0, 2640.0], [1164.0, 2683.0], [216.0, 2683.0]]}, {"text": "how zero-shot CLIP compares to few-shot logistic regres-", "confidence": 0.9954885840415955, "text_region": [[220.0, 2686.0], [1164.0, 2686.0], [1164.0, 2732.0], [220.0, 2732.0]]}, {"text": "sion on the features of many image models including the", "confidence": 0.9978246688842773, "text_region": [[216.0, 2736.0], [1164.0, 2736.0], [1164.0, 2779.0], [216.0, 2779.0]]}, {"text": "best publicly available ImageNet models, self-supervised", "confidence": 0.9919606447219849, "text_region": [[216.0, 2775.0], [1167.0, 2775.0], [1167.0, 2831.0], [216.0, 2831.0]]}, {"text": "learning methods, and CLIP itself. While it is intuitive to", "confidence": 0.9890356659889221, "text_region": [[220.0, 2828.0], [1164.0, 2828.0], [1164.0, 2874.0], [220.0, 2874.0]]}], "img_idx": 0, "score": 0.9908663630485535}
{"type": "text", "bbox": [1224, 280, 2168, 1006], "res": [{"text": " expect zero-shot to underperform one-shot, we instead find", "confidence": 0.980709969997406, "text_region": [[1221.0, 271.0], [2172.0, 267.0], [2172.0, 323.0], [1221.0, 327.0]]}, {"text": "that zero-shot CLIP matches the performance of 4-shot lo-", "confidence": 0.9934688210487366, "text_region": [[1224.0, 323.0], [2172.0, 323.0], [2172.0, 370.0], [1224.0, 370.0]]}, {"text": "gistic regression on the same feature space. This is likely", "confidence": 0.9955227375030518, "text_region": [[1227.0, 373.0], [2172.0, 373.0], [2172.0, 419.0], [1227.0, 419.0]]}, {"text": "due to an important difference between the zero-shot and", "confidence": 0.9822317957878113, "text_region": [[1227.0, 422.0], [2172.0, 422.0], [2172.0, 465.0], [1227.0, 465.0]]}, {"text": "few-shot approach. First, CLIP's zero-shot classifier is gen-", "confidence": 0.9907670021057129, "text_region": [[1224.0, 462.0], [2175.0, 462.0], [2175.0, 518.0], [1224.0, 518.0]]}, {"text": "erated via natural language which allows for visual concepts", "confidence": 0.9867299795150757, "text_region": [[1227.0, 515.0], [2172.0, 515.0], [2172.0, 561.0], [1227.0, 561.0]]}, {"text": "to be directly specified (*\"communicated\"). By contrast,", "confidence": 0.9840058088302612, "text_region": [[1221.0, 558.0], [2175.0, 558.0], [2175.0, 614.0], [1221.0, 614.0]]}, {"text": "\u201cnormal\u2019 supervised learning must infer concepts indirectly", "confidence": 0.9790182709693909, "text_region": [[1227.0, 610.0], [2172.0, 610.0], [2172.0, 657.0], [1227.0, 657.0]]}, {"text": "from training examples. Context-less example-based learn-", "confidence": 0.9951272010803223, "text_region": [[1221.0, 650.0], [2175.0, 653.0], [2175.0, 710.0], [1221.0, 706.0]]}, {"text": "ing has the drawback that many different hypotheses can", "confidence": 0.9967764616012573, "text_region": [[1224.0, 706.0], [2172.0, 706.0], [2172.0, 752.0], [1224.0, 752.0]]}, {"text": " be consistent with the data, especially in the one-shot case.", "confidence": 0.9851904511451721, "text_region": [[1217.0, 746.0], [2175.0, 749.0], [2175.0, 805.0], [1217.0, 802.0]]}, {"text": "A single image often contains many different visual con-", "confidence": 0.9937947392463684, "text_region": [[1224.0, 802.0], [2172.0, 802.0], [2172.0, 848.0], [1224.0, 848.0]]}, {"text": "cepts. Although a capable learner is able to exploit visual", "confidence": 0.9891583919525146, "text_region": [[1221.0, 845.0], [2175.0, 841.0], [2175.0, 898.0], [1221.0, 901.0]]}, {"text": "cues and heuristics, such as assuming that the concept being", "confidence": 0.9964646100997925, "text_region": [[1227.0, 898.0], [2169.0, 898.0], [2169.0, 944.0], [1227.0, 944.0]]}, {"text": " demonstrated is the primary object in an image, there is no", "confidence": 0.9934918284416199, "text_region": [[1217.0, 937.0], [2175.0, 941.0], [2175.0, 997.0], [1217.0, 993.0]]}, {"text": "guarantee.", "confidence": 0.9996635317802429, "text_region": [[1227.0, 1000.0], [1394.0, 1000.0], [1394.0, 1036.0], [1227.0, 1036.0]]}], "img_idx": 0, "score": 0.9881611466407776}
{"type": "figure", "bbox": [197, 261, 1161, 1130], "res": [{"text": "75", "confidence": 0.9999430179595947, "text_region": [[259.0, 261.0], [306.0, 261.0], [306.0, 304.0], [259.0, 304.0]]}, {"text": "Linear Probe CLIP", "confidence": 0.9666271209716797, "text_region": [[875.0, 297.0], [1137.0, 293.0], [1138.0, 340.0], [875.0, 343.0]]}, {"text": "70-", "confidence": 0.9150113463401794, "text_region": [[256.0, 346.0], [316.0, 346.0], [316.0, 389.0], [256.0, 389.0]]}, {"text": "65", "confidence": 0.9997395277023315, "text_region": [[256.0, 439.0], [313.0, 439.0], [313.0, 475.0], [256.0, 475.0]]}, {"text": "Zero-Shot", "confidence": 0.9982060194015503, "text_region": [[316.0, 442.0], [472.0, 442.0], [472.0, 478.0], [316.0, 478.0]]}, {"text": "BiT-M (ImageNet-21K)", "confidence": 0.9740551710128784, "text_region": [[812.0, 452.0], [1138.0, 452.0], [1138.0, 498.0], [812.0, 498.0]]}, {"text": "\u2605CLIP", "confidence": 0.9582775235176086, "text_region": [[314.0, 480.0], [439.0, 467.0], [444.0, 513.0], [319.0, 526.0]]}, {"text": "SimCLRv2", "confidence": 0.9982037544250488, "text_region": [[991.0, 515.0], [1134.0, 515.0], [1134.0, 551.0], [991.0, 551.0]]}, {"text": "60", "confidence": 0.9976440668106079, "text_region": [[246.0, 531.0], [309.0, 531.0], [309.0, 568.0], [246.0, 568.0]]}, {"text": "55", "confidence": 0.9998563528060913, "text_region": [[256.0, 614.0], [309.0, 614.0], [309.0, 657.0], [256.0, 657.0]]}, {"text": "ResNet50", "confidence": 0.9994399547576904, "text_region": [[988.0, 610.0], [1134.0, 610.0], [1134.0, 644.0], [988.0, 644.0]]}, {"text": "45", "confidence": 0.9999010562896729, "text_region": [[256.0, 799.0], [306.0, 799.0], [306.0, 832.0], [256.0, 832.0]]}, {"text": "40", "confidence": 0.9996493458747864, "text_region": [[256.0, 881.0], [313.0, 881.0], [313.0, 921.0], [256.0, 921.0]]}, {"text": "35", "confidence": 0.9999091029167175, "text_region": [[256.0, 970.0], [309.0, 970.0], [309.0, 1010.0], [256.0, 1010.0]]}, {"text": "30", "confidence": 0.9997290372848511, "text_region": [[259.0, 1059.0], [313.0, 1059.0], [313.0, 1099.0], [259.0, 1099.0]]}, {"text": "012", "confidence": 0.9951999187469482, "text_region": [[326.0, 1089.0], [452.0, 1089.0], [452.0, 1125.0], [326.0, 1125.0]]}, {"text": "4", "confidence": 0.9996113181114197, "text_region": [[528.0, 1098.0], [545.0, 1095.0], [550.0, 1119.0], [533.0, 1123.0]]}, {"text": "8", "confidence": 0.999607264995575, "text_region": [[732.0, 1096.0], [752.0, 1096.0], [752.0, 1125.0], [732.0, 1125.0]]}, {"text": "16", "confidence": 0.9994533061981201, "text_region": [[1124.0, 1096.0], [1157.0, 1096.0], [1157.0, 1125.0], [1124.0, 1125.0]]}], "img_idx": 0, "score": 0.9605399370193481}
{"type": "figure_caption", "bbox": [433, 1137, 1027, 1168], "res": [{"text": "# of labeled training examples per class", "confidence": 0.9861677885055542, "text_region": [[429.0, 1132.0], [1031.0, 1132.0], [1031.0, 1178.0], [429.0, 1178.0]]}], "img_idx": 0, "score": 0.9025693535804749}
{"type": "header", "bbox": [2143, 193, 2165, 221], "res": [], "img_idx": 0, "score": 0.7262935042381287}
{"type": "header", "bbox": [485, 194, 1366, 225], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9977254271507263, "text_region": [[220.0, 188.0], [1370.0, 188.0], [1370.0, 231.0], [220.0, 231.0]]}], "img_idx": 0, "score": 0.6652043461799622}
