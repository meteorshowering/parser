{"type": "text", "bbox": [1227, 1733, 2168, 2343], "res": [{"text": "A concern with pre-training on a very large internet dataset", "confidence": 0.9931201934814453, "text_region": [[1227.0, 1729.0], [2172.0, 1729.0], [2172.0, 1772.0], [1227.0, 1772.0]]}, {"text": "is unintentional overlap with downstream evals. This is", "confidence": 0.9943480491638184, "text_region": [[1224.0, 1775.0], [2172.0, 1775.0], [2172.0, 1822.0], [1224.0, 1822.0]]}, {"text": "important to investigate since, in a worst-case scenario, a", "confidence": 0.9876920580863953, "text_region": [[1224.0, 1825.0], [2172.0, 1825.0], [2172.0, 1871.0], [1224.0, 1871.0]]}, {"text": " complete copy of an evaluation dataset could leak into the", "confidence": 0.9963992238044739, "text_region": [[1221.0, 1871.0], [2172.0, 1868.0], [2172.0, 1914.0], [1221.0, 1917.0]]}, {"text": " pre-training dataset and invalidate the evaluation as a mean-", "confidence": 0.9894425868988037, "text_region": [[1224.0, 1921.0], [2175.0, 1921.0], [2175.0, 1967.0], [1224.0, 1967.0]]}, {"text": "ingful test of generalization. One option to prevent this is to", "confidence": 0.9924917817115784, "text_region": [[1224.0, 1967.0], [2172.0, 1967.0], [2172.0, 2013.0], [1224.0, 2013.0]]}, {"text": "identify and remove all duplicates before training a model.", "confidence": 0.9887346029281616, "text_region": [[1221.0, 2010.0], [2179.0, 2010.0], [2179.0, 2066.0], [1221.0, 2066.0]]}, {"text": "While this guarantees reporting true hold-out performance,", "confidence": 0.9926365613937378, "text_region": [[1221.0, 2059.0], [2172.0, 2063.0], [2172.0, 2109.0], [1221.0, 2105.0]]}, {"text": "it requires knowing all possible data which a model might", "confidence": 0.9969695806503296, "text_region": [[1221.0, 2112.0], [2172.0, 2112.0], [2172.0, 2155.0], [1221.0, 2155.0]]}, {"text": "be evaluated on ahead of time. This has the downside of", "confidence": 0.9957811236381531, "text_region": [[1221.0, 2158.0], [2172.0, 2155.0], [2172.0, 2201.0], [1221.0, 2204.0]]}, {"text": "limiting the scope of benchmarking and analysis. Adding a", "confidence": 0.9872241020202637, "text_region": [[1227.0, 2208.0], [2169.0, 2208.0], [2169.0, 2254.0], [1227.0, 2254.0]]}, {"text": " new evaluation would require an expensive re-train or risk", "confidence": 0.9861121773719788, "text_region": [[1221.0, 2251.0], [2169.0, 2251.0], [2169.0, 2297.0], [1221.0, 2297.0]]}, {"text": "reporting an un-quantified benefit due to overlap.", "confidence": 0.9935119152069092, "text_region": [[1227.0, 2303.0], [2012.0, 2303.0], [2012.0, 2350.0], [1227.0, 2350.0]]}], "img_idx": 0, "score": 0.9941769242286682}
{"type": "text", "bbox": [1227, 2546, 2170, 2866], "res": [{"text": "1) For each evaluation dataset, we run a duplicate detector", "confidence": 0.9931822419166565, "text_region": [[1224.0, 2541.0], [2169.0, 2541.0], [2169.0, 2587.0], [1224.0, 2587.0]]}, {"text": "(see Appendix C) on its examples. We then manually inspect", "confidence": 0.9768148064613342, "text_region": [[1221.0, 2584.0], [2175.0, 2584.0], [2175.0, 2640.0], [1221.0, 2640.0]]}, {"text": "the found nearest neighbors and set a per dataset threshold", "confidence": 0.9924879670143127, "text_region": [[1224.0, 2637.0], [2172.0, 2637.0], [2172.0, 2683.0], [1224.0, 2683.0]]}, {"text": " to keep high precision while maximizing recall. Using", "confidence": 0.983367919921875, "text_region": [[1217.0, 2676.0], [2175.0, 2680.0], [2175.0, 2736.0], [1217.0, 2732.0]]}, {"text": "this threshold, we then create two new subsets, Overlap,", "confidence": 0.9990392327308655, "text_region": [[1221.0, 2729.0], [2175.0, 2732.0], [2175.0, 2779.0], [1221.0, 2775.0]]}, {"text": "which contains all examples which have a similarity to a", "confidence": 0.9927014112472534, "text_region": [[1221.0, 2779.0], [2169.0, 2779.0], [2169.0, 2825.0], [1221.0, 2825.0]]}, {"text": "training example above the threshold, and Clean, which", "confidence": 0.9943711161613464, "text_region": [[1224.0, 2828.0], [2172.0, 2828.0], [2172.0, 2874.0], [1224.0, 2874.0]]}], "img_idx": 0, "score": 0.9924416542053223}
{"type": "text", "bbox": [219, 2449, 1160, 2868], "res": [{"text": "This suggests that there are still algorithmic improvements", "confidence": 0.9930610060691833, "text_region": [[216.0, 2445.0], [1164.0, 2445.0], [1164.0, 2492.0], [216.0, 2492.0]]}, {"text": "waiting to be made to decrease the gap between machine", "confidence": 0.9897006154060364, "text_region": [[216.0, 2495.0], [1164.0, 2495.0], [1164.0, 2541.0], [216.0, 2541.0]]}, {"text": "and human sample efficiency, as noted by Lake et al. (2016)", "confidence": 0.9860560894012451, "text_region": [[216.0, 2541.0], [1164.0, 2541.0], [1164.0, 2587.0], [216.0, 2587.0]]}, {"text": "and others. Because these few-shot evaluations of CLIP", "confidence": 0.9900566935539246, "text_region": [[216.0, 2587.0], [1164.0, 2587.0], [1164.0, 2633.0], [216.0, 2633.0]]}, {"text": "don't make effective use of prior knowledge and the humans", "confidence": 0.996102511882782, "text_region": [[216.0, 2637.0], [1164.0, 2637.0], [1164.0, 2683.0], [216.0, 2683.0]]}, {"text": " do, we speculate that finding a method to properly integrate", "confidence": 0.9864136576652527, "text_region": [[210.0, 2676.0], [1168.0, 2680.0], [1167.0, 2736.0], [209.0, 2732.0]]}, {"text": " prior knowledge into few-shot learning is an important step", "confidence": 0.9936580657958984, "text_region": [[213.0, 2726.0], [1168.0, 2729.0], [1167.0, 2785.0], [213.0, 2782.0]]}, {"text": "in algorithmic improvements to CLIP. To our knowledge,", "confidence": 0.9980899691581726, "text_region": [[216.0, 2775.0], [1171.0, 2775.0], [1171.0, 2831.0], [216.0, 2831.0]]}, {"text": "using a linear classifier on top of the features of a high-", "confidence": 0.9818129539489746, "text_region": [[216.0, 2828.0], [1164.0, 2828.0], [1164.0, 2874.0], [216.0, 2874.0]]}], "img_idx": 0, "score": 0.9920480847358704}
{"type": "text", "bbox": [1226, 1301, 2168, 1575], "res": [{"text": " If we plot human accuracy vs CLIP's zero shot accuracy", "confidence": 0.9738763570785522, "text_region": [[1221.0, 1297.0], [2169.0, 1297.0], [2169.0, 1343.0], [1221.0, 1343.0]]}, {"text": "(Figure 16), we see that the hardest problems for CLIP are", "confidence": 0.9991785883903503, "text_region": [[1224.0, 1343.0], [2172.0, 1343.0], [2172.0, 1389.0], [1224.0, 1389.0]]}, {"text": "also hard for humans. To the extent that errors are consistent,", "confidence": 0.9869068264961243, "text_region": [[1224.0, 1393.0], [2172.0, 1393.0], [2172.0, 1436.0], [1224.0, 1436.0]]}, {"text": "our hypothesis is that this is due to at least a two factors:", "confidence": 0.9920305609703064, "text_region": [[1224.0, 1439.0], [2175.0, 1439.0], [2175.0, 1485.0], [1224.0, 1485.0]]}, {"text": "noise in the dataset (including mislabeled images) and out of", "confidence": 0.9928606748580933, "text_region": [[1224.0, 1485.0], [2172.0, 1488.0], [2172.0, 1535.0], [1224.0, 1531.0]]}, {"text": " distribution images being hard for both humans and models.", "confidence": 0.9927279353141785, "text_region": [[1217.0, 1528.0], [2175.0, 1531.0], [2175.0, 1587.0], [1217.0, 1584.0]]}], "img_idx": 0, "score": 0.9914351105690002}
{"type": "text", "bbox": [1226, 639, 2169, 982], "res": [{"text": "Table 2. Comparison of human performance on Oxford IIT Pets.", "confidence": 0.9935300946235657, "text_region": [[1227.0, 637.0], [2175.0, 637.0], [2175.0, 683.0], [1227.0, 683.0]]}, {"text": "As in Parkhi et al. (2012), the metric is average per-class classfica-", "confidence": 0.9716081023216248, "text_region": [[1221.0, 676.0], [2172.0, 676.0], [2172.0, 733.0], [1221.0, 733.0]]}, {"text": "tion accuracy. Most of the gain in performance when going from", "confidence": 0.9928677082061768, "text_region": [[1224.0, 723.0], [2172.0, 723.0], [2172.0, 769.0], [1224.0, 769.0]]}, {"text": "the human zero shot case to the human one shot case is on images", "confidence": 0.9805848598480225, "text_region": [[1221.0, 762.0], [2169.0, 769.0], [2168.0, 815.0], [1221.0, 808.0]]}, {"text": "that participants were highly uncertain on. \u201cGuesses\"\u2019 refers to", "confidence": 0.9735172986984253, "text_region": [[1224.0, 812.0], [2172.0, 812.0], [2172.0, 855.0], [1224.0, 855.0]]}, {"text": "restricting the dataset to where participants selected an answer", "confidence": 0.9764409065246582, "text_region": [[1224.0, 851.0], [2169.0, 851.0], [2169.0, 898.0], [1224.0, 898.0]]}, {"text": "other than \u201cI don't know\", the \u201cmajority vote\" is taking the most ", "confidence": 0.9701073169708252, "text_region": [[1224.0, 894.0], [2175.0, 898.0], [2175.0, 944.0], [1224.0, 940.0]]}, {"text": "frequent (exclusive of ties) answer per image.", "confidence": 0.9876487851142883, "text_region": [[1221.0, 940.0], [1886.0, 944.0], [1886.0, 990.0], [1221.0, 987.0]]}], "img_idx": 0, "score": 0.9913507699966431}
{"type": "text", "bbox": [219, 1806, 1159, 2413], "res": [{"text": " Interestingly, humans went from a performance average of", "confidence": 0.9906529784202576, "text_region": [[216.0, 1795.0], [1168.0, 1799.0], [1167.0, 1845.0], [216.0, 1841.0]]}, {"text": "54% to 76% with just one training example per class, and", "confidence": 0.9927220344543457, "text_region": [[220.0, 1848.0], [1161.0, 1848.0], [1161.0, 1894.0], [220.0, 1894.0]]}, {"text": "the marginal gain from an additional training example is", "confidence": 0.9933590888977051, "text_region": [[213.0, 1888.0], [1164.0, 1891.0], [1164.0, 1947.0], [213.0, 1944.0]]}, {"text": "minimal. The gain in accuracy going from zero to one shot", "confidence": 0.977520227432251, "text_region": [[220.0, 1944.0], [1167.0, 1944.0], [1167.0, 1990.0], [220.0, 1990.0]]}, {"text": "is almost entirely on images that humans were uncertain", "confidence": 0.9881152510643005, "text_region": [[216.0, 1987.0], [1161.0, 1990.0], [1161.0, 2036.0], [216.0, 2033.0]]}, {"text": "about. This suggests that humans \u201cknow what they don't", "confidence": 0.9877669811248779, "text_region": [[216.0, 2036.0], [1164.0, 2036.0], [1164.0, 2082.0], [216.0, 2082.0]]}, {"text": "know' and are able to update their priors on the images they", "confidence": 0.9884440302848816, "text_region": [[213.0, 2079.0], [1164.0, 2082.0], [1164.0, 2138.0], [213.0, 2135.0]]}, {"text": "are most uncertain in based on a single example. Given this,", "confidence": 0.9888136982917786, "text_region": [[216.0, 2135.0], [1164.0, 2135.0], [1164.0, 2181.0], [216.0, 2181.0]]}, {"text": "it seems that while CLIP is a promising training strategy", "confidence": 0.9909034371376038, "text_region": [[210.0, 2171.0], [1164.0, 2181.0], [1164.0, 2238.0], [209.0, 2227.0]]}, {"text": "for zero-shot performance (Figure 5) and does well on tests", "confidence": 0.9970366358757019, "text_region": [[216.0, 2224.0], [1164.0, 2228.0], [1164.0, 2274.0], [216.0, 2270.0]]}, {"text": " of natural distribution shift (Figure 13), there is a large", "confidence": 0.9930465221405029, "text_region": [[210.0, 2270.0], [1168.0, 2274.0], [1167.0, 2330.0], [209.0, 2326.0]]}, {"text": " difference between how humans learn from a few examples", "confidence": 0.9872266054153442, "text_region": [[213.0, 2323.0], [1164.0, 2327.0], [1164.0, 2373.0], [213.0, 2369.0]]}, {"text": " and the few-shot methods in this paper.", "confidence": 0.9888954162597656, "text_region": [[210.0, 2366.0], [852.0, 2369.0], [851.0, 2426.0], [209.0, 2422.0]]}], "img_idx": 0, "score": 0.9908554553985596}
{"type": "text", "bbox": [218, 1232, 1160, 1526], "res": [{"text": "Figure 15. Few-shot CLIP also increases effective robustness", "confidence": 0.9961205124855042, "text_region": [[220.0, 1228.0], [1164.0, 1228.0], [1164.0, 1270.0], [220.0, 1270.0]]}, {"text": "compared to existing ImageNet models but is less robust than", "confidence": 0.9913479089736938, "text_region": [[216.0, 1270.0], [1164.0, 1270.0], [1164.0, 1317.0], [216.0, 1317.0]]}, {"text": "zero-shot CLIP. Minimizing the amount of ImageNet training", "confidence": 0.9877448678016663, "text_region": [[216.0, 1313.0], [1168.0, 1317.0], [1167.0, 1363.0], [216.0, 1360.0]]}, {"text": "data used for adaption increases effective robustness at the cost of", "confidence": 0.9831344485282898, "text_region": [[220.0, 1360.0], [1167.0, 1360.0], [1167.0, 1402.0], [220.0, 1402.0]]}, {"text": "decreasing relative robustness. 16-shot logistic regression CLIP", "confidence": 0.9918805360794067, "text_region": [[216.0, 1402.0], [1164.0, 1402.0], [1164.0, 1449.0], [216.0, 1449.0]]}, {"text": "matches zero-shot CLIP on ImageNet, as previously reported in", "confidence": 0.9969073534011841, "text_region": [[220.0, 1445.0], [1161.0, 1445.0], [1161.0, 1492.0], [220.0, 1492.0]]}, {"text": "Figure 7, but is less robust.", "confidence": 0.9814037680625916, "text_region": [[220.0, 1492.0], [609.0, 1492.0], [609.0, 1534.0], [220.0, 1534.0]]}], "img_idx": 0, "score": 0.989549994468689}
{"type": "text", "bbox": [1229, 2380, 2169, 2510], "res": [{"text": " Instead, we document how much overlap occurs and how", "confidence": 0.9940552711486816, "text_region": [[1221.0, 2369.0], [2169.0, 2373.0], [2169.0, 2419.0], [1221.0, 2416.0]]}, {"text": "performance changes due to these overlaps. To do this, we", "confidence": 0.9905260801315308, "text_region": [[1227.0, 2422.0], [2169.0, 2422.0], [2169.0, 2468.0], [1227.0, 2468.0]]}, {"text": "use the following procedure:", "confidence": 0.9982231259346008, "text_region": [[1224.0, 2465.0], [1690.0, 2465.0], [1690.0, 2521.0], [1224.0, 2521.0]]}], "img_idx": 0, "score": 0.9822822213172913}
{"type": "text", "bbox": [1226, 1085, 2167, 1265], "res": [{"text": "quality pre-trained model is near state-of-the-art for few", "confidence": 0.9897392392158508, "text_region": [[1224.0, 1082.0], [2169.0, 1082.0], [2169.0, 1129.0], [1224.0, 1129.0]]}, {"text": "shot learning (Tian et al., 2020), which suggests that there is", "confidence": 0.9967635869979858, "text_region": [[1221.0, 1125.0], [2172.0, 1125.0], [2172.0, 1181.0], [1221.0, 1181.0]]}, {"text": "a gap between the best few-shot machine learning methods", "confidence": 0.9924998879432678, "text_region": [[1224.0, 1178.0], [2172.0, 1178.0], [2172.0, 1224.0], [1224.0, 1224.0]]}, {"text": " and human few-shot learning.", "confidence": 0.9947585463523865, "text_region": [[1218.0, 1218.0], [1710.0, 1221.0], [1709.0, 1277.0], [1217.0, 1274.0]]}], "img_idx": 0, "score": 0.9809659123420715}
{"type": "text", "bbox": [218, 1685, 1161, 1766], "res": [{"text": "and 97-100% accuracy on the subset of attention check", "confidence": 0.9888275265693665, "text_region": [[216.0, 1680.0], [1161.0, 1680.0], [1161.0, 1726.0], [216.0, 1726.0]]}, {"text": "images increased our trust in the human workers.", "confidence": 0.9986770153045654, "text_region": [[213.0, 1723.0], [1008.0, 1719.0], [1008.0, 1775.0], [213.0, 1779.0]]}], "img_idx": 0, "score": 0.9549461007118225}
{"type": "text", "bbox": [228, 193, 1125, 224], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9977254271507263, "text_region": [[220.0, 188.0], [1370.0, 188.0], [1370.0, 231.0], [220.0, 231.0]]}], "img_idx": 0, "score": 0.6725760102272034}
{"type": "title", "bbox": [1229, 1651, 1741, 1692], "res": [{"text": "5. Data Overlap Analysis", "confidence": 0.9931421279907227, "text_region": [[1224.0, 1647.0], [1743.0, 1647.0], [1743.0, 1693.0], [1224.0, 1693.0]]}], "img_idx": 0, "score": 0.9423669576644897}
{"type": "figure", "bbox": [196, 255, 1174, 1152], "res": [{"text": "75", "confidence": 0.9998862147331238, "text_region": [[259.0, 290.0], [289.0, 290.0], [289.0, 317.0], [259.0, 317.0]]}, {"text": "%", "confidence": 0.8082177042961121, "text_region": [[220.0, 317.0], [246.0, 317.0], [246.0, 350.0], [220.0, 350.0]]}, {"text": "70 -", "confidence": 0.8923094868659973, "text_region": [[220.0, 356.0], [303.0, 356.0], [303.0, 406.0], [220.0, 406.0]]}, {"text": "65", "confidence": 0.9999433159828186, "text_region": [[256.0, 442.0], [289.0, 442.0], [289.0, 472.0], [256.0, 472.0]]}, {"text": "45", "confidence": 0.9999060034751892, "text_region": [[259.0, 706.0], [293.0, 706.0], [293.0, 733.0], [259.0, 733.0]]}, {"text": "40", "confidence": 0.9996541738510132, "text_region": [[259.0, 772.0], [289.0, 772.0], [289.0, 799.0], [259.0, 799.0]]}, {"text": " 30 :", "confidence": 0.8754974603652954, "text_region": [[223.0, 904.0], [299.0, 904.0], [299.0, 940.0], [223.0, 940.0]]}, {"text": "nodel (y =x)", "confidence": 0.9196520447731018, "text_region": [[994.0, 908.0], [1121.0, 908.0], [1121.0, 931.0], [994.0, 931.0]]}, {"text": "Few-Shot CLIP (best model)", "confidence": 0.9673525094985962, "text_region": [[851.0, 931.0], [1138.0, 931.0], [1138.0, 964.0], [851.0, 964.0]]}, {"text": "Zero-Shot CLIP (best model)", "confidence": 0.9975823163986206, "text_region": [[848.0, 964.0], [1141.0, 964.0], [1141.0, 997.0], [848.0, 997.0]]}, {"text": "25", "confidence": 0.9998459815979004, "text_region": [[256.0, 997.0], [293.0, 997.0], [293.0, 1023.0], [256.0, 1023.0]]}, {"text": "Standard ImageNet training", "confidence": 0.9967527389526367, "text_region": [[851.0, 993.0], [1141.0, 993.0], [1141.0, 1026.0], [851.0, 1026.0]]}, {"text": "Robustnes intervention", "confidence": 0.9771961569786072, "text_region": [[846.0, 1013.0], [1105.0, 1020.0], [1104.0, 1063.0], [844.0, 1056.0]]}, {"text": "Trained with more data", "confidence": 0.9870836734771729, "text_region": [[848.0, 1053.0], [1094.0, 1053.0], [1094.0, 1086.0], [848.0, 1086.0]]}, {"text": "20", "confidence": 0.9990673065185547, "text_region": [[259.0, 1089.0], [296.0, 1089.0], [296.0, 1112.0], [259.0, 1112.0]]}, {"text": "65", "confidence": 0.9998135566711426, "text_region": [[289.0, 1115.0], [329.0, 1115.0], [329.0, 1138.0], [289.0, 1138.0]]}, {"text": "70", "confidence": 0.9995786547660828, "text_region": [[369.0, 1115.0], [409.0, 1115.0], [409.0, 1142.0], [369.0, 1142.0]]}, {"text": "75", "confidence": 0.9999099969863892, "text_region": [[456.0, 1112.0], [492.0, 1112.0], [492.0, 1142.0], [456.0, 1142.0]]}, {"text": "80", "confidence": 0.9988186955451965, "text_region": [[555.0, 1112.0], [595.0, 1112.0], [595.0, 1142.0], [555.0, 1142.0]]}, {"text": "85", "confidence": 0.9995831251144409, "text_region": [[675.0, 1112.0], [715.0, 1112.0], [715.0, 1142.0], [675.0, 1142.0]]}, {"text": "90", "confidence": 0.999580979347229, "text_region": [[838.0, 1112.0], [878.0, 1112.0], [878.0, 1142.0], [838.0, 1142.0]]}, {"text": "95", "confidence": 0.9999237656593323, "text_region": [[1104.0, 1112.0], [1138.0, 1112.0], [1138.0, 1138.0], [1104.0, 1138.0]]}, {"text": " Average on class subsampled ImageNet (top-1, %)", "confidence": 0.9907267689704895, "text_region": [[399.0, 1135.0], [1061.0, 1135.0], [1061.0, 1181.0], [399.0, 1181.0]]}], "img_idx": 0, "score": 0.9568866491317749}
{"type": "figure_caption", "bbox": [403, 1142, 1055, 1175], "res": [{"text": "70", "confidence": 0.9995786547660828, "text_region": [[369.0, 1115.0], [409.0, 1115.0], [409.0, 1142.0], [369.0, 1142.0]]}, {"text": "75", "confidence": 0.9999099969863892, "text_region": [[456.0, 1112.0], [492.0, 1112.0], [492.0, 1142.0], [456.0, 1142.0]]}, {"text": "80", "confidence": 0.9988186955451965, "text_region": [[555.0, 1112.0], [595.0, 1112.0], [595.0, 1142.0], [555.0, 1142.0]]}, {"text": "85", "confidence": 0.9995831251144409, "text_region": [[675.0, 1112.0], [715.0, 1112.0], [715.0, 1142.0], [675.0, 1142.0]]}, {"text": "90", "confidence": 0.999580979347229, "text_region": [[838.0, 1112.0], [878.0, 1112.0], [878.0, 1142.0], [838.0, 1142.0]]}, {"text": " Average on class subsampled ImageNet (top-1, %)", "confidence": 0.9907267689704895, "text_region": [[399.0, 1135.0], [1061.0, 1135.0], [1061.0, 1181.0], [399.0, 1181.0]]}], "img_idx": 0, "score": 0.6826313734054565}
{"type": "table", "bbox": [1220, 302, 2175, 589], "res": {"cell_bbox": [[145.11024475097656, 52.19451141357422, 266.94708251953125, 51.79682159423828, 266.1571350097656, 79.4554672241211, 141.4443359375, 79.23848724365234], [240.87107849121094, 48.6641845703125, 367.6485595703125, 47.6167106628418, 370.3030700683594, 98.11869812011719, 241.0362091064453, 99.0582504272461], [391.1529541015625, 38.53485107421875, 558.49560546875, 37.675811767578125, 561.7811889648438, 101.26606750488281, 393.43206787109375, 102.62728118896484], [592.5140991210938, 39.447662353515625, 725.2095947265625, 38.699180603027344, 727.2728271484375, 100.73462677001953, 595.138916015625, 101.82321166992188], [760.0989379882812, 24.520366668701172, 930.1779174804688, 24.61009407043457, 929.9614868164062, 115.39108276367188, 760.0438232421875, 116.03013610839844], [30.911571502685547, 138.65631103515625, 224.4109344482422, 138.83920288085938, 222.89540100097656, 165.9426727294922, 29.94737434387207, 165.72560119628906], [287.46356201171875, 136.61871337890625, 334.4745178222656, 136.45516967773438, 334.4769287109375, 163.5281524658203, 285.96331787109375, 163.73211669921875], [452.7044982910156, 135.5902099609375, 531.3162231445312, 135.42506408691406, 532.1841430664062, 163.27989196777344, 452.5039978027344, 163.6758575439453], [634.226806640625, 135.2523956298828, 707.1618041992188, 134.75408935546875, 707.6917724609375, 163.78765869140625, 634.104736328125, 164.48756408691406], [809.3690795898438, 133.57858276367188, 875.40087890625, 132.760498046875, 875.0947875976562, 161.91607666015625, 808.4800415039062, 163.1221160888672], [25.751970291137695, 171.53375244140625, 210.60353088378906, 171.3731689453125, 210.53306579589844, 198.1439971923828, 25.526159286499023, 198.4597625732422], [289.4500732421875, 170.57632446289062, 334.8203125, 170.40147399902344, 334.834716796875, 197.6830291748047, 288.596435546875, 198.0032501220703], [455.3790283203125, 170.12168884277344, 520.992919921875, 170.0921173095703, 521.1804809570312, 198.26600646972656, 455.0792541503906, 198.61805725097656], [634.6915893554688, 169.457763671875, 698.9108276367188, 169.2767333984375, 699.1026000976562, 198.17156982421875, 634.7825317382812, 198.58523559570312], [806.9896240234375, 169.32254028320312, 872.2500610351562, 168.79873657226562, 871.69775390625, 197.7556610107422, 806.2046508789062, 198.48561096191406], [24.438533782958984, 202.11065673828125, 196.31898498535156, 202.28179931640625, 196.01817321777344, 231.0707244873047, 24.190616607666016, 230.9916229248047], [293.3123474121094, 203.0511474609375, 334.4995422363281, 203.3407440185547, 333.346923828125, 230.99534606933594, 291.2727355957031, 230.84413146972656], [455.31170654296875, 203.1849822998047, 518.5433349609375, 203.39334106445312, 518.005615234375, 231.22964477539062, 453.833251953125, 231.250732421875], [635.9642944335938, 202.63003540039062, 695.07421875, 202.6614227294922, 694.7363891601562, 231.13473510742188, 635.1982421875, 231.20794677734375], [810.5759887695312, 202.82752990722656, 872.8156127929688, 202.68089294433594, 872.3837280273438, 230.9033660888672, 809.9802856445312, 231.09962463378906], [24.621410369873047, 234.61729431152344, 205.3227996826172, 234.97198486328125, 205.44525146484375, 264.5465393066406, 24.242351531982422, 264.2852783203125], [297.51129150390625, 235.4492645263672, 336.48895263671875, 235.8801727294922, 335.0712890625, 262.9729309082031, 294.9329833984375, 262.6910705566406], [461.19866943359375, 235.96884155273438, 517.5897216796875, 236.25604248046875, 517.3303833007812, 263.84124755859375, 459.4943542480469, 263.7057800292969], [642.6994018554688, 235.9437255859375, 692.1895141601562, 236.04721069335938, 691.5286865234375, 264.42279052734375, 641.5269775390625, 264.3541564941406], [808.9833374023438, 236.6123809814453, 870.7149658203125, 236.56520080566406, 870.2167358398438, 264.4036865234375, 808.3199462890625, 264.41119384765625]], "html": "<html><body><table><thead><tr><td></td><td>Accuracy</td><td>Majority Vote on Full Dataset</td><td>Accuracy on Guesses</td><td>Majority Vote Accuracy on Guesses</td></tr></thead><tbody><tr><td>Zero-shot human</td><td>53.7</td><td>57.0</td><td>69.7</td><td>63.9</td></tr><tr><td>Zero-shot CLIP</td><td>93.5</td><td>93.5</td><td>93.5</td><td>93.5</td></tr><tr><td>One-shot human</td><td>75.7</td><td>80.3</td><td>78.5</td><td>81.2</td></tr><tr><td>Two-shot human</td><td>75.7</td><td>85.0</td><td>79.2</td><td>86.1</td></tr></tbody></table></body></html>"}, "img_idx": 0, "score": 0.9542580842971802}
{"type": "header", "bbox": [2129, 193, 2163, 218], "res": [{"text": "17", "confidence": 0.9998793601989746, "text_region": [[2129.0, 191.0], [2169.0, 191.0], [2169.0, 221.0], [2129.0, 221.0]]}], "img_idx": 0, "score": 0.9094043374061584}
