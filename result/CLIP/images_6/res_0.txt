{"type": "text", "bbox": [219, 859, 1160, 1701], "res": [{"text": "CLIP is a significant step towards fexible and practical", "confidence": 0.9971478581428528, "text_region": [[216.0, 851.0], [1161.0, 851.0], [1161.0, 898.0], [216.0, 898.0]]}, {"text": "zero-shot computer vision classifiers. As mentioned above,", "confidence": 0.9918870329856873, "text_region": [[216.0, 898.0], [1164.0, 898.0], [1164.0, 944.0], [216.0, 944.0]]}, {"text": "the comparison to Visual N-Grams is meant for contextu-", "confidence": 0.9911237359046936, "text_region": [[220.0, 947.0], [1167.0, 947.0], [1167.0, 993.0], [220.0, 993.0]]}, {"text": "alizing the performance of CLIP and should not be inter-", "confidence": 0.9980579018592834, "text_region": [[220.0, 997.0], [1161.0, 997.0], [1161.0, 1043.0], [220.0, 1043.0]]}, {"text": " preted as a direct methods comparison between CLIP and", "confidence": 0.9870423674583435, "text_region": [[213.0, 1040.0], [1164.0, 1033.0], [1164.0, 1089.0], [213.0, 1096.0]]}, {"text": "Visual N-Grams as many performance relevant differences", "confidence": 0.9919633269309998, "text_region": [[216.0, 1082.0], [1164.0, 1086.0], [1164.0, 1142.0], [216.0, 1138.0]]}, {"text": "between the two systems were not controlled for. For in-", "confidence": 0.9978943467140198, "text_region": [[220.0, 1138.0], [1167.0, 1138.0], [1167.0, 1185.0], [220.0, 1185.0]]}, {"text": "stance, we train on a dataset that is 10x larger, use a vision", "confidence": 0.989996075630188, "text_region": [[216.0, 1188.0], [1164.0, 1188.0], [1164.0, 1234.0], [216.0, 1234.0]]}, {"text": "model that requires nearly 100x more compute per predic-", "confidence": 0.9813404083251953, "text_region": [[216.0, 1231.0], [1167.0, 1231.0], [1167.0, 1287.0], [216.0, 1287.0]]}, {"text": "tion, likely used over 1000x their training compute, and", "confidence": 0.9860865473747253, "text_region": [[220.0, 1284.0], [1164.0, 1284.0], [1164.0, 1330.0], [220.0, 1330.0]]}, {"text": "use a transformer-based model which did not exist when", "confidence": 0.9949799180030823, "text_region": [[220.0, 1330.0], [1161.0, 1330.0], [1161.0, 1376.0], [220.0, 1376.0]]}, {"text": "Visual N-Grams was published. As a closer comparison, we", "confidence": 0.9905868172645569, "text_region": [[220.0, 1379.0], [1161.0, 1379.0], [1161.0, 1426.0], [220.0, 1426.0]]}, {"text": "trained a CLIP ResNet-50 on the same YFCC100M dataset", "confidence": 0.9833754301071167, "text_region": [[220.0, 1426.0], [1164.0, 1426.0], [1164.0, 1472.0], [220.0, 1472.0]]}, {"text": "that Visual N-Grams was trained on and found it matched", "confidence": 0.9962369799613953, "text_region": [[220.0, 1475.0], [1161.0, 1475.0], [1161.0, 1521.0], [220.0, 1521.0]]}, {"text": "their reported ImageNet performance within a V100 GPU", "confidence": 0.9889014363288879, "text_region": [[216.0, 1521.0], [1161.0, 1518.0], [1161.0, 1564.0], [216.0, 1568.0]]}, {"text": "day. This baseline was also trained from scratch instead of", "confidence": 0.9951890110969543, "text_region": [[220.0, 1571.0], [1161.0, 1571.0], [1161.0, 1617.0], [220.0, 1617.0]]}, {"text": "being initialized from pre-trained ImageNet weights as in", "confidence": 0.9871851801872253, "text_region": [[216.0, 1614.0], [1164.0, 1614.0], [1164.0, 1670.0], [216.0, 1670.0]]}, {"text": "Visual N-Grams.", "confidence": 0.9990847706794739, "text_region": [[220.0, 1667.0], [492.0, 1667.0], [492.0, 1713.0], [220.0, 1713.0]]}], "img_idx": 0, "score": 0.9937039613723755}
{"type": "text", "bbox": [220, 1740, 1159, 2160], "res": [{"text": "CLIP also outperforms Visual N-Grams on the other 2 re-", "confidence": 0.990871012210846, "text_region": [[216.0, 1732.0], [1167.0, 1732.0], [1167.0, 1789.0], [216.0, 1789.0]]}, {"text": "ported datasets. On aYahoo, CLIP achieves a 95% reduction", "confidence": 0.9985527992248535, "text_region": [[216.0, 1785.0], [1161.0, 1782.0], [1161.0, 1828.0], [216.0, 1832.0]]}, {"text": "in the number of errors, and on SUN, CLIP more than dou-", "confidence": 0.9915874600410461, "text_region": [[220.0, 1835.0], [1164.0, 1835.0], [1164.0, 1878.0], [220.0, 1878.0]]}, {"text": "bles the accuracy of Visual N-Grams. To conduct a more", "confidence": 0.9995806217193604, "text_region": [[220.0, 1881.0], [1161.0, 1881.0], [1161.0, 1927.0], [220.0, 1927.0]]}, {"text": "comprehensive analysis and stress test, we implement a", "confidence": 0.9823265671730042, "text_region": [[220.0, 1930.0], [1161.0, 1930.0], [1161.0, 1977.0], [220.0, 1977.0]]}, {"text": "much larger evaluation suite detailed in Appendix A. In", "confidence": 0.980644166469574, "text_region": [[220.0, 1977.0], [1164.0, 1977.0], [1164.0, 2023.0], [220.0, 2023.0]]}, {"text": "total we expand from the 3 datasets reported in Visual N-", "confidence": 0.9986873865127563, "text_region": [[213.0, 2020.0], [1171.0, 2016.0], [1171.0, 2072.0], [213.0, 2076.0]]}, {"text": "Grams to include over 30 datasets and compare to over 50", "confidence": 0.9932012557983398, "text_region": [[216.0, 2069.0], [1164.0, 2072.0], [1164.0, 2119.0], [216.0, 2115.0]]}, {"text": "existing computer vision systems to contextualize results.", "confidence": 0.9960172772407532, "text_region": [[216.0, 2122.0], [1138.0, 2122.0], [1138.0, 2168.0], [216.0, 2168.0]]}], "img_idx": 0, "score": 0.9915926456451416}
{"type": "text", "bbox": [1226, 2400, 2167, 2866], "res": [{"text": "Another issue we encountered is that it's relatively rare in", "confidence": 0.9935819506645203, "text_region": [[1227.0, 2396.0], [2169.0, 2396.0], [2169.0, 2442.0], [1227.0, 2442.0]]}, {"text": "our pre-training dataset for the text paired with the image", "confidence": 0.9837204813957214, "text_region": [[1227.0, 2445.0], [2169.0, 2445.0], [2169.0, 2492.0], [1227.0, 2492.0]]}, {"text": "to be just a single word. Usually the text is a full sentence", "confidence": 0.9821489453315735, "text_region": [[1227.0, 2492.0], [2169.0, 2492.0], [2169.0, 2538.0], [1227.0, 2538.0]]}, {"text": "describing the image in some way. To help bridge this", "confidence": 0.9774765968322754, "text_region": [[1221.0, 2538.0], [2172.0, 2538.0], [2172.0, 2594.0], [1221.0, 2594.0]]}, {"text": " distribution gap, we found that using the prompt template", "confidence": 0.9979168176651001, "text_region": [[1217.0, 2581.0], [2175.0, 2584.0], [2175.0, 2640.0], [1217.0, 2637.0]]}, {"text": "\"A photo of a {label}.\u201d to be a good default that", "confidence": 0.9557395577430725, "text_region": [[1224.0, 2637.0], [2172.0, 2637.0], [2172.0, 2683.0], [1224.0, 2683.0]]}, {"text": "helps specify the text is about the content of the image. This", "confidence": 0.9850466251373291, "text_region": [[1221.0, 2680.0], [2172.0, 2680.0], [2172.0, 2736.0], [1221.0, 2736.0]]}, {"text": " often improves performance over the baseline of using only", "confidence": 0.9918528199195862, "text_region": [[1224.0, 2736.0], [2165.0, 2736.0], [2165.0, 2779.0], [1224.0, 2779.0]]}, {"text": " the label text. For instance, just using this prompt improves", "confidence": 0.9860517978668213, "text_region": [[1217.0, 2772.0], [2172.0, 2775.0], [2172.0, 2831.0], [1217.0, 2828.0]]}, {"text": "accuracy on ImageNet by 1.3%.", "confidence": 0.9877218008041382, "text_region": [[1224.0, 2828.0], [1740.0, 2828.0], [1740.0, 2874.0], [1224.0, 2874.0]]}], "img_idx": 0, "score": 0.990794837474823}
{"type": "text", "bbox": [220, 2295, 1158, 2710], "res": [{"text": "Most standard image classification datasets treat the infor-", "confidence": 0.9911342263221741, "text_region": [[220.0, 2287.0], [1161.0, 2287.0], [1161.0, 2333.0], [220.0, 2333.0]]}, {"text": "mation naming or describing classes which enables natural", "confidence": 0.9858822226524353, "text_region": [[216.0, 2333.0], [1164.0, 2333.0], [1164.0, 2389.0], [216.0, 2389.0]]}, {"text": "language based zero-shot transfer as an afterthought. The", "confidence": 0.9812876582145691, "text_region": [[220.0, 2386.0], [1161.0, 2386.0], [1161.0, 2432.0], [220.0, 2432.0]]}, {"text": "vast majority of datasets annotate images with just a numeric", "confidence": 0.997890830039978, "text_region": [[220.0, 2435.0], [1164.0, 2435.0], [1164.0, 2478.0], [220.0, 2478.0]]}, {"text": "id of the label and contain a file mapping these ids back to", "confidence": 0.9852361083030701, "text_region": [[216.0, 2482.0], [1164.0, 2482.0], [1164.0, 2528.0], [216.0, 2528.0]]}, {"text": "their names in English. Some datasets, such as Flowers102", "confidence": 0.9995765089988708, "text_region": [[216.0, 2528.0], [1161.0, 2528.0], [1161.0, 2574.0], [216.0, 2574.0]]}, {"text": "and GTSRB, don't appear to include this mapping at all", "confidence": 0.9933578968048096, "text_region": [[220.0, 2577.0], [1161.0, 2577.0], [1161.0, 2624.0], [220.0, 2624.0]]}, {"text": "in their released versions preventing zero-shot transfer en-", "confidence": 0.9952128529548645, "text_region": [[213.0, 2617.0], [1171.0, 2620.0], [1171.0, 2676.0], [213.0, 2673.0]]}, {"text": "tirely.2 For many datasets, we observed these labels may be", "confidence": 0.9867720007896423, "text_region": [[216.0, 2673.0], [1164.0, 2673.0], [1164.0, 2719.0], [216.0, 2719.0]]}], "img_idx": 0, "score": 0.9898651242256165}
{"type": "text", "bbox": [221, 541, 1165, 748], "res": [{"text": "Table 1. Comparing CLIP to prior zero-shot transfer image classi-", "confidence": 0.9925316572189331, "text_region": [[220.0, 538.0], [1164.0, 538.0], [1164.0, 581.0], [220.0, 581.0]]}, {"text": "fication results. CLIP improves performance on all three datasets", "confidence": 0.9996204376220703, "text_region": [[220.0, 581.0], [1164.0, 581.0], [1164.0, 627.0], [220.0, 627.0]]}, {"text": "by a large amount. This improvement reflects many differences", "confidence": 0.997703492641449, "text_region": [[220.0, 624.0], [1157.0, 624.0], [1157.0, 670.0], [220.0, 670.0]]}, {"text": "in the 4 years since the development of Visual N-Grams (Li et al.,", "confidence": 0.9771682024002075, "text_region": [[220.0, 667.0], [1167.0, 667.0], [1167.0, 713.0], [220.0, 713.0]]}, {"text": "2017).", "confidence": 0.9999423027038574, "text_region": [[220.0, 716.0], [319.0, 716.0], [319.0, 752.0], [220.0, 752.0]]}], "img_idx": 0, "score": 0.9887033700942993}
{"type": "text", "bbox": [1225, 1236, 2166, 1534], "res": [{"text": "Figure 4. Prompt engineering and ensembling improve zero-", "confidence": 0.99884432554245, "text_region": [[1224.0, 1228.0], [2175.0, 1228.0], [2175.0, 1274.0], [1224.0, 1274.0]]}, {"text": "shot performance. Compared to the baseline of using contextless", "confidence": 0.9952100515365601, "text_region": [[1224.0, 1274.0], [2172.0, 1274.0], [2172.0, 1320.0], [1224.0, 1320.0]]}, {"text": "class names, prompt engineering and ensembling boost zero-shot", "confidence": 0.9909653067588806, "text_region": [[1227.0, 1320.0], [2172.0, 1320.0], [2172.0, 1366.0], [1227.0, 1366.0]]}, {"text": "classification performance by almost 5 points on average across", "confidence": 0.9889289140701294, "text_region": [[1224.0, 1356.0], [2169.0, 1363.0], [2168.0, 1409.0], [1224.0, 1402.0]]}, {"text": "36 datasets. This improvement is similar to the gain from using", "confidence": 0.9869837164878845, "text_region": [[1224.0, 1406.0], [2172.0, 1406.0], [2172.0, 1452.0], [1224.0, 1452.0]]}, {"text": "4 times more compute with the baseline zero-shot method but is", "confidence": 0.9917644262313843, "text_region": [[1224.0, 1449.0], [2172.0, 1449.0], [2172.0, 1495.0], [1224.0, 1495.0]]}, {"text": "\"free\u201d when amortized over many predictions.", "confidence": 0.9792358875274658, "text_region": [[1221.0, 1488.0], [1889.0, 1492.0], [1889.0, 1538.0], [1221.0, 1534.0]]}], "img_idx": 0, "score": 0.9879167079925537}
{"type": "text", "bbox": [218, 2754, 1157, 2864], "res": [{"text": "2 Alec learned much more about flower species and German", "confidence": 0.9847943186759949, "text_region": [[263.0, 2739.0], [1161.0, 2746.0], [1161.0, 2802.0], [263.0, 2795.0]]}, {"text": "traffic signs over the course of this project than he originally antic-", "confidence": 0.9773316979408264, "text_region": [[213.0, 2785.0], [1161.0, 2789.0], [1161.0, 2835.0], [213.0, 2831.0]]}, {"text": "ipated.", "confidence": 0.9998798370361328, "text_region": [[220.0, 2835.0], [323.0, 2835.0], [323.0, 2871.0], [220.0, 2871.0]]}], "img_idx": 0, "score": 0.9715381860733032}
{"type": "text", "bbox": [1227, 1687, 2167, 1814], "res": [{"text": "chosen somewhat haphazardly and do not anticipate issues", "confidence": 0.9988764524459839, "text_region": [[1227.0, 1680.0], [2165.0, 1680.0], [2165.0, 1726.0], [1227.0, 1726.0]]}, {"text": "related to zero-shot transfer which relies on task description", "confidence": 0.9843619465827942, "text_region": [[1227.0, 1723.0], [2165.0, 1723.0], [2165.0, 1769.0], [1227.0, 1769.0]]}, {"text": "in order to transfer successfully.", "confidence": 0.9670600891113281, "text_region": [[1224.0, 1775.0], [1740.0, 1775.0], [1740.0, 1822.0], [1224.0, 1822.0]]}], "img_idx": 0, "score": 0.968166708946228}
{"type": "text", "bbox": [1226, 1853, 2167, 2371], "res": [{"text": "A common issue is polysemy. When the name of a class", "confidence": 0.9872812628746033, "text_region": [[1224.0, 1848.0], [2169.0, 1848.0], [2169.0, 1894.0], [1224.0, 1894.0]]}, {"text": " is the only information provided to CLIP's text encoder it", "confidence": 0.9948159456253052, "text_region": [[1217.0, 1894.0], [2172.0, 1887.0], [2172.0, 1940.0], [1218.0, 1947.0]]}, {"text": "is unable to differentiate which word sense is meant due to", "confidence": 0.9937013387680054, "text_region": [[1227.0, 1944.0], [2172.0, 1944.0], [2172.0, 1987.0], [1227.0, 1987.0]]}, {"text": "the lack of context. In some cases multiple meanings of the", "confidence": 0.99896639585495, "text_region": [[1224.0, 1990.0], [2169.0, 1990.0], [2169.0, 2036.0], [1224.0, 2036.0]]}, {"text": "same word might be included as different classes in the same", "confidence": 0.9948043823242188, "text_region": [[1227.0, 2039.0], [2169.0, 2039.0], [2169.0, 2086.0], [1227.0, 2086.0]]}, {"text": "dataset! This happens in ImageNet which contains both", "confidence": 0.9969087839126587, "text_region": [[1224.0, 2082.0], [2175.0, 2082.0], [2175.0, 2138.0], [1224.0, 2138.0]]}, {"text": "construction cranes and cranes that fy. Another example is", "confidence": 0.9864765405654907, "text_region": [[1227.0, 2132.0], [2169.0, 2132.0], [2169.0, 2178.0], [1227.0, 2178.0]]}, {"text": "found in classes of the Oxford-IIIT Pet dataset where the", "confidence": 0.9896055459976196, "text_region": [[1224.0, 2178.0], [2172.0, 2181.0], [2172.0, 2228.0], [1224.0, 2224.0]]}, {"text": "word boxer is, from context, clearly referring to a breed of", "confidence": 0.9919925332069397, "text_region": [[1227.0, 2228.0], [2172.0, 2228.0], [2172.0, 2274.0], [1227.0, 2274.0]]}, {"text": "dog, but to a text encoder lacking context could just as likely", "confidence": 0.9957044720649719, "text_region": [[1224.0, 2277.0], [2169.0, 2277.0], [2169.0, 2323.0], [1224.0, 2323.0]]}, {"text": "refer to a type of athlete.", "confidence": 0.9989793300628662, "text_region": [[1224.0, 2326.0], [1620.0, 2326.0], [1620.0, 2373.0], [1224.0, 2373.0]]}], "img_idx": 0, "score": 0.9676305055618286}
{"type": "text", "bbox": [292, 2221, 1051, 2251], "res": [{"text": "3.1.4. PROMPT ENGINEERING AND ENSEMBLING", "confidence": 0.9988144040107727, "text_region": [[220.0, 2214.0], [1054.0, 2214.0], [1054.0, 2257.0], [220.0, 2257.0]]}], "img_idx": 0, "score": 0.6280255913734436}
{"type": "figure", "bbox": [1208, 261, 2183, 1150], "res": [{"text": "70", "confidence": 0.999599814414978, "text_region": [[1267.0, 264.0], [1317.0, 264.0], [1317.0, 304.0], [1267.0, 304.0]]}, {"text": "RN50x64", "confidence": 0.9737812876701355, "text_region": [[1999.0, 297.0], [2099.0, 297.0], [2099.0, 330.0], [1999.0, 330.0]]}, {"text": "65", "confidence": 0.9999281764030457, "text_region": [[1267.0, 426.0], [1320.0, 426.0], [1320.0, 465.0], [1267.0, 465.0]]}, {"text": "5 point", "confidence": 0.9983564019203186, "text_region": [[1873.0, 528.0], [1962.0, 528.0], [1962.0, 561.0], [1873.0, 561.0]]}, {"text": "improvement", "confidence": 0.9993040561676025, "text_region": [[1873.0, 558.0], [2029.0, 558.0], [2029.0, 591.0], [1873.0, 591.0]]}, {"text": "60", "confidence": 0.9996263980865479, "text_region": [[1267.0, 584.0], [1317.0, 584.0], [1317.0, 624.0], [1267.0, 624.0]]}, {"text": "RN50x16", "confidence": 0.993354320526123, "text_region": [[1886.0, 660.0], [1992.0, 660.0], [1992.0, 696.0], [1886.0, 696.0]]}, {"text": "%", "confidence": 0.9182532429695129, "text_region": [[1227.0, 693.0], [1267.0, 693.0], [1267.0, 739.0], [1227.0, 739.0]]}, {"text": "55", "confidence": 0.9996989965438843, "text_region": [[1258.0, 748.0], [1311.0, 737.0], [1320.0, 778.0], [1266.0, 788.0]]}, {"text": "RN50x4", "confidence": 0.9775148034095764, "text_region": [[1630.0, 815.0], [1723.0, 815.0], [1723.0, 838.0], [1630.0, 838.0]]}, {"text": "RN101", "confidence": 0.9993199110031128, "text_region": [[1480.0, 893.0], [1561.0, 902.0], [1557.0, 942.0], [1475.0, 933.0]]}, {"text": "50-", "confidence": 0.7763592600822449, "text_region": [[1267.0, 908.0], [1324.0, 908.0], [1324.0, 947.0], [1267.0, 947.0]]}, {"text": " Contextless class names (Li et al. 2017) ", "confidence": 0.9763203859329224, "text_region": [[1650.0, 1026.0], [2142.0, 1026.0], [2142.0, 1069.0], [1650.0, 1069.0]]}, {"text": "45", "confidence": 0.9999608397483826, "text_region": [[1271.0, 1072.0], [1314.0, 1072.0], [1314.0, 1102.0], [1271.0, 1102.0]]}, {"text": "6.1", "confidence": 0.9995853304862976, "text_region": [[1337.0, 1099.0], [1394.0, 1099.0], [1394.0, 1135.0], [1337.0, 1135.0]]}, {"text": "9.9", "confidence": 0.9999011158943176, "text_region": [[1430.0, 1099.0], [1490.0, 1099.0], [1490.0, 1135.0], [1430.0, 1135.0]]}, {"text": "21.5", "confidence": 0.9994458556175232, "text_region": [[1580.0, 1099.0], [1656.0, 1099.0], [1656.0, 1135.0], [1580.0, 1135.0]]}, {"text": "75.3", "confidence": 0.999953031539917, "text_region": [[1836.0, 1099.0], [1909.0, 1099.0], [1909.0, 1135.0], [1836.0, 1135.0]]}, {"text": "265.9", "confidence": 0.9999807476997375, "text_region": [[2079.0, 1099.0], [2169.0, 1099.0], [2169.0, 1135.0], [2079.0, 1135.0]]}, {"text": "Model GFLOPs", "confidence": 0.982571542263031, "text_region": [[1630.0, 1142.0], [1856.0, 1142.0], [1856.0, 1178.0], [1630.0, 1178.0]]}], "img_idx": 0, "score": 0.9570696353912354}
{"type": "figure_caption", "bbox": [1638, 1146, 1849, 1170], "res": [{"text": "Model GFLOPs", "confidence": 0.982571542263031, "text_region": [[1630.0, 1142.0], [1856.0, 1142.0], [1856.0, 1178.0], [1630.0, 1178.0]]}], "img_idx": 0, "score": 0.546257734298706}
{"type": "table", "bbox": [258, 315, 1108, 506], "res": {"cell_bbox": [[66.76061248779297, 13.754213333129883, 515.2373046875, 13.637687683105469, 514.3576049804688, 54.48553466796875, 65.42777252197266, 55.100433349609375], [381.450927734375, 16.50946617126465, 600.703857421875, 16.060970306396484, 603.5155029296875, 52.89689636230469, 384.7399597167969, 53.89834213256836], [562.3887939453125, 18.15748405456543, 760.4037475585938, 17.844968795776367, 762.2008056640625, 57.69629669189453, 566.9619750976562, 58.30426025390625], [8.058074951171875, 75.66936492919922, 365.1871337890625, 75.65652465820312, 366.7171936035156, 136.83311462402344, 7.821898460388184, 136.7643280029297], [406.4968566894531, 78.67935943603516, 540.8015747070312, 78.7176513671875, 540.9699096679688, 131.90432739257812, 405.70074462890625, 131.8571319580078], [661.374755859375, 73.01163482666016, 796.51416015625, 72.88513946533203, 795.8629150390625, 125.92157745361328, 660.877685546875, 125.89989471435547], [68.22125244140625, 127.46665954589844, 294.09197998046875, 127.46587371826172, 291.3202209472656, 172.08450317382812, 66.91519165039062, 171.914306640625], [479.7328796386719, 125.79900360107422, 602.9733276367188, 125.75724792480469, 601.3475341796875, 167.7559814453125, 478.6985168457031, 167.59400939941406], [715.68310546875, 126.21553802490234, 810.175537109375, 126.14179992675781, 809.54931640625, 167.77500915527344, 715.31640625, 167.62977600097656]], "html": "<html><body><table><thead><tr><td></td><td>aYahoo</td><td>ImageNet SUN</td></tr></thead><tbody><tr><td>Visual N-Grams</td><td>72.4 98.4</td><td>23.0</td></tr><tr><td>CLIP</td><td>11.5 76.2</td><td>58.5</td></tr></tbody></table></body></html>"}, "img_idx": 0, "score": 0.9160321950912476}
{"type": "header", "bbox": [228, 194, 1124, 224], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9977254271507263, "text_region": [[220.0, 188.0], [1370.0, 188.0], [1370.0, 231.0], [220.0, 231.0]]}], "img_idx": 0, "score": 0.730149507522583}
{"type": "header", "bbox": [2144, 192, 2161, 221], "res": [], "img_idx": 0, "score": 0.6060490608215332}
