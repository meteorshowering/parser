{"type": "text", "bbox": [1227, 2169, 2168, 2825], "res": [{"text": "While zero-shot CLIP improves effective robustness, Figure", "confidence": 0.9765056371688843, "text_region": [[1224.0, 2158.0], [2169.0, 2165.0], [2168.0, 2211.0], [1224.0, 2204.0]]}, {"text": "14 shows that the benefit is almost entirely gone in a fully", "confidence": 0.986524760723114, "text_region": [[1224.0, 2211.0], [2169.0, 2211.0], [2169.0, 2257.0], [1224.0, 2257.0]]}, {"text": "supervised setting. To better understand this difference, we", "confidence": 0.9926809072494507, "text_region": [[1227.0, 2257.0], [2172.0, 2257.0], [2172.0, 2303.0], [1227.0, 2303.0]]}, {"text": "investigate how effective robustness changes on the contin-", "confidence": 0.9928205013275146, "text_region": [[1224.0, 2307.0], [2169.0, 2307.0], [2169.0, 2353.0], [1224.0, 2353.0]]}, {"text": "uum from zero-shot to fully supervised. In Figure 15 we", "confidence": 0.9925336837768555, "text_region": [[1224.0, 2356.0], [2169.0, 2356.0], [2169.0, 2402.0], [1224.0, 2402.0]]}, {"text": "visualize the performance of 0-shot, 1-shot, 2-shot, 4-shot ", "confidence": 0.9920962452888489, "text_region": [[1227.0, 2402.0], [2175.0, 2402.0], [2175.0, 2449.0], [1227.0, 2449.0]]}, {"text": "., 128-shot, and fully supervised logistic regression classi-", "confidence": 0.9559003114700317, "text_region": [[1234.0, 2442.0], [2179.0, 2449.0], [2178.0, 2502.0], [1234.0, 2495.0]]}, {"text": "fiers on the best CLIP model's features. We see that while", "confidence": 0.9881182312965393, "text_region": [[1224.0, 2495.0], [2172.0, 2495.0], [2172.0, 2541.0], [1224.0, 2541.0]]}, {"text": "few-shot models also show higher effective robustness than", "confidence": 0.9987787008285522, "text_region": [[1227.0, 2544.0], [2172.0, 2544.0], [2172.0, 2590.0], [1227.0, 2590.0]]}, {"text": "existing models, this benefit fades as in-distribution per-", "confidence": 0.9883973598480225, "text_region": [[1224.0, 2594.0], [2169.0, 2594.0], [2169.0, 2640.0], [1224.0, 2640.0]]}, {"text": " formance increases with more training data and is mostly,", "confidence": 0.9869047403335571, "text_region": [[1217.0, 2633.0], [2175.0, 2637.0], [2175.0, 2693.0], [1217.0, 2689.0]]}, {"text": "though not entirely, gone for the fully supervised model.", "confidence": 0.9965630173683167, "text_region": [[1224.0, 2693.0], [2172.0, 2693.0], [2172.0, 2736.0], [1224.0, 2736.0]]}, {"text": "Additionally, zero-shot CLIP is notably more robust than", "confidence": 0.9885239005088806, "text_region": [[1227.0, 2736.0], [2169.0, 2736.0], [2169.0, 2782.0], [1227.0, 2782.0]]}, {"text": "a few-shot model with equivalent ImageNet performance.", "confidence": 0.9981681108474731, "text_region": [[1224.0, 2785.0], [2172.0, 2785.0], [2172.0, 2831.0], [1224.0, 2831.0]]}], "img_idx": 0, "score": 0.9951476454734802}
{"type": "text", "bbox": [1227, 1426, 2168, 2131], "res": [{"text": "pooling predictions across all sub-classes according to the", "confidence": 0.9947375059127808, "text_region": [[1227.0, 1422.0], [2169.0, 1422.0], [2169.0, 1468.0], [1227.0, 1468.0]]}, {"text": " ImageNet class hierarchy. Sometimes this mapping is much", "confidence": 0.9803035259246826, "text_region": [[1221.0, 1465.0], [2175.0, 1465.0], [2175.0, 1521.0], [1221.0, 1521.0]]}, {"text": "less than perfect. For the person class in Youtube-BB, pre-", "confidence": 0.9873324036598206, "text_region": [[1217.0, 1508.0], [2172.0, 1511.0], [2172.0, 1568.0], [1217.0, 1564.0]]}, {"text": " dictions are made by pooling over the ImageNet classes for", "confidence": 0.9808587431907654, "text_region": [[1221.0, 1561.0], [2172.0, 1561.0], [2172.0, 1617.0], [1221.0, 1617.0]]}, {"text": "a baseball player, a bridegroom, and a scuba diver. With", "confidence": 0.9994171857833862, "text_region": [[1224.0, 1614.0], [2172.0, 1614.0], [2172.0, 1660.0], [1224.0, 1660.0]]}, {"text": "CLIP we can instead generate a custom zero-shot classi-", "confidence": 0.9979203939437866, "text_region": [[1224.0, 1660.0], [2175.0, 1660.0], [2175.0, 1706.0], [1224.0, 1706.0]]}, {"text": "fier for each dataset directly based on its class names. In", "confidence": 0.9937780499458313, "text_region": [[1224.0, 1706.0], [2175.0, 1706.0], [2175.0, 1752.0], [1224.0, 1752.0]]}, {"text": "Figure 14 we see that this improves average effective ro-", "confidence": 0.9946866631507874, "text_region": [[1224.0, 1756.0], [2175.0, 1756.0], [2175.0, 1802.0], [1224.0, 1802.0]]}, {"text": "bustness by 5% but is concentrated in large improvements", "confidence": 0.9875358939170837, "text_region": [[1227.0, 1805.0], [2169.0, 1805.0], [2169.0, 1851.0], [1227.0, 1851.0]]}, {"text": "on only a few datasets. Curiously, accuracy on ObjectNet", "confidence": 0.9909820556640625, "text_region": [[1224.0, 1855.0], [2172.0, 1855.0], [2172.0, 1901.0], [1224.0, 1901.0]]}, {"text": "also increases by 2.3%. Although the dataset was designed", "confidence": 0.995926022529602, "text_region": [[1224.0, 1901.0], [2172.0, 1901.0], [2172.0, 1944.0], [1224.0, 1944.0]]}, {"text": "to closely overlap with ImageNet classes, using the names", "confidence": 0.9944736957550049, "text_region": [[1224.0, 1947.0], [2169.0, 1947.0], [2169.0, 1993.0], [1224.0, 1993.0]]}, {"text": "provided for each class by ObjectNet's creators still helps a", "confidence": 0.9895480871200562, "text_region": [[1227.0, 1993.0], [2169.0, 1993.0], [2169.0, 2039.0], [1227.0, 2039.0]]}, {"text": "small amount compared to using ImageNet class names and", "confidence": 0.9924834370613098, "text_region": [[1227.0, 2043.0], [2172.0, 2043.0], [2172.0, 2089.0], [1227.0, 2089.0]]}, {"text": "pooling predictions when necessary.", "confidence": 0.9910306930541992, "text_region": [[1224.0, 2089.0], [1803.0, 2089.0], [1803.0, 2135.0], [1224.0, 2135.0]]}], "img_idx": 0, "score": 0.993983805179596}
{"type": "text", "bbox": [221, 1425, 1163, 1603], "res": [{"text": "ImageNet-R, 3.8% on ObjectNet, 2.8% on ImageNet Sketch,", "confidence": 0.9968392848968506, "text_region": [[216.0, 1422.0], [1164.0, 1422.0], [1164.0, 1465.0], [216.0, 1465.0]]}, {"text": "and 1.9% on ImageNet-A. The change in accuracy on the", "confidence": 0.9987714290618896, "text_region": [[216.0, 1468.0], [1164.0, 1468.0], [1164.0, 1515.0], [216.0, 1515.0]]}, {"text": "two other datasets, Youtube-BB and ImageNet Vid, is in-", "confidence": 0.990216076374054, "text_region": [[220.0, 1518.0], [1161.0, 1518.0], [1161.0, 1561.0], [220.0, 1561.0]]}, {"text": "significant.", "confidence": 0.9589384198188782, "text_region": [[216.0, 1564.0], [402.0, 1564.0], [402.0, 1610.0], [216.0, 1610.0]]}], "img_idx": 0, "score": 0.9875736236572266}
{"type": "text", "bbox": [219, 1061, 2167, 1321], "res": [{"text": "Figure 13. Zero-shot CLIP is much more robust to distribution shift than standard ImageNet models. (Left) An ideal robust model", "confidence": 0.9947912693023682, "text_region": [[216.0, 1059.0], [2172.0, 1059.0], [2172.0, 1102.0], [216.0, 1102.0]]}, {"text": "(dashed line) performs equally well on the ImageNet distribution and (", "confidence": 0.9896942377090454, "text_region": [[216.0, 1099.0], [1217.0, 1106.0], [1217.0, 1152.0], [216.0, 1145.0]]}, {"text": "1 on other natural image distributions. Zero-shot CLIP models shrink", "confidence": 0.9922522306442261, "text_region": [[1194.0, 1099.0], [2165.0, 1099.0], [2165.0, 1142.0], [1194.0, 1142.0]]}, {"text": "this \u201crobustness gap\" by up to 75%. Linear fits on logit transformed values are shown with bootstrap estimated 95% confidence intervals.", "confidence": 0.9879956841468811, "text_region": [[216.0, 1142.0], [2172.0, 1145.0], [2172.0, 1191.0], [216.0, 1188.0]]}, {"text": "(Right) Visualizing distribution shift for bananas, a class shared across 5 of the 7 natural distribution shift datasets. The performance of", "confidence": 0.9920883178710938, "text_region": [[216.0, 1191.0], [2175.0, 1191.0], [2175.0, 1238.0], [216.0, 1238.0]]}, {"text": "the best zero-shot CLIP model, ViT-L/14@336px, is compared with a model that has the same performance on the ImageNet validation", "confidence": 0.993469774723053, "text_region": [[216.0, 1234.0], [2172.0, 1234.0], [2172.0, 1280.0], [216.0, 1280.0]]}, {"text": "set, ResNet-101.", "confidence": 0.9991220831871033, "text_region": [[220.0, 1284.0], [459.0, 1284.0], [459.0, 1320.0], [220.0, 1320.0]]}], "img_idx": 0, "score": 0.9854348301887512}
{"type": "text", "bbox": [219, 1638, 1158, 2685], "res": [{"text": "How is it possible to improve accuracy by 9.2% on the Im-", "confidence": 0.9920044541358948, "text_region": [[220.0, 1637.0], [1164.0, 1637.0], [1164.0, 1683.0], [220.0, 1683.0]]}, {"text": "ageNet dataset with little to no increase in accuracy under", "confidence": 0.9755184650421143, "text_region": [[213.0, 1680.0], [1167.0, 1673.0], [1168.0, 1729.0], [213.0, 1736.0]]}, {"text": "distribution shift? Is the gain primarily from \u201cexploiting", "confidence": 0.9972261786460876, "text_region": [[220.0, 1732.0], [1161.0, 1732.0], [1161.0, 1779.0], [220.0, 1779.0]]}, {"text": "spurious correlations\"? Is this behavior unique to some com-", "confidence": 0.993503212928772, "text_region": [[216.0, 1779.0], [1167.0, 1779.0], [1167.0, 1825.0], [216.0, 1825.0]]}, {"text": "bination of CLIP, the ImageNet datatset, and the distribution", "confidence": 0.9941696524620056, "text_region": [[216.0, 1828.0], [1161.0, 1828.0], [1161.0, 1874.0], [216.0, 1874.0]]}, {"text": "shifts studied, or a more general phenomena? Does it hold", "confidence": 0.9953534603118896, "text_region": [[216.0, 1878.0], [1164.0, 1878.0], [1164.0, 1924.0], [216.0, 1924.0]]}, {"text": "for end-to-end finetuning as well as linear classifiers? We", "confidence": 0.9795669317245483, "text_region": [[216.0, 1924.0], [1161.0, 1924.0], [1161.0, 1970.0], [216.0, 1970.0]]}, {"text": "do not have confident answers to these questions at this time.", "confidence": 0.9986155033111572, "text_region": [[220.0, 1970.0], [1164.0, 1970.0], [1164.0, 2016.0], [220.0, 2016.0]]}, {"text": "Prior work has also pre-trained models on distributions other", "confidence": 0.9809778928756714, "text_region": [[220.0, 2020.0], [1161.0, 2020.0], [1161.0, 2062.0], [220.0, 2062.0]]}, {"text": "than ImageNet, but it is common to study and release mod-", "confidence": 0.989296555519104, "text_region": [[220.0, 2066.0], [1167.0, 2066.0], [1167.0, 2112.0], [220.0, 2112.0]]}, {"text": "els only after they have been fine-tuned to ImageNet. As a", "confidence": 0.9855212569236755, "text_region": [[216.0, 2115.0], [1164.0, 2115.0], [1164.0, 2162.0], [216.0, 2162.0]]}, {"text": "step towards understanding whether pre-trained zero-shot", "confidence": 0.991381049156189, "text_region": [[216.0, 2165.0], [1164.0, 2165.0], [1164.0, 2211.0], [216.0, 2211.0]]}, {"text": "models consistently have higher effective robustness than", "confidence": 0.9738184213638306, "text_region": [[220.0, 2211.0], [1164.0, 2211.0], [1164.0, 2254.0], [220.0, 2254.0]]}, {"text": "fine-tuned models, we encourage the authors of Mahajan", "confidence": 0.9992790222167969, "text_region": [[220.0, 2257.0], [1161.0, 2257.0], [1161.0, 2303.0], [220.0, 2303.0]]}, {"text": "et al. (2018), Kolesnikov et al. (2019), and Dosovitskiy et al.", "confidence": 0.9864157438278198, "text_region": [[220.0, 2307.0], [1164.0, 2307.0], [1164.0, 2353.0], [220.0, 2353.0]]}, {"text": "(2020) to, if possible, study these questions on their models ", "confidence": 0.9897539019584656, "text_region": [[216.0, 2353.0], [1167.0, 2353.0], [1167.0, 2399.0], [216.0, 2399.0]]}, {"text": "as well.", "confidence": 0.9983432292938232, "text_region": [[213.0, 2402.0], [349.0, 2402.0], [349.0, 2449.0], [213.0, 2449.0]]}, {"text": "We also investigate another robustness intervention enabled", "confidence": 0.9823565483093262, "text_region": [[210.0, 2465.0], [1168.0, 2468.0], [1167.0, 2525.0], [209.0, 2521.0]]}, {"text": "by flexible zero-shot natural-language-based image classi-", "confidence": 0.9874527454376221, "text_region": [[216.0, 2518.0], [1164.0, 2518.0], [1164.0, 2564.0], [216.0, 2564.0]]}, {"text": "fiers. The target classes across the 7 transfer datasets are", "confidence": 0.9938010573387146, "text_region": [[213.0, 2561.0], [1168.0, 2564.0], [1167.0, 2620.0], [213.0, 2617.0]]}, {"text": "not always perfectly aligned with those of ImageNet. Two", "confidence": 0.9769210815429688, "text_region": [[220.0, 2620.0], [1164.0, 2620.0], [1164.0, 2663.0], [220.0, 2663.0]]}, {"text": "datasets, Youtube-BB and ImageNet-Vid, consist of super-", "confidence": 0.9824285507202148, "text_region": [[220.0, 2666.0], [1164.0, 2666.0], [1164.0, 2713.0], [220.0, 2713.0]]}], "img_idx": 0, "score": 0.9769242405891418}
{"type": "figure", "bbox": [223, 271, 2145, 1024], "res": [{"text": "ImageNet ", "confidence": 0.9566136598587036, "text_region": [[1789.0, 261.0], [1899.0, 261.0], [1899.0, 294.0], [1789.0, 294.0]]}, {"text": "Zero-Shot", "confidence": 0.999531626701355, "text_region": [[1893.0, 257.0], [2002.0, 257.0], [2002.0, 294.0], [1893.0, 294.0]]}, {"text": "100", "confidence": 0.9969634413719177, "text_region": [[276.0, 287.0], [309.0, 287.0], [309.0, 314.0], [276.0, 314.0]]}, {"text": "\u25b3Score", "confidence": 0.9191899299621582, "text_region": [[2034.0, 283.0], [2106.0, 291.0], [2103.0, 318.0], [2031.0, 309.0]]}, {"text": "90", "confidence": 0.9989060759544373, "text_region": [[286.0, 366.0], [319.0, 366.0], [319.0, 403.0], [286.0, 403.0]]}, {"text": "76.2", "confidence": 0.9996291399002075, "text_region": [[1819.0, 363.0], [1866.0, 363.0], [1866.0, 386.0], [1819.0, 386.0]]}, {"text": "76.2", "confidence": 0.9998874068260193, "text_region": [[1929.0, 363.0], [1979.0, 363.0], [1979.0, 389.0], [1929.0, 389.0]]}, {"text": "0%", "confidence": 0.8650870323181152, "text_region": [[2049.0, 360.0], [2089.0, 360.0], [2089.0, 389.0], [2049.0, 389.0]]}, {"text": "85", "confidence": 0.9999052882194519, "text_region": [[286.0, 403.0], [316.0, 403.0], [316.0, 439.0], [286.0, 439.0]]}, {"text": "80", "confidence": 0.9984050989151001, "text_region": [[289.0, 442.0], [316.0, 442.0], [316.0, 475.0], [289.0, 475.0]]}, {"text": "ImageNetV2", "confidence": 0.9445549249649048, "text_region": [[921.0, 478.0], [1078.0, 478.0], [1078.0, 511.0], [921.0, 511.0]]}, {"text": "64.3", "confidence": 0.9981442093849182, "text_region": [[1819.0, 479.0], [1863.0, 479.0], [1863.0, 502.0], [1819.0, 502.0]]}, {"text": "70.1", "confidence": 0.9997833371162415, "text_region": [[1929.0, 479.0], [1976.0, 479.0], [1976.0, 502.0], [1929.0, 502.0]]}, {"text": "+5.8%", "confidence": 0.9993982315063477, "text_region": [[2032.0, 479.0], [2102.0, 479.0], [2102.0, 505.0], [2032.0, 505.0]]}, {"text": "5", "confidence": 0.9989749193191528, "text_region": [[296.0, 578.0], [313.0, 578.0], [313.0, 594.0], [296.0, 594.0]]}, {"text": "37.7", "confidence": 0.9999125003814697, "text_region": [[1813.0, 584.0], [1869.0, 584.0], [1869.0, 620.0], [1813.0, 620.0]]}, {"text": "ImageNet-R", "confidence": 0.9884966015815735, "text_region": [[924.0, 595.0], [1083.0, 587.0], [1085.0, 620.0], [926.0, 628.0]]}, {"text": "88.9", "confidence": 0.9996665716171265, "text_region": [[1929.0, 591.0], [1979.0, 591.0], [1979.0, 617.0], [1929.0, 617.0]]}, {"text": "+51.2%", "confidence": 0.989732027053833, "text_region": [[2032.0, 587.0], [2112.0, 587.0], [2112.0, 624.0], [2032.0, 624.0]]}, {"text": "60", "confidence": 0.6912484765052795, "text_region": [[286.0, 620.0], [301.0, 606.0], [318.0, 622.0], [303.0, 637.0]]}, {"text": "ObjectNet", "confidence": 0.999496340751648, "text_region": [[955.0, 703.0], [1084.0, 703.0], [1084.0, 739.0], [955.0, 739.0]]}, {"text": "32.6", "confidence": 0.9970036745071411, "text_region": [[1819.0, 706.0], [1866.0, 706.0], [1866.0, 733.0], [1819.0, 733.0]]}, {"text": "72.3", "confidence": 0.9997096061706543, "text_region": [[1929.0, 706.0], [1979.0, 706.0], [1979.0, 733.0], [1929.0, 733.0]]}, {"text": "+39.7%", "confidence": 0.9999279379844666, "text_region": [[2022.0, 703.0], [2112.0, 703.0], [2112.0, 736.0], [2022.0, 736.0]]}, {"text": "ImageNet", "confidence": 0.9700517654418945, "text_region": [[958.0, 802.0], [1091.0, 802.0], [1091.0, 838.0], [958.0, 838.0]]}, {"text": "85", "confidence": 0.7595493793487549, "text_region": [[293.0, 815.0], [309.0, 815.0], [309.0, 832.0], [293.0, 832.0]]}, {"text": "25.2", "confidence": 0.999896228313446, "text_region": [[1816.0, 818.0], [1866.0, 818.0], [1866.0, 845.0], [1816.0, 845.0]]}, {"text": "60.2", "confidence": 0.9999234676361084, "text_region": [[1929.0, 818.0], [1982.0, 818.0], [1982.0, 845.0], [1929.0, 845.0]]}, {"text": "+35.0%", "confidence": 0.9997208714485168, "text_region": [[2029.0, 822.0], [2112.0, 822.0], [2112.0, 851.0], [2029.0, 851.0]]}, {"text": "Sketch", "confidence": 0.9994418621063232, "text_region": [[994.0, 838.0], [1091.0, 838.0], [1091.0, 871.0], [994.0, 871.0]]}, {"text": "2.7", "confidence": 0.9987966418266296, "text_region": [[1822.0, 936.0], [1852.0, 928.0], [1859.0, 955.0], [1828.0, 962.0]]}, {"text": "77.1", "confidence": 0.9999003410339355, "text_region": [[1926.0, 927.0], [1982.0, 927.0], [1982.0, 964.0], [1926.0, 964.0]]}, {"text": "ImageNet-A", "confidence": 0.9884672164916992, "text_region": [[908.0, 940.0], [1078.0, 940.0], [1078.0, 973.0], [908.0, 973.0]]}, {"text": "+74.4%", "confidence": 0.9517636895179749, "text_region": [[2032.0, 934.0], [2109.0, 934.0], [2109.0, 960.0], [2032.0, 960.0]]}, {"text": "70", "confidence": 0.9996841549873352, "text_region": [[396.0, 960.0], [426.0, 960.0], [426.0, 980.0], [396.0, 980.0]]}, {"text": "75", "confidence": 0.999888002872467, "text_region": [[472.0, 957.0], [509.0, 957.0], [509.0, 980.0], [472.0, 980.0]]}, {"text": "80", "confidence": 0.9972264766693115, "text_region": [[552.0, 954.0], [585.0, 954.0], [585.0, 980.0], [552.0, 980.0]]}, {"text": "85", "confidence": 0.9999456405639648, "text_region": [[639.0, 954.0], [669.0, 954.0], [669.0, 980.0], [639.0, 980.0]]}, {"text": "90", "confidence": 0.9997061491012573, "text_region": [[718.0, 960.0], [748.0, 960.0], [748.0, 980.0], [718.0, 980.0]]}, {"text": "Average onclasssubsampled ImageNet(top-1,%)", "confidence": 0.944851279258728, "text_region": [[366.0, 987.0], [858.0, 987.0], [858.0, 1010.0], [366.0, 1010.0]]}], "img_idx": 0, "score": 0.9534950852394104}
{"type": "header", "bbox": [2129, 194, 2164, 219], "res": [{"text": "15", "confidence": 0.9994919896125793, "text_region": [[2132.0, 191.0], [2165.0, 191.0], [2165.0, 221.0], [2132.0, 221.0]]}], "img_idx": 0, "score": 0.9061659574508667}
{"type": "header", "bbox": [228, 193, 1124, 224], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9925623536109924, "text_region": [[220.0, 188.0], [1367.0, 188.0], [1367.0, 231.0], [220.0, 231.0]]}], "img_idx": 0, "score": 0.7201156616210938}
