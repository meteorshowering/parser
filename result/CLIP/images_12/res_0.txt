{"type": "text", "bbox": [218, 1757, 1161, 2601], "res": [{"text": " In 2015, it was announced that a deep learning model ex-", "confidence": 0.9907553195953369, "text_region": [[213.0, 1742.0], [1161.0, 1746.0], [1161.0, 1802.0], [213.0, 1798.0]]}, {"text": " ceeded human performance on the ImageNet test set (He", "confidence": 0.9839905500411987, "text_region": [[213.0, 1799.0], [1164.0, 1795.0], [1164.0, 1841.0], [213.0, 1845.0]]}, {"text": "et al., 2015). However, research in the subsequent years", "confidence": 0.9802645444869995, "text_region": [[220.0, 1845.0], [1161.0, 1845.0], [1161.0, 1891.0], [220.0, 1891.0]]}, {"text": "has repeatedly found that these models still make many sim-", "confidence": 0.9809548258781433, "text_region": [[216.0, 1891.0], [1171.0, 1891.0], [1171.0, 1947.0], [216.0, 1947.0]]}, {"text": "ple mistakes (Dodge & Karam, 2017; Geirhos et al., 2018;", "confidence": 0.995137095451355, "text_region": [[220.0, 1944.0], [1167.0, 1944.0], [1167.0, 1990.0], [220.0, 1990.0]]}, {"text": "Alcorn et al., 2019), and new benchmarks testing these sys-", "confidence": 0.9832467436790466, "text_region": [[220.0, 1990.0], [1167.0, 1990.0], [1167.0, 2036.0], [220.0, 2036.0]]}, {"text": "tems has often found their performance to be much lower", "confidence": 0.9993650317192078, "text_region": [[220.0, 2039.0], [1164.0, 2039.0], [1164.0, 2086.0], [220.0, 2086.0]]}, {"text": "than both their ImageNet accuracy and human accuracy", "confidence": 0.98781418800354, "text_region": [[216.0, 2082.0], [1161.0, 2089.0], [1161.0, 2135.0], [216.0, 2128.0]]}, {"text": "(Recht et al., 2019; Barbu et al., 2019). What explains this", "confidence": 0.9929749965667725, "text_region": [[220.0, 2135.0], [1161.0, 2135.0], [1161.0, 2178.0], [220.0, 2178.0]]}, {"text": " discrepancy? Various ideas have been suggested and stud-", "confidence": 0.9880839586257935, "text_region": [[213.0, 2178.0], [1171.0, 2178.0], [1171.0, 2234.0], [213.0, 2234.0]]}, {"text": "ied (Ilyas et al., 2019; Geirhos et al., 2020). A common", "confidence": 0.9777921438217163, "text_region": [[220.0, 2228.0], [1161.0, 2228.0], [1161.0, 2274.0], [220.0, 2274.0]]}, {"text": "theme of proposed explanations is that deep learning models", "confidence": 0.9980733394622803, "text_region": [[213.0, 2274.0], [1164.0, 2274.0], [1164.0, 2330.0], [213.0, 2330.0]]}, {"text": "are exceedingly adept at finding correlations and patterns", "confidence": 0.9903603792190552, "text_region": [[216.0, 2326.0], [1161.0, 2326.0], [1161.0, 2373.0], [216.0, 2373.0]]}, {"text": "which hold across their training dataset and thus improve", "confidence": 0.9969650506973267, "text_region": [[213.0, 2366.0], [1164.0, 2369.0], [1164.0, 2426.0], [213.0, 2422.0]]}, {"text": "in-distribution performance. However many of these corre.", "confidence": 0.993328869342804, "text_region": [[216.0, 2422.0], [1157.0, 2422.0], [1157.0, 2468.0], [216.0, 2468.0]]}, {"text": "lations and patterns are actually spurious and do not hold for", "confidence": 0.9940201044082642, "text_region": [[220.0, 2468.0], [1161.0, 2468.0], [1161.0, 2515.0], [220.0, 2515.0]]}, {"text": " Other distributions and result in large drops in performance", "confidence": 0.9795200228691101, "text_region": [[216.0, 2518.0], [1161.0, 2518.0], [1161.0, 2564.0], [216.0, 2564.0]]}, {"text": " on other datasets.", "confidence": 0.9867405295372009, "text_region": [[213.0, 2567.0], [502.0, 2567.0], [502.0, 2610.0], [213.0, 2610.0]]}], "img_idx": 0, "score": 0.9946047067642212}
{"type": "text", "bbox": [1227, 1473, 2170, 1893], "res": [{"text": "Across these collected datasets, the accuracy of ImageNet", "confidence": 0.9994317293167114, "text_region": [[1224.0, 1472.0], [2172.0, 1472.0], [2172.0, 1518.0], [1224.0, 1518.0]]}, {"text": "models drop well below the expectation set by the Ima-", "confidence": 0.990366518497467, "text_region": [[1227.0, 1518.0], [2172.0, 1518.0], [2172.0, 1564.0], [1227.0, 1564.0]]}, {"text": "geNet validation set. For the following summary discussion", "confidence": 0.9910617470741272, "text_region": [[1227.0, 1568.0], [2169.0, 1568.0], [2169.0, 1614.0], [1227.0, 1614.0]]}, {"text": "we report average accuracy across all 7 natural distribution", "confidence": 0.9975380301475525, "text_region": [[1227.0, 1617.0], [2169.0, 1617.0], [2169.0, 1663.0], [1227.0, 1663.0]]}, {"text": "shift datasets and average accuracy across the correspond-", "confidence": 0.9929662346839905, "text_region": [[1227.0, 1663.0], [2172.0, 1663.0], [2172.0, 1709.0], [1227.0, 1709.0]]}, {"text": "ing class subsets of ImageNet unless otherwise specified.", "confidence": 0.9955353736877441, "text_region": [[1227.0, 1709.0], [2172.0, 1709.0], [2172.0, 1756.0], [1227.0, 1756.0]]}, {"text": "Additionally, for Youtube-BB and ImageNet-Vid, which", "confidence": 0.996299684047699, "text_region": [[1227.0, 1759.0], [2172.0, 1759.0], [2172.0, 1805.0], [1227.0, 1805.0]]}, {"text": " have two different evaluation settings, we use the average", "confidence": 0.9835104942321777, "text_region": [[1218.0, 1798.0], [2175.0, 1805.0], [2175.0, 1858.0], [1217.0, 1851.0]]}, {"text": " of pm-0 and pm-10 accuracy.", "confidence": 0.9939091801643372, "text_region": [[1218.0, 1844.0], [1700.0, 1852.0], [1699.0, 1908.0], [1217.0, 1901.0]]}], "img_idx": 0, "score": 0.991155743598938}
{"type": "text", "bbox": [1226, 277, 2169, 1438], "res": [{"text": "combination of the two? CLIP models, which are trained via", "confidence": 0.99749755859375, "text_region": [[1227.0, 274.0], [2169.0, 274.0], [2169.0, 317.0], [1227.0, 317.0]]}, {"text": "natural language supervision on a very large dataset and are", "confidence": 0.9971676468849182, "text_region": [[1227.0, 323.0], [2169.0, 323.0], [2169.0, 370.0], [1227.0, 370.0]]}, {"text": "capable of high zero-shot performance, are an opportunity", "confidence": 0.999358594417572, "text_region": [[1227.0, 373.0], [2172.0, 373.0], [2172.0, 419.0], [1227.0, 419.0]]}, {"text": "to investigate this question from a different angle.", "confidence": 0.9964795708656311, "text_region": [[1224.0, 422.0], [2022.0, 422.0], [2022.0, 465.0], [1224.0, 465.0]]}, {"text": "Taori et al. (2020) is a recent comprehensive study mov-", "confidence": 0.9834639430046082, "text_region": [[1224.0, 485.0], [2175.0, 492.0], [2175.0, 538.0], [1224.0, 531.0]]}, {"text": "ing towards quantifying and understanding these behaviors", "confidence": 0.9969289898872375, "text_region": [[1224.0, 538.0], [2169.0, 538.0], [2169.0, 584.0], [1224.0, 584.0]]}, {"text": "for ImageNet models. Taori et al. (2020) study how the", "confidence": 0.9843278527259827, "text_region": [[1217.0, 581.0], [2172.0, 577.0], [2172.0, 634.0], [1217.0, 637.0]]}, {"text": " performance of ImageNet models change when evaluated", "confidence": 0.996974527835846, "text_region": [[1217.0, 630.0], [2175.0, 627.0], [2175.0, 683.0], [1217.0, 686.0]]}, {"text": "on natural distribution shifts. They measure performance", "confidence": 0.9908829927444458, "text_region": [[1224.0, 680.0], [2172.0, 683.0], [2172.0, 729.0], [1224.0, 726.0]]}, {"text": "on a set of 7 distribution shifts: ImageNetV2 (Recht et al.", "confidence": 0.9843263626098633, "text_region": [[1227.0, 729.0], [2169.0, 729.0], [2169.0, 776.0], [1227.0, 776.0]]}, {"text": "2019), ImageNet Sketch (Wang et al., 2019), Youtube-BB", "confidence": 0.9927788376808167, "text_region": [[1227.0, 776.0], [2172.0, 776.0], [2172.0, 822.0], [1227.0, 822.0]]}, {"text": " and ImageNet-Vid (Shankar et al., 2019), ObjectNet (Barbu", "confidence": 0.9828503727912903, "text_region": [[1221.0, 825.0], [2169.0, 825.0], [2169.0, 871.0], [1221.0, 871.0]]}, {"text": "et al., 2019), ImageNet Adversarial (Hendrycks et al., 2019),", "confidence": 0.9879407286643982, "text_region": [[1224.0, 871.0], [2175.0, 871.0], [2175.0, 917.0], [1224.0, 917.0]]}, {"text": "and ImageNet Rendition (Hendrycks et al., 2020a). They", "confidence": 0.9990330934524536, "text_region": [[1224.0, 917.0], [2165.0, 917.0], [2165.0, 964.0], [1224.0, 964.0]]}, {"text": " distinguish these datasets, which all consist of novel images", "confidence": 0.9793657660484314, "text_region": [[1221.0, 960.0], [2172.0, 964.0], [2172.0, 1020.0], [1221.0, 1016.0]]}, {"text": "collected from a variety of sources, from synthetic distri-", "confidence": 0.9772303104400635, "text_region": [[1227.0, 1016.0], [2172.0, 1016.0], [2172.0, 1063.0], [1227.0, 1063.0]]}, {"text": " bution shifts such as ImageNet-C (Hendrycks & Dietterich,", "confidence": 0.961989164352417, "text_region": [[1221.0, 1059.0], [2175.0, 1059.0], [2175.0, 1115.0], [1221.0, 1115.0]]}, {"text": "2019), Stylized ImageNet (Geirhos et al., 2018), or adver-", "confidence": 0.9977459907531738, "text_region": [[1227.0, 1112.0], [2175.0, 1112.0], [2175.0, 1158.0], [1227.0, 1158.0]]}, {"text": "sarial attacks (Goodfellow et al., 2014) which are created by", "confidence": 0.9995346069335938, "text_region": [[1224.0, 1158.0], [2172.0, 1158.0], [2172.0, 1204.0], [1224.0, 1204.0]]}, {"text": "perturbing existing images in various ways. They propose", "confidence": 0.9805012941360474, "text_region": [[1224.0, 1204.0], [2172.0, 1204.0], [2172.0, 1261.0], [1224.0, 1261.0]]}, {"text": "this distinction because in part because they find that while", "confidence": 0.9893588423728943, "text_region": [[1221.0, 1247.0], [2172.0, 1251.0], [2172.0, 1307.0], [1221.0, 1303.0]]}, {"text": " several techniques have been demonstrated to improve per-", "confidence": 0.9718416333198547, "text_region": [[1221.0, 1297.0], [2175.0, 1300.0], [2175.0, 1356.0], [1221.0, 1353.0]]}, {"text": "formance on synthetic distribution shifts, they often fail to", "confidence": 0.9861315488815308, "text_region": [[1224.0, 1353.0], [2175.0, 1353.0], [2175.0, 1399.0], [1224.0, 1399.0]]}, {"text": " yield consistent improvements on natural distributions.3", "confidence": 0.988874614238739, "text_region": [[1217.0, 1399.0], [2122.0, 1389.0], [2122.0, 1445.0], [1218.0, 1455.0]]}], "img_idx": 0, "score": 0.9900780916213989}
{"type": "text", "bbox": [1226, 1934, 2168, 2645], "res": [{"text": "A ResNet-101 makes 5 times as many mistakes when eval-", "confidence": 0.9872145652770996, "text_region": [[1224.0, 1921.0], [2172.0, 1921.0], [2172.0, 1967.0], [1224.0, 1967.0]]}, {"text": "uated on these natural distribution shifts compared to the", "confidence": 0.9990978837013245, "text_region": [[1224.0, 1973.0], [2172.0, 1973.0], [2172.0, 2020.0], [1224.0, 2020.0]]}, {"text": " ImageNet validation set. Encouragingly however, Taori et al.", "confidence": 0.9858954548835754, "text_region": [[1217.0, 2013.0], [2175.0, 2016.0], [2175.0, 2072.0], [1217.0, 2069.0]]}, {"text": "(2020) find that accuracy under distribution shift increases", "confidence": 0.9909809231758118, "text_region": [[1227.0, 2069.0], [2172.0, 2069.0], [2172.0, 2115.0], [1227.0, 2115.0]]}, {"text": " predictably with ImageNet accuracy and is well modeled", "confidence": 0.9874365329742432, "text_region": [[1217.0, 2112.0], [2175.0, 2109.0], [2175.0, 2165.0], [1217.0, 2168.0]]}, {"text": "as a linear function of logit-transformed accuracy. Taori", "confidence": 0.9823587536811829, "text_region": [[1224.0, 2165.0], [2172.0, 2165.0], [2172.0, 2211.0], [1224.0, 2211.0]]}, {"text": "et al. (2020) use this finding to propose that robustness", "confidence": 0.9949727058410645, "text_region": [[1221.0, 2204.0], [2172.0, 2208.0], [2172.0, 2264.0], [1221.0, 2260.0]]}, {"text": "analysis should distinguish between effective and relative", "confidence": 0.9974135756492615, "text_region": [[1227.0, 2260.0], [2169.0, 2260.0], [2169.0, 2307.0], [1227.0, 2307.0]]}, {"text": "robustness. Effective robustness measures improvements", "confidence": 0.9899356961250305, "text_region": [[1227.0, 2307.0], [2169.0, 2307.0], [2169.0, 2353.0], [1227.0, 2353.0]]}, {"text": " in accuracy under distribution shift above what is predicted", "confidence": 0.9832217693328857, "text_region": [[1221.0, 2353.0], [2165.0, 2353.0], [2165.0, 2399.0], [1221.0, 2399.0]]}, {"text": "by the documented relationship between in-distribution and", "confidence": 0.9961802363395691, "text_region": [[1224.0, 2402.0], [2175.0, 2402.0], [2175.0, 2449.0], [1224.0, 2449.0]]}, {"text": "out-of-distribution accuracy. Relative robustness captures", "confidence": 0.9855694770812988, "text_region": [[1227.0, 2452.0], [2172.0, 2452.0], [2172.0, 2498.0], [1227.0, 2498.0]]}, {"text": " any improvement in out-of-distribution accuracy. Taori et al.", "confidence": 0.9884236454963684, "text_region": [[1221.0, 2495.0], [2178.0, 2491.0], [2179.0, 2548.0], [1221.0, 2551.0]]}, {"text": "(2020) argue that robustness techniques should aim to im-", "confidence": 0.985506534576416, "text_region": [[1227.0, 2548.0], [2175.0, 2548.0], [2175.0, 2594.0], [1227.0, 2594.0]]}, {"text": " prove both effective robustness and relative robustness.", "confidence": 0.9804955720901489, "text_region": [[1221.0, 2591.0], [2115.0, 2587.0], [2115.0, 2643.0], [1221.0, 2647.0]]}], "img_idx": 0, "score": 0.9895600080490112}
{"type": "text", "bbox": [220, 2640, 1158, 2867], "res": [{"text": "We caution that, to date, most of these studies limit their", "confidence": 0.9983018636703491, "text_region": [[220.0, 2637.0], [1164.0, 2637.0], [1164.0, 2683.0], [220.0, 2683.0]]}, {"text": "evaluation to models trained on ImageNet. Recalling the", "confidence": 0.9982854127883911, "text_region": [[220.0, 2683.0], [1161.0, 2683.0], [1161.0, 2729.0], [220.0, 2729.0]]}, {"text": "topic of discussion, it may be a mistake to generalize too", "confidence": 0.9857562780380249, "text_region": [[220.0, 2732.0], [1164.0, 2732.0], [1164.0, 2779.0], [220.0, 2779.0]]}, {"text": "far from these initial findings. To what degree are these", "confidence": 0.9844702482223511, "text_region": [[216.0, 2775.0], [1167.0, 2775.0], [1167.0, 2831.0], [216.0, 2831.0]]}, {"text": "failures attributable to deep learning, ImageNet, or some", "confidence": 0.9884149432182312, "text_region": [[216.0, 2828.0], [1161.0, 2828.0], [1161.0, 2874.0], [216.0, 2874.0]]}], "img_idx": 0, "score": 0.9855056405067444}
{"type": "text", "bbox": [219, 1364, 1155, 1527], "res": [{"text": "Figure 11. CLIP's features outperform the features of the best", "confidence": 0.9916606545448303, "text_region": [[220.0, 1360.0], [1164.0, 1360.0], [1164.0, 1402.0], [220.0, 1402.0]]}, {"text": "ImageNet model on a wide variety of datasets. Fitting a linear", "confidence": 0.9815090298652649, "text_region": [[216.0, 1402.0], [1161.0, 1402.0], [1161.0, 1449.0], [216.0, 1449.0]]}, {"text": "classifier on CLIP's features outperforms using the Noisy Student", "confidence": 0.9872983694076538, "text_region": [[216.0, 1449.0], [1164.0, 1449.0], [1164.0, 1492.0], [216.0, 1492.0]]}, {"text": "EfficientNet-L2 on 21 out of 27 datasets.", "confidence": 0.9879047274589539, "text_region": [[220.0, 1492.0], [812.0, 1492.0], [812.0, 1534.0], [220.0, 1534.0]]}], "img_idx": 0, "score": 0.9649962186813354}
{"type": "text", "bbox": [1230, 2675, 2166, 2815], "res": [{"text": "Almost all models studied in Taori et al. (2020) are trained", "confidence": 0.9746826887130737, "text_region": [[1224.0, 2666.0], [2172.0, 2666.0], [2172.0, 2709.0], [1224.0, 2709.0]]}, {"text": "3We refer readers to Hendrycks et al. (2020a) for additional", "confidence": 0.9974095821380615, "text_region": [[1274.0, 2736.0], [2172.0, 2739.0], [2172.0, 2785.0], [1274.0, 2782.0]]}, {"text": " experiments and discussion on this claim.", "confidence": 0.9843463897705078, "text_region": [[1221.0, 2782.0], [1829.0, 2779.0], [1829.0, 2828.0], [1221.0, 2831.0]]}], "img_idx": 0, "score": 0.5888454914093018}
{"type": "title", "bbox": [219, 1682, 977, 1712], "res": [{"text": " 3.3. Robustness to Natural Distribution Shift", "confidence": 0.9922571778297424, "text_region": [[213.0, 1676.0], [981.0, 1676.0], [981.0, 1719.0], [213.0, 1719.0]]}], "img_idx": 0, "score": 0.9397268295288086}
{"type": "title", "bbox": [221, 1581, 607, 1619], "res": [{"text": "low for both approaches.", "confidence": 0.9929882884025574, "text_region": [[216.0, 1577.0], [619.0, 1577.0], [619.0, 1624.0], [216.0, 1624.0]]}], "img_idx": 0, "score": 0.634293794631958}
{"type": "figure", "bbox": [221, 257, 1151, 1240], "res": [{"text": "SST2", "confidence": 0.999890148639679, "text_region": [[402.0, 271.0], [492.0, 271.0], [492.0, 307.0], [402.0, 307.0]]}, {"text": "+23.6", "confidence": 0.9971305727958679, "text_region": [[1044.0, 271.0], [1154.0, 271.0], [1154.0, 307.0], [1044.0, 307.0]]}, {"text": "Country211", "confidence": 0.9981403350830078, "text_region": [[284.0, 296.0], [493.0, 304.0], [491.0, 350.0], [282.0, 343.0]]}, {"text": "+22.7", "confidence": 0.9996227025985718, "text_region": [[1031.0, 307.0], [1138.0, 307.0], [1138.0, 340.0], [1031.0, 340.0]]}, {"text": "HatefulMemes", "confidence": 0.9927874207496643, "text_region": [[240.0, 330.0], [493.0, 334.0], [492.0, 380.0], [239.0, 376.0]]}, {"text": "+18.8", "confidence": 0.9952878952026367, "text_region": [[935.0, 340.0], [1048.0, 340.0], [1048.0, 376.0], [935.0, 376.0]]}, {"text": "StanfordCars", "confidence": 0.9941511750221252, "text_region": [[266.0, 373.0], [489.0, 373.0], [489.0, 409.0], [266.0, 409.0]]}, {"text": "+15.9", "confidence": 0.9984469413757324, "text_region": [[868.0, 373.0], [981.0, 373.0], [981.0, 406.0], [868.0, 406.0]]}, {"text": "GTSRB", "confidence": 0.999747633934021, "text_region": [[359.0, 403.0], [492.0, 403.0], [492.0, 439.0], [359.0, 439.0]]}, {"text": "+14.7", "confidence": 0.9993915557861328, "text_region": [[845.0, 403.0], [951.0, 403.0], [951.0, 439.0], [845.0, 439.0]]}, {"text": "SUN397", "confidence": 0.99936443567276, "text_region": [[349.0, 436.0], [489.0, 436.0], [489.0, 472.0], [349.0, 472.0]]}, {"text": "+6.5", "confidence": 0.9988342523574829, "text_region": [[659.0, 442.0], [725.0, 442.0], [725.0, 469.0], [659.0, 469.0]]}, {"text": "Kinetics700", "confidence": 0.9996547698974609, "text_region": [[289.0, 472.0], [489.0, 472.0], [489.0, 508.0], [289.0, 508.0]]}, {"text": "+6.2", "confidence": 0.9997085332870483, "text_region": [[649.0, 469.0], [725.0, 469.0], [725.0, 505.0], [649.0, 505.0]]}, {"text": "RESISC45", "confidence": 0.999083399772644, "text_region": [[323.0, 505.0], [492.0, 505.0], [492.0, 538.0], [323.0, 538.0]]}, {"text": "+5.1", "confidence": 0.8251845240592957, "text_region": [[629.0, 508.0], [692.0, 508.0], [692.0, 531.0], [629.0, 531.0]]}, {"text": "FER2013", "confidence": 0.9984362721443176, "text_region": [[328.0, 535.0], [491.0, 527.0], [493.0, 570.0], [331.0, 578.0]]}, {"text": "Food101", "confidence": 0.9467608332633972, "text_region": [[343.0, 571.0], [482.0, 571.0], [482.0, 607.0], [343.0, 607.0]]}, {"text": "3.9", "confidence": 0.9994770884513855, "text_region": [[609.0, 574.0], [669.0, 574.0], [669.0, 601.0], [609.0, 601.0]]}, {"text": "FGVCAircraft", "confidence": 0.9986469149589539, "text_region": [[264.0, 593.0], [493.0, 601.0], [491.0, 647.0], [262.0, 640.0]]}, {"text": "3.2", "confidence": 0.9997161030769348, "text_region": [[595.0, 607.0], [652.0, 607.0], [652.0, 630.0], [595.0, 630.0]]}, {"text": "UCF101", "confidence": 0.9994449019432068, "text_region": [[349.0, 637.0], [486.0, 637.0], [486.0, 673.0], [349.0, 673.0]]}, {"text": "3.1", "confidence": 0.9996021389961243, "text_region": [[589.0, 640.0], [645.0, 640.0], [645.0, 663.0], [589.0, 663.0]]}, {"text": "KITTI Distance", "confidence": 0.9736713767051697, "text_region": [[249.0, 670.0], [486.0, 670.0], [486.0, 706.0], [249.0, 706.0]]}, {"text": "Birdsnap", "confidence": 0.999893844127655, "text_region": [[336.0, 703.0], [486.0, 703.0], [486.0, 739.0], [336.0, 739.0]]}, {"text": "1.4", "confidence": 0.9996412396430969, "text_region": [[552.0, 706.0], [609.0, 706.0], [609.0, 729.0], [552.0, 729.0]]}, {"text": "Flowers102", "confidence": 0.9956561326980591, "text_region": [[293.0, 736.0], [482.0, 736.0], [482.0, 772.0], [293.0, 772.0]]}, {"text": "+1.4", "confidence": 0.991266131401062, "text_region": [[542.0, 739.0], [609.0, 739.0], [609.0, 766.0], [542.0, 766.0]]}, {"text": "Caltech101", "confidence": 0.9958711862564087, "text_region": [[299.0, 769.0], [482.0, 769.0], [482.0, 805.0], [299.0, 805.0]]}, {"text": "+1.3", "confidence": 0.9974741339683533, "text_region": [[539.0, 772.0], [605.0, 772.0], [605.0, 799.0], [539.0, 799.0]]}, {"text": "EuroSAT", "confidence": 0.9991596937179565, "text_region": [[343.0, 802.0], [489.0, 802.0], [489.0, 838.0], [343.0, 838.0]]}, {"text": "+0.9", "confidence": 0.8823020458221436, "text_region": [[532.0, 805.0], [599.0, 805.0], [599.0, 832.0], [532.0, 832.0]]}, {"text": "MNIST", "confidence": 0.9952026605606079, "text_region": [[376.0, 835.0], [489.0, 835.0], [489.0, 871.0], [376.0, 871.0]]}, {"text": "0.6", "confidence": 0.9994757175445557, "text_region": [[526.0, 838.0], [592.0, 838.0], [592.0, 865.0], [526.0, 865.0]]}, {"text": "DTD", "confidence": 0.9993433952331543, "text_region": [[409.0, 868.0], [489.0, 868.0], [489.0, 904.0], [409.0, 904.0]]}, {"text": "+0.5", "confidence": 0.9993972778320312, "text_region": [[519.0, 871.0], [589.0, 871.0], [589.0, 898.0], [519.0, 898.0]]}, {"text": "VOC2007", "confidence": 0.9964613318443298, "text_region": [[326.0, 901.0], [492.0, 901.0], [492.0, 937.0], [326.0, 937.0]]}, {"text": "+0.5", "confidence": 0.9981158971786499, "text_region": [[512.0, 904.0], [585.0, 904.0], [585.0, 931.0], [512.0, 931.0]]}, {"text": "STL10", "confidence": 0.995550274848938, "text_region": [[382.0, 934.0], [496.0, 934.0], [496.0, 970.0], [382.0, 970.0]]}, {"text": "+0.0", "confidence": 0.9907844066619873, "text_region": [[509.0, 934.0], [582.0, 934.0], [582.0, 970.0], [509.0, 970.0]]}, {"text": "OxfordPets", "confidence": 0.9991470575332642, "text_region": [[502.0, 967.0], [692.0, 967.0], [692.0, 1003.0], [502.0, 1003.0]]}, {"text": "0.5", "confidence": 0.9987950325012207, "text_region": [[419.0, 970.0], [489.0, 970.0], [489.0, 997.0], [419.0, 997.0]]}, {"text": "0.8", "confidence": 0.9988304972648621, "text_region": [[412.0, 1003.0], [489.0, 1003.0], [489.0, 1030.0], [412.0, 1030.0]]}, {"text": "CIFAR10", "confidence": 0.9976434707641602, "text_region": [[506.0, 1000.0], [655.0, 1000.0], [655.0, 1036.0], [506.0, 1036.0]]}, {"text": "PatchCamelyon", "confidence": 0.9954081177711487, "text_region": [[499.0, 1026.0], [772.0, 1030.0], [771.0, 1076.0], [499.0, 1072.0]]}, {"text": "1.2", "confidence": 0.9980891346931458, "text_region": [[398.0, 1040.0], [481.0, 1032.0], [483.0, 1059.0], [401.0, 1067.0]]}, {"text": "CIFAR100", "confidence": 0.9984592199325562, "text_region": [[509.0, 1066.0], [672.0, 1066.0], [672.0, 1102.0], [509.0, 1102.0]]}, {"text": "2.4", "confidence": 0.9999647736549377, "text_region": [[379.0, 1102.0], [439.0, 1102.0], [439.0, 1129.0], [379.0, 1129.0]]}, {"text": "CLEVRCounts", "confidence": 0.9991762042045593, "text_region": [[509.0, 1099.0], [732.0, 1099.0], [732.0, 1135.0], [509.0, 1135.0]]}, {"text": "3.0", "confidence": 0.9322134852409363, "text_region": [[376.0, 1135.0], [429.0, 1135.0], [429.0, 1158.0], [376.0, 1158.0]]}, {"text": "ImageNet", "confidence": 0.9963680505752563, "text_region": [[496.0, 1132.0], [675.0, 1132.0], [675.0, 1178.0], [496.0, 1178.0]]}, {"text": "-10", "confidence": 0.9849560856819153, "text_region": [[233.0, 1185.0], [299.0, 1185.0], [299.0, 1221.0], [233.0, 1221.0]]}, {"text": "5", "confidence": 0.9995013475418091, "text_region": [[599.0, 1185.0], [629.0, 1185.0], [629.0, 1218.0], [599.0, 1218.0]]}, {"text": "10", "confidence": 0.9983546733856201, "text_region": [[705.0, 1181.0], [762.0, 1181.0], [762.0, 1221.0], [705.0, 1221.0]]}, {"text": "15", "confidence": 0.9997451305389404, "text_region": [[825.0, 1181.0], [875.0, 1181.0], [875.0, 1221.0], [825.0, 1221.0]]}, {"text": "20", "confidence": 0.9997455477714539, "text_region": [[941.0, 1178.0], [994.0, 1178.0], [994.0, 1224.0], [941.0, 1224.0]]}, {"text": "-5", "confidence": 0.9811438918113708, "text_region": [[359.0, 1188.0], [402.0, 1188.0], [402.0, 1218.0], [359.0, 1218.0]]}, {"text": "0", "confidence": 0.949080228805542, "text_region": [[482.0, 1188.0], [509.0, 1188.0], [509.0, 1218.0], [482.0, 1218.0]]}, {"text": "25", "confidence": 0.9998623132705688, "text_region": [[1058.0, 1181.0], [1111.0, 1181.0], [1111.0, 1221.0], [1058.0, 1221.0]]}, {"text": "\u25b3 Score (%)", "confidence": 0.906332790851593, "text_region": [[592.0, 1231.0], [795.0, 1231.0], [795.0, 1267.0], [592.0, 1267.0]]}], "img_idx": 0, "score": 0.9556890726089478}
{"type": "figure_caption", "bbox": [292, 1264, 1102, 1300], "res": [{"text": "\u25b3 Score (%)", "confidence": 0.906332790851593, "text_region": [[592.0, 1231.0], [795.0, 1231.0], [795.0, 1267.0], [592.0, 1267.0]]}, {"text": "Logistic Regression on CLIP vs. EfficientNet L2 NS", "confidence": 0.9965855479240417, "text_region": [[279.0, 1264.0], [1107.0, 1261.0], [1108.0, 1307.0], [280.0, 1310.0]]}], "img_idx": 0, "score": 0.8633983731269836}
{"type": "header", "bbox": [2129, 193, 2163, 218], "res": [{"text": "13", "confidence": 0.9995574355125427, "text_region": [[2129.0, 191.0], [2165.0, 191.0], [2165.0, 221.0], [2129.0, 221.0]]}], "img_idx": 0, "score": 0.9060396552085876}
