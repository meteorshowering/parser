{"type": "text", "bbox": [1226, 592, 2167, 1292], "res": [{"text": "Learning from natural language has several potential", "confidence": 0.9937893748283386, "text_region": [[1221.0, 584.0], [2169.0, 584.0], [2169.0, 630.0], [1221.0, 630.0]]}, {"text": "strengths over other training methods. It's much easier", "confidence": 0.9911090731620789, "text_region": [[1221.0, 630.0], [2172.0, 627.0], [2172.0, 683.0], [1221.0, 686.0]]}, {"text": "to scale natural language supervision compared to standard", "confidence": 0.9853274822235107, "text_region": [[1221.0, 680.0], [2172.0, 676.0], [2172.0, 733.0], [1221.0, 736.0]]}, {"text": "crowd-sourced labeling for image classification since it does", "confidence": 0.9901087284088135, "text_region": [[1224.0, 726.0], [2169.0, 729.0], [2169.0, 776.0], [1224.0, 772.0]]}, {"text": "not require annotations to be in a classic \u201cmachine learning", "confidence": 0.9982659220695496, "text_region": [[1221.0, 769.0], [2175.0, 772.0], [2175.0, 828.0], [1221.0, 825.0]]}, {"text": "compatible format\u2019 such as the canonical 1-of-N majority", "confidence": 0.9921227693557739, "text_region": [[1227.0, 825.0], [2169.0, 825.0], [2169.0, 871.0], [1227.0, 871.0]]}, {"text": "vote \u201cgold label'. Instead, methods which work on natural", "confidence": 0.9875034093856812, "text_region": [[1224.0, 874.0], [2172.0, 874.0], [2172.0, 921.0], [1224.0, 921.0]]}, {"text": "language can learn passively from the supervision contained", "confidence": 0.9935672283172607, "text_region": [[1224.0, 917.0], [2175.0, 917.0], [2175.0, 973.0], [1224.0, 973.0]]}, {"text": "in the vast amount of text on the internet. Learning from", "confidence": 0.9983533024787903, "text_region": [[1221.0, 964.0], [2169.0, 967.0], [2169.0, 1013.0], [1221.0, 1010.0]]}, {"text": "natural language also has an important advantage over most", "confidence": 0.9972778558731079, "text_region": [[1227.0, 1016.0], [2172.0, 1016.0], [2172.0, 1063.0], [1227.0, 1063.0]]}, {"text": "unsupervised or self-supervised learning approaches in that", "confidence": 0.9951806664466858, "text_region": [[1227.0, 1063.0], [2169.0, 1063.0], [2169.0, 1109.0], [1227.0, 1109.0]]}, {"text": "it doesn't \"just' learn a representation but also connects that", "confidence": 0.9798625111579895, "text_region": [[1221.0, 1109.0], [2172.0, 1112.0], [2172.0, 1158.0], [1221.0, 1155.0]]}, {"text": "representation to language which enables fexible zero-shot", "confidence": 0.9938228726387024, "text_region": [[1224.0, 1158.0], [2169.0, 1158.0], [2169.0, 1204.0], [1224.0, 1204.0]]}, {"text": "transfer. In the following subsections, we detail the specific", "confidence": 0.9872394800186157, "text_region": [[1221.0, 1204.0], [2169.0, 1208.0], [2169.0, 1254.0], [1221.0, 1251.0]]}, {"text": "approach we settled on.", "confidence": 0.999535083770752, "text_region": [[1220.0, 1254.0], [1612.0, 1244.0], [1614.0, 1300.0], [1222.0, 1311.0]]}], "img_idx": 0, "score": 0.9942258596420288}
{"type": "text", "bbox": [1226, 2221, 2169, 2853], "res": [{"text": "A major motivation for natural language supervision is the", "confidence": 0.9955443739891052, "text_region": [[1224.0, 2221.0], [2169.0, 2221.0], [2169.0, 2267.0], [1224.0, 2267.0]]}, {"text": "large quantities of data of this form available publicly on the", "confidence": 0.9809431433677673, "text_region": [[1227.0, 2270.0], [2169.0, 2270.0], [2169.0, 2317.0], [1227.0, 2317.0]]}, {"text": "internet. Since existing datasets do not adequately reflect", "confidence": 0.99554443359375, "text_region": [[1224.0, 2313.0], [2165.0, 2313.0], [2165.0, 2360.0], [1224.0, 2360.0]]}, {"text": "this possibility, considering results only on them would un-", "confidence": 0.9925321340560913, "text_region": [[1227.0, 2366.0], [2172.0, 2366.0], [2172.0, 2412.0], [1227.0, 2412.0]]}, {"text": "derestimate the potential of this line of research. To address", "confidence": 0.9975680708885193, "text_region": [[1227.0, 2412.0], [2169.0, 2412.0], [2169.0, 2458.0], [1227.0, 2458.0]]}, {"text": "this, we constructed a new dataset of 400 million (image,", "confidence": 0.99464350938797, "text_region": [[1221.0, 2458.0], [2172.0, 2462.0], [2172.0, 2508.0], [1221.0, 2505.0]]}, {"text": "text) pairs collected form a variety of publicly available", "confidence": 0.9986161589622498, "text_region": [[1227.0, 2508.0], [2172.0, 2508.0], [2172.0, 2554.0], [1227.0, 2554.0]]}, {"text": "sources on the Internet. To attempt to cover as broad a set", "confidence": 0.9870179295539856, "text_region": [[1224.0, 2551.0], [2175.0, 2551.0], [2175.0, 2607.0], [1224.0, 2607.0]]}, {"text": "of visual concepts as possible, we search for (image, text)", "confidence": 0.996932864189148, "text_region": [[1224.0, 2604.0], [2172.0, 2604.0], [2172.0, 2650.0], [1224.0, 2650.0]]}, {"text": "pairs as part of the construction process whose text includes", "confidence": 0.9967800378799438, "text_region": [[1227.0, 2653.0], [2169.0, 2653.0], [2169.0, 2699.0], [1227.0, 2699.0]]}, {"text": "one of a set of 500,000 queries.1 We approximately class", "confidence": 0.9860069155693054, "text_region": [[1224.0, 2693.0], [2172.0, 2693.0], [2172.0, 2749.0], [1224.0, 2749.0]]}, {"text": "IThe base query list is all words occurring at least 100 times in", "confidence": 0.9639081954956055, "text_region": [[1277.0, 2772.0], [2172.0, 2772.0], [2172.0, 2828.0], [1277.0, 2828.0]]}, {"text": "the English version of Wikipedia. This is augmented with bi-grams", "confidence": 0.9994064569473267, "text_region": [[1224.0, 2815.0], [2165.0, 2818.0], [2165.0, 2864.0], [1224.0, 2861.0]]}], "img_idx": 0, "score": 0.9938510656356812}
{"type": "text", "bbox": [219, 1393, 1159, 1903], "res": [{"text": "it can be competitive with prior task-specific supervised", "confidence": 0.9913221001625061, "text_region": [[220.0, 1389.0], [1161.0, 1389.0], [1161.0, 1436.0], [220.0, 1436.0]]}, {"text": "models. We also confirm these findings with linear-probe", "confidence": 0.9945818781852722, "text_region": [[220.0, 1436.0], [1161.0, 1436.0], [1161.0, 1482.0], [220.0, 1482.0]]}, {"text": "representation learning analysis and show that CLIP out-", "confidence": 0.9808191657066345, "text_region": [[213.0, 1482.0], [1171.0, 1478.0], [1171.0, 1534.0], [213.0, 1538.0]]}, {"text": "performs the best publicly available ImageNet model while", "confidence": 0.9989399909973145, "text_region": [[220.0, 1534.0], [1164.0, 1534.0], [1164.0, 1581.0], [220.0, 1581.0]]}, {"text": " also being more computationally efficient. We additionally", "confidence": 0.98305743932724, "text_region": [[213.0, 1574.0], [1164.0, 1577.0], [1164.0, 1634.0], [213.0, 1630.0]]}, {"text": "find that zero-shot CLIP models are much more robust than", "confidence": 0.9977738857269287, "text_region": [[220.0, 1627.0], [1157.0, 1627.0], [1157.0, 1673.0], [220.0, 1673.0]]}, {"text": "equivalent accuracy supervised ImageNet models which", "confidence": 0.9904963970184326, "text_region": [[216.0, 1680.0], [1161.0, 1673.0], [1161.0, 1716.0], [216.0, 1723.0]]}, {"text": "suggests that zero-shot evaluation of task-agnostic models is", "confidence": 0.9952602982521057, "text_region": [[213.0, 1723.0], [1164.0, 1716.0], [1164.0, 1772.0], [213.0, 1779.0]]}, {"text": "much more representative of a model's capability. These re-", "confidence": 0.9938143491744995, "text_region": [[220.0, 1775.0], [1164.0, 1775.0], [1164.0, 1818.0], [220.0, 1818.0]]}, {"text": "sults have significant policy and ethical implications, which", "confidence": 0.9984142184257507, "text_region": [[216.0, 1822.0], [1164.0, 1822.0], [1164.0, 1868.0], [216.0, 1868.0]]}, {"text": "we consider in Section 7.", "confidence": 0.9983853101730347, "text_region": [[213.0, 1868.0], [622.0, 1868.0], [622.0, 1914.0], [213.0, 1914.0]]}], "img_idx": 0, "score": 0.9937679171562195}
{"type": "text", "bbox": [1227, 1436, 2167, 2192], "res": [{"text": "Existing work has mainly used three datasets, MS-COCO", "confidence": 0.97774338722229, "text_region": [[1227.0, 1432.0], [2172.0, 1432.0], [2172.0, 1478.0], [1227.0, 1478.0]]}, {"text": "(Lin et al., 2014), Visual Genome (Krishna et al., 2017), and", "confidence": 0.9889119863510132, "text_region": [[1227.0, 1478.0], [2172.0, 1478.0], [2172.0, 1525.0], [1227.0, 1525.0]]}, {"text": "YFCC100M (Thomee et al., 2016). While MS-COCO and", "confidence": 0.995139479637146, "text_region": [[1227.0, 1525.0], [2172.0, 1525.0], [2172.0, 1571.0], [1227.0, 1571.0]]}, {"text": "Visual Genome are high quality crowd-labeled datasets, they", "confidence": 0.9992175698280334, "text_region": [[1227.0, 1574.0], [2169.0, 1574.0], [2169.0, 1620.0], [1227.0, 1620.0]]}, {"text": "are small by modern standards with approximately 100,000", "confidence": 0.9885000586509705, "text_region": [[1224.0, 1624.0], [2172.0, 1624.0], [2172.0, 1670.0], [1224.0, 1670.0]]}, {"text": "training photos each. By comparison, other computer vision", "confidence": 0.9979197978973389, "text_region": [[1224.0, 1673.0], [2169.0, 1673.0], [2169.0, 1719.0], [1224.0, 1719.0]]}, {"text": "systems are trained on up to 3.5 billion Instagram photos", "confidence": 0.9900468587875366, "text_region": [[1224.0, 1716.0], [2169.0, 1716.0], [2169.0, 1762.0], [1224.0, 1762.0]]}, {"text": "(Mahajan et al., 2018). YFCC100M, at 100 million photos,", "confidence": 0.9902685880661011, "text_region": [[1224.0, 1766.0], [2175.0, 1766.0], [2175.0, 1812.0], [1224.0, 1812.0]]}, {"text": "is a possible alternative, but the metadata for each image is", "confidence": 0.9890868663787842, "text_region": [[1224.0, 1815.0], [2172.0, 1815.0], [2172.0, 1861.0], [1224.0, 1861.0]]}, {"text": " sparse and of varying quality. Many images use automati-", "confidence": 0.9773844480514526, "text_region": [[1221.0, 1855.0], [2179.0, 1858.0], [2178.0, 1914.0], [1221.0, 1911.0]]}, {"text": "cally generated filenames like 20160716-113957.JPG", "confidence": 0.9889785051345825, "text_region": [[1227.0, 1911.0], [2169.0, 1911.0], [2169.0, 1957.0], [1227.0, 1957.0]]}, {"text": "as \u201ctitles\u201d or contain \u201cdescriptions\u201d of camera exposure", "confidence": 0.9728205800056458, "text_region": [[1224.0, 1957.0], [2169.0, 1957.0], [2169.0, 2003.0], [1224.0, 2003.0]]}, {"text": "settings. After filtering to keep only images with natural", "confidence": 0.989070475101471, "text_region": [[1227.0, 2006.0], [2172.0, 2006.0], [2172.0, 2053.0], [1227.0, 2053.0]]}, {"text": "language titles and/or descriptions in English, the dataset", "confidence": 0.996722936630249, "text_region": [[1227.0, 2053.0], [2172.0, 2053.0], [2172.0, 2099.0], [1227.0, 2099.0]]}, {"text": "shrunk by a factor of 6 to only 15 million photos. This is", "confidence": 0.9825599789619446, "text_region": [[1224.0, 2092.0], [2175.0, 2096.0], [2175.0, 2152.0], [1224.0, 2148.0]]}, {"text": "approximately the same size as ImageNet.", "confidence": 0.9994117617607117, "text_region": [[1227.0, 2152.0], [1903.0, 2152.0], [1903.0, 2198.0], [1227.0, 2198.0]]}], "img_idx": 0, "score": 0.9922970533370972}
{"type": "text", "bbox": [220, 2138, 1164, 2651], "res": [{"text": "At the core of our approach is the idea of learning percep-", "confidence": 0.9969111084938049, "text_region": [[216.0, 2132.0], [1164.0, 2132.0], [1164.0, 2188.0], [216.0, 2188.0]]}, {"text": " tion from supervision contained in natural language.", "confidence": 0.9658165574073792, "text_region": [[213.0, 2175.0], [1101.0, 2178.0], [1101.0, 2234.0], [213.0, 2231.0]]}, {"text": ".As", "confidence": 0.981900691986084, "text_region": [[1081.0, 2188.0], [1161.0, 2188.0], [1161.0, 2224.0], [1081.0, 2224.0]]}, {"text": "discussed in the introduction, this is not at all a new idea,", "confidence": 0.9777802228927612, "text_region": [[216.0, 2224.0], [1164.0, 2228.0], [1164.0, 2274.0], [216.0, 2270.0]]}, {"text": "however terminology used to describe work in this space", "confidence": 0.9839528203010559, "text_region": [[216.0, 2274.0], [1164.0, 2274.0], [1164.0, 2330.0], [216.0, 2330.0]]}, {"text": "is varied, even seemingly contradictory, and stated motiva-", "confidence": 0.9907098412513733, "text_region": [[216.0, 2323.0], [1161.0, 2323.0], [1161.0, 2369.0], [216.0, 2369.0]]}, {"text": "tions are diverse. Zhang et al. (2020), Gomez et al. (2017),", "confidence": 0.9958474040031433, "text_region": [[213.0, 2369.0], [1167.0, 2366.0], [1168.0, 2422.0], [213.0, 2426.0]]}, {"text": "Joulin et al. (2016), and Desai & Johnson (2020) all intro-", "confidence": 0.9792141914367676, "text_region": [[220.0, 2422.0], [1157.0, 2422.0], [1157.0, 2465.0], [220.0, 2465.0]]}, {"text": " duce methods which learn visual representations from text", "confidence": 0.9971065521240234, "text_region": [[210.0, 2462.0], [1164.0, 2465.0], [1164.0, 2521.0], [209.0, 2518.0]]}, {"text": "paired with images but describe their approaches as unsuper-", "confidence": 0.9997157454490662, "text_region": [[220.0, 2518.0], [1167.0, 2518.0], [1167.0, 2564.0], [220.0, 2564.0]]}, {"text": "vised, self-supervised, weakly supervised, and supervised", "confidence": 0.9789072275161743, "text_region": [[220.0, 2567.0], [1164.0, 2567.0], [1164.0, 2614.0], [220.0, 2614.0]]}, {"text": "respectively.", "confidence": 0.9975675940513611, "text_region": [[220.0, 2620.0], [416.0, 2620.0], [416.0, 2656.0], [220.0, 2656.0]]}], "img_idx": 0, "score": 0.9920509457588196}
{"type": "text", "bbox": [219, 933, 1159, 1273], "res": [{"text": "Figure 2. CLIP is much more efficient at zero-shot transfer", "confidence": 0.9914331436157227, "text_region": [[220.0, 927.0], [1161.0, 927.0], [1161.0, 973.0], [220.0, 973.0]]}, {"text": "than our image caption baseline. Although highly expressive,", "confidence": 0.9981430768966675, "text_region": [[216.0, 970.0], [1161.0, 970.0], [1161.0, 1016.0], [216.0, 1016.0]]}, {"text": "we found that transformer-based language models are relatively", "confidence": 0.998579740524292, "text_region": [[220.0, 1016.0], [1161.0, 1016.0], [1161.0, 1063.0], [220.0, 1063.0]]}, {"text": "weak at zero-shot ImageNet classification. Here, we see that it", "confidence": 0.9928162693977356, "text_region": [[220.0, 1063.0], [1164.0, 1063.0], [1164.0, 1106.0], [220.0, 1106.0]]}, {"text": "learns 3x slower than a baseline which predicts a bag-of-words", "confidence": 0.9983347058296204, "text_region": [[223.0, 1106.0], [1164.0, 1106.0], [1164.0, 1152.0], [223.0, 1152.0]]}, {"text": "(BoW) encoding of the text (Joulin et al., 2016). Swapping the", "confidence": 0.9960106611251831, "text_region": [[216.0, 1148.0], [1164.0, 1148.0], [1164.0, 1195.0], [216.0, 1195.0]]}, {"text": "prediction objective for the contrastive objective of CLIP further", "confidence": 0.9954829216003418, "text_region": [[220.0, 1191.0], [1164.0, 1191.0], [1164.0, 1238.0], [220.0, 1238.0]]}, {"text": "improves efficiency another 4x.", "confidence": 0.9962596893310547, "text_region": [[220.0, 1238.0], [675.0, 1238.0], [675.0, 1280.0], [220.0, 1280.0]]}], "img_idx": 0, "score": 0.9916331171989441}
{"type": "text", "bbox": [1227, 280, 2167, 551], "res": [{"text": "vision. Although early work wrestled with the complexity", "confidence": 0.9977489709854126, "text_region": [[1231.0, 274.0], [2169.0, 274.0], [2169.0, 320.0], [1231.0, 320.0]]}, {"text": "of natural language when using topic model and n-gram", "confidence": 0.9793280959129333, "text_region": [[1224.0, 323.0], [2169.0, 323.0], [2169.0, 370.0], [1224.0, 370.0]]}, {"text": "representations, improvements in deep contextual represen-", "confidence": 0.9995222687721252, "text_region": [[1227.0, 373.0], [2175.0, 373.0], [2175.0, 419.0], [1227.0, 419.0]]}, {"text": "tation learning suggest we now have the tools to effectively", "confidence": 0.9890549182891846, "text_region": [[1227.0, 422.0], [2172.0, 422.0], [2172.0, 469.0], [1227.0, 469.0]]}, {"text": " leverage this abundant source of supervision (McCann et al.,", "confidence": 0.9794943332672119, "text_region": [[1221.0, 462.0], [2169.0, 462.0], [2169.0, 508.0], [1221.0, 508.0]]}, {"text": "2017).", "confidence": 0.9996293187141418, "text_region": [[1223.0, 504.0], [1338.0, 513.0], [1334.0, 562.0], [1219.0, 553.0]]}], "img_idx": 0, "score": 0.9911791086196899}
{"type": "text", "bbox": [221, 2688, 1157, 2867], "res": [{"text": "We emphasize that what is common across this line of work", "confidence": 0.9863091111183167, "text_region": [[220.0, 2683.0], [1157.0, 2683.0], [1157.0, 2729.0], [220.0, 2729.0]]}, {"text": "is not any of the details of the particular methods used but", "confidence": 0.9880872964859009, "text_region": [[220.0, 2732.0], [1164.0, 2732.0], [1164.0, 2779.0], [220.0, 2779.0]]}, {"text": "the appreciation of natural language as a training signal. All", "confidence": 0.9851990938186646, "text_region": [[213.0, 2775.0], [1167.0, 2772.0], [1168.0, 2828.0], [213.0, 2831.0]]}, {"text": "these approaches are learning from natural language super-", "confidence": 0.9919669032096863, "text_region": [[220.0, 2831.0], [1161.0, 2831.0], [1161.0, 2878.0], [220.0, 2878.0]]}], "img_idx": 0, "score": 0.9893779158592224}
{"type": "title", "bbox": [220, 1983, 472, 2022], "res": [{"text": "2. Approach", "confidence": 0.9911897778511047, "text_region": [[216.0, 1977.0], [479.0, 1977.0], [479.0, 2033.0], [216.0, 2033.0]]}], "img_idx": 0, "score": 0.9629603028297424}
{"type": "title", "bbox": [1234, 1362, 1934, 1397], "res": [{"text": "2.2. Creating a Sufficiently Large Dataset", "confidence": 0.9982386231422424, "text_region": [[1224.0, 1356.0], [1936.0, 1356.0], [1936.0, 1402.0], [1224.0, 1402.0]]}], "img_idx": 0, "score": 0.9611229300498962}
{"type": "title", "bbox": [221, 2062, 812, 2096], "res": [{"text": "2.1. Natural Language Supervision", "confidence": 0.9938517212867737, "text_region": [[216.0, 2056.0], [818.0, 2059.0], [818.0, 2105.0], [216.0, 2102.0]]}], "img_idx": 0, "score": 0.9533593654632568}
{"type": "figure", "bbox": [204, 259, 1157, 878], "res": [{"text": "40", "confidence": 0.9996631145477295, "text_region": [[251.0, 284.0], [301.0, 275.0], [307.0, 312.0], [257.0, 320.0]]}, {"text": " 35", "confidence": 0.888641357421875, "text_region": [[221.0, 352.0], [292.0, 337.0], [298.0, 371.0], [227.0, 385.0]]}, {"text": " 25", "confidence": 0.9408308863639832, "text_region": [[223.0, 465.0], [293.0, 465.0], [293.0, 505.0], [223.0, 505.0]]}, {"text": "29 20", "confidence": 0.7324122786521912, "text_region": [[226.0, 531.0], [293.0, 531.0], [293.0, 568.0], [226.0, 568.0]]}, {"text": " 4X efficiency", "confidence": 0.9584670662879944, "text_region": [[399.0, 571.0], [565.0, 571.0], [565.0, 604.0], [399.0, 604.0]]}, {"text": "3X efficiency", "confidence": 0.9970486760139465, "text_region": [[782.0, 571.0], [941.0, 571.0], [941.0, 607.0], [782.0, 607.0]]}, {"text": " 15", "confidence": 0.965320348739624, "text_region": [[218.0, 595.0], [293.0, 586.0], [298.0, 626.0], [223.0, 635.0]]}, {"text": "10", "confidence": 0.9331578016281128, "text_region": [[263.0, 670.0], [286.0, 670.0], [286.0, 686.0], [263.0, 686.0]]}, {"text": " Bag of Words Contrastive (CLIP)", "confidence": 0.9890146255493164, "text_region": [[692.0, 673.0], [1104.0, 673.0], [1104.0, 716.0], [692.0, 716.0]]}, {"text": " Bag of Words Prediction ", "confidence": 0.9378801584243774, "text_region": [[698.0, 716.0], [1011.0, 716.0], [1011.0, 749.0], [698.0, 749.0]]}, {"text": " Transformer Language Model", "confidence": 0.9769755601882935, "text_region": [[692.0, 746.0], [1074.0, 746.0], [1074.0, 792.0], [692.0, 792.0]]}, {"text": "2M 33M 67M", "confidence": 0.9579593539237976, "text_region": [[283.0, 809.0], [472.0, 809.0], [472.0, 845.0], [283.0, 845.0]]}, {"text": "134M", "confidence": 0.9995908737182617, "text_region": [[542.0, 809.0], [619.0, 809.0], [619.0, 845.0], [542.0, 845.0]]}, {"text": "268M", "confidence": 0.9997366666793823, "text_region": [[815.0, 805.0], [895.0, 805.0], [895.0, 845.0], [815.0, 845.0]]}, {"text": "400M", "confidence": 0.9929477572441101, "text_region": [[1094.0, 815.0], [1154.0, 815.0], [1154.0, 842.0], [1094.0, 842.0]]}, {"text": "# of images processed ", "confidence": 0.9778619408607483, "text_region": [[565.0, 845.0], [865.0, 845.0], [865.0, 888.0], [565.0, 888.0]]}], "img_idx": 0, "score": 0.9559377431869507}
{"type": "header", "bbox": [2146, 192, 2163, 221], "res": [], "img_idx": 0, "score": 0.5871941447257996}
{"type": "header", "bbox": [485, 193, 1366, 225], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9977254271507263, "text_region": [[220.0, 188.0], [1370.0, 188.0], [1370.0, 231.0], [220.0, 231.0]]}], "img_idx": 0, "score": 0.549272894859314}
