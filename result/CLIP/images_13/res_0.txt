{"type": "text", "bbox": [1226, 1744, 2168, 2861], "res": [{"text": "in much more robust models regardless of whether they", "confidence": 0.9987027645111084, "text_region": [[1221.0, 1746.0], [2169.0, 1749.0], [2169.0, 1795.0], [1221.0, 1792.0]]}, {"text": "are zero-shot or fine-tuned. As an initial experiment to", "confidence": 0.9917964339256287, "text_region": [[1224.0, 1795.0], [2172.0, 1795.0], [2172.0, 1841.0], [1224.0, 1841.0]]}, {"text": " potentially begin narrowing this down, we also measure", "confidence": 0.9867261052131653, "text_region": [[1221.0, 1842.0], [2172.0, 1835.0], [2172.0, 1887.0], [1221.0, 1894.0]]}, {"text": "how the performance of CLIP models change after adapting", "confidence": 0.9935293793678284, "text_region": [[1221.0, 1888.0], [2165.0, 1888.0], [2165.0, 1934.0], [1221.0, 1934.0]]}, {"text": " to the ImageNet distribution via a L2 regularized logistic", "confidence": 0.9787207841873169, "text_region": [[1217.0, 1934.0], [2172.0, 1930.0], [2172.0, 1987.0], [1217.0, 1990.0]]}, {"text": " regression classifier fit to CLIP features on the ImageNet", "confidence": 0.9715756177902222, "text_region": [[1217.0, 1983.0], [2175.0, 1980.0], [2175.0, 2036.0], [1217.0, 2039.0]]}, {"text": " training set. We visualize how performance changes from", "confidence": 0.9955030083656311, "text_region": [[1217.0, 2026.0], [2172.0, 2026.0], [2172.0, 2082.0], [1217.0, 2082.0]]}, {"text": "the zero-shot classifier in Figure 14. Although adapting", "confidence": 0.9828504920005798, "text_region": [[1224.0, 2082.0], [2172.0, 2082.0], [2172.0, 2128.0], [1224.0, 2128.0]]}, {"text": " CLIP to the ImageNet distribution increases its ImageNet", "confidence": 0.9692704677581787, "text_region": [[1221.0, 2125.0], [2169.0, 2125.0], [2169.0, 2171.0], [1221.0, 2171.0]]}, {"text": "accuracy by 9.2% to 85.4% overall, and ties the accuracy", "confidence": 0.9965584874153137, "text_region": [[1221.0, 2175.0], [2169.0, 2178.0], [2169.0, 2224.0], [1221.0, 2221.0]]}, {"text": " of the 2018 SOTA from Mahajan et al. (2018), average", "confidence": 0.9882773160934448, "text_region": [[1217.0, 2218.0], [2175.0, 2221.0], [2175.0, 2277.0], [1217.0, 2274.0]]}, {"text": "accuracy under distribution shift slightly decreases.", "confidence": 0.9972630739212036, "text_region": [[1224.0, 2274.0], [2059.0, 2274.0], [2059.0, 2320.0], [1224.0, 2320.0]]}, {"text": "It is surprising to see a 9.2% increase in accuracy, which cor-", "confidence": 0.996523916721344, "text_region": [[1224.0, 2346.0], [2169.0, 2346.0], [2169.0, 2392.0], [1224.0, 2392.0]]}, {"text": "responds to roughly 3 years of improvement in SOTA, fail", "confidence": 0.9963809251785278, "text_region": [[1224.0, 2392.0], [2172.0, 2392.0], [2172.0, 2439.0], [1224.0, 2439.0]]}, {"text": "to translate into any improvement in average performance", "confidence": 0.9849443435668945, "text_region": [[1224.0, 2442.0], [2172.0, 2442.0], [2172.0, 2488.0], [1224.0, 2488.0]]}, {"text": "under distribution shift. We also break down the differences", "confidence": 0.9945491552352905, "text_region": [[1224.0, 2488.0], [2169.0, 2488.0], [2169.0, 2534.0], [1224.0, 2534.0]]}, {"text": "between zero-shot accuracy and linear classifier accuracy", "confidence": 0.99021977186203, "text_region": [[1221.0, 2538.0], [2165.0, 2538.0], [2165.0, 2584.0], [1221.0, 2584.0]]}, {"text": " per dataset in Figure 14 and find performance stillincreases", "confidence": 0.9568153619766235, "text_region": [[1217.0, 2581.0], [2172.0, 2577.0], [2172.0, 2633.0], [1217.0, 2637.0]]}, {"text": " significantly on one dataset, ImageNetV2. ImageNetV2", "confidence": 0.9845598340034485, "text_region": [[1217.0, 2627.0], [2169.0, 2627.0], [2169.0, 2683.0], [1217.0, 2683.0]]}, {"text": " closely followed the creation process of the original Ima-", "confidence": 0.9877932667732239, "text_region": [[1221.0, 2676.0], [2175.0, 2680.0], [2175.0, 2726.0], [1221.0, 2722.0]]}, {"text": "geNet dataset which suggests that gains in accuracy from", "confidence": 0.9913355112075806, "text_region": [[1224.0, 2729.0], [2172.0, 2729.0], [2172.0, 2775.0], [1224.0, 2775.0]]}, {"text": " supervised adaptation are closely concentrated around the", "confidence": 0.9904051423072815, "text_region": [[1221.0, 2779.0], [2172.0, 2775.0], [2172.0, 2821.0], [1221.0, 2825.0]]}, {"text": "ImageNet distribution. Performance decreases by 4.7% on", "confidence": 0.9965296387672424, "text_region": [[1224.0, 2825.0], [2172.0, 2825.0], [2172.0, 2871.0], [1224.0, 2871.0]]}], "img_idx": 0, "score": 0.9925081133842468}
{"type": "text", "bbox": [219, 1754, 1159, 2407], "res": [{"text": "or fine-tuned on the ImageNet dataset. Returning to the", "confidence": 0.9873466491699219, "text_region": [[216.0, 1749.0], [1161.0, 1749.0], [1161.0, 1795.0], [216.0, 1795.0]]}, {"text": " discussion in the introduction to this section - is training", "confidence": 0.981817364692688, "text_region": [[210.0, 1785.0], [1168.0, 1792.0], [1167.0, 1848.0], [209.0, 1841.0]]}, {"text": "or adapting to the ImageNet dataset distribution the cause", "confidence": 0.9871528148651123, "text_region": [[220.0, 1845.0], [1161.0, 1845.0], [1161.0, 1891.0], [220.0, 1891.0]]}, {"text": "of the observed robustness gap? Intuitively, a zero-shot", "confidence": 0.9822582006454468, "text_region": [[216.0, 1891.0], [1167.0, 1891.0], [1167.0, 1937.0], [216.0, 1937.0]]}, {"text": "model should not be able to exploit spurious correlations", "confidence": 0.9941815733909607, "text_region": [[216.0, 1940.0], [1164.0, 1940.0], [1164.0, 1983.0], [216.0, 1983.0]]}, {"text": " or patterns that hold only on a specific distribution, since it", "confidence": 0.9813429117202759, "text_region": [[213.0, 1983.0], [1171.0, 1983.0], [1171.0, 2039.0], [213.0, 2039.0]]}, {"text": "is not trained on that distribution. 4 Thus it is reasonable", "confidence": 0.9955301284790039, "text_region": [[216.0, 2030.0], [1161.0, 2030.0], [1161.0, 2076.0], [216.0, 2076.0]]}, {"text": "to expect zero-shot models to have much higher effective", "confidence": 0.9982460737228394, "text_region": [[216.0, 2082.0], [1164.0, 2082.0], [1164.0, 2128.0], [216.0, 2128.0]]}, {"text": "robustness. In Figure 13, we compare the performance of", "confidence": 0.9920871257781982, "text_region": [[216.0, 2132.0], [1167.0, 2132.0], [1167.0, 2178.0], [216.0, 2178.0]]}, {"text": "zero-shot CLIP with existing ImageNet models on natural", "confidence": 0.9797798991203308, "text_region": [[216.0, 2178.0], [1164.0, 2178.0], [1164.0, 2224.0], [216.0, 2224.0]]}, {"text": "distribution shifts. All zero-shot CLIP models improve", "confidence": 0.9969695806503296, "text_region": [[213.0, 2218.0], [1168.0, 2221.0], [1167.0, 2277.0], [213.0, 2274.0]]}, {"text": "effective robustness by a large amount and reduce the size", "confidence": 0.9940273761749268, "text_region": [[216.0, 2274.0], [1164.0, 2274.0], [1164.0, 2320.0], [216.0, 2320.0]]}, {"text": " of the gap between ImageNet accuracy and accuracy under", "confidence": 0.996632993221283, "text_region": [[210.0, 2313.0], [1168.0, 2317.0], [1167.0, 2373.0], [209.0, 2369.0]]}, {"text": "distribution shift by up to 75%.", "confidence": 0.9750289916992188, "text_region": [[216.0, 2369.0], [718.0, 2369.0], [718.0, 2416.0], [216.0, 2416.0]]}], "img_idx": 0, "score": 0.9849610328674316}
{"type": "text", "bbox": [220, 2446, 1159, 2669], "res": [{"text": "While these results show that zero-shot models can be much", "confidence": 0.9812487959861755, "text_region": [[216.0, 2442.0], [1161.0, 2442.0], [1161.0, 2485.0], [216.0, 2485.0]]}, {"text": "more robust, they do not necessarily mean that supervised", "confidence": 0.9945234060287476, "text_region": [[213.0, 2485.0], [1167.0, 2482.0], [1168.0, 2538.0], [213.0, 2541.0]]}, {"text": "learning on ImageNet causes a robustness gap. Other details", "confidence": 0.9961915016174316, "text_region": [[216.0, 2534.0], [1167.0, 2534.0], [1167.0, 2590.0], [216.0, 2590.0]]}, {"text": " of CLIP, such as its large and diverse pre-training dataset", "confidence": 0.9941904544830322, "text_region": [[210.0, 2577.0], [1168.0, 2581.0], [1167.0, 2637.0], [209.0, 2633.0]]}, {"text": "or use of natural language supervision could also result", "confidence": 0.9936545491218567, "text_region": [[216.0, 2633.0], [1167.0, 2633.0], [1167.0, 2680.0], [216.0, 2680.0]]}], "img_idx": 0, "score": 0.9485893845558167}
{"type": "text", "bbox": [220, 1539, 2174, 1663], "res": [{"text": "Figure 12. CLIP's features are more robust to task shift when compared to models pre-trained on ImageNet. For both dataset", "confidence": 0.9879921674728394, "text_region": [[216.0, 1534.0], [2172.0, 1534.0], [2172.0, 1581.0], [216.0, 1581.0]]}, {"text": "splits, the transfer scores of linear probes trained on the representations of CLIP models are higher than other models with similar", "confidence": 0.9897562861442566, "text_region": [[216.0, 1577.0], [2169.0, 1577.0], [2169.0, 1624.0], [216.0, 1624.0]]}, {"text": "ImageNet performance. This suggests that the representations of models trained on ImageNet are somewhat overfit to their task.", "confidence": 0.9896626472473145, "text_region": [[220.0, 1624.0], [2059.0, 1624.0], [2059.0, 1667.0], [220.0, 1667.0]]}], "img_idx": 0, "score": 0.9412800669670105}
{"type": "figure", "bbox": [201, 301, 1179, 1213], "res": [{"text": " Linear probe average over Kornblith et al.'s 12 datasets", "confidence": 0.9752678871154785, "text_region": [[299.0, 277.0], [1161.0, 277.0], [1161.0, 320.0], [299.0, 320.0]]}, {"text": "90", "confidence": 0.9990869164466858, "text_region": [[273.0, 310.0], [309.0, 310.0], [309.0, 337.0], [273.0, 337.0]]}, {"text": "\u2605", "confidence": 0.9668534398078918, "text_region": [[928.0, 356.0], [961.0, 356.0], [961.0, 383.0], [928.0, 383.0]]}, {"text": "\u2606", "confidence": 0.8943707942962646, "text_region": [[918.0, 403.0], [948.0, 403.0], [948.0, 436.0], [918.0, 436.0]]}, {"text": "85", "confidence": 0.999665379524231, "text_region": [[266.0, 465.0], [313.0, 465.0], [313.0, 505.0], [266.0, 505.0]]}, {"text": "\u2605\u2606", "confidence": 0.936394453048706, "text_region": [[812.0, 472.0], [881.0, 472.0], [881.0, 508.0], [812.0, 508.0]]}, {"text": "\u2606", "confidence": 0.8928884267807007, "text_region": [[575.0, 683.0], [609.0, 683.0], [609.0, 716.0], [575.0, 716.0]]}, {"text": "75", "confidence": 0.9998440742492676, "text_region": [[269.0, 799.0], [303.0, 799.0], [303.0, 825.0], [269.0, 825.0]]}, {"text": "70", "confidence": 0.9996000528335571, "text_region": [[273.0, 960.0], [306.0, 960.0], [306.0, 990.0], [273.0, 990.0]]}, {"text": "65", "confidence": 0.9998379945755005, "text_region": [[273.0, 1125.0], [306.0, 1125.0], [306.0, 1152.0], [273.0, 1152.0]]}, {"text": "70", "confidence": 0.9996591210365295, "text_region": [[466.0, 1152.0], [509.0, 1152.0], [509.0, 1181.0], [466.0, 1181.0]]}, {"text": "75", "confidence": 0.9999183416366577, "text_region": [[632.0, 1152.0], [669.0, 1152.0], [669.0, 1181.0], [632.0, 1181.0]]}, {"text": "80", "confidence": 0.9975385069847107, "text_region": [[792.0, 1152.0], [842.0, 1152.0], [842.0, 1178.0], [792.0, 1178.0]]}, {"text": "85", "confidence": 0.9998921155929565, "text_region": [[961.0, 1152.0], [998.0, 1152.0], [998.0, 1181.0], [961.0, 1181.0]]}, {"text": "90", "confidence": 0.9995965957641602, "text_region": [[1124.0, 1152.0], [1161.0, 1152.0], [1161.0, 1181.0], [1124.0, 1181.0]]}, {"text": "65", "confidence": 0.9997445344924927, "text_region": [[299.0, 1155.0], [336.0, 1155.0], [336.0, 1181.0], [299.0, 1181.0]]}, {"text": "ImageNet Score (%)", "confidence": 0.9981924891471863, "text_region": [[589.0, 1178.0], [871.0, 1175.0], [872.0, 1221.0], [589.0, 1225.0]]}], "img_idx": 0, "score": 0.9099911451339722}
{"type": "figure", "bbox": [625, 1311, 1854, 1492], "res": [{"text": "CLIP-ViT", "confidence": 0.9954172372817993, "text_region": [[691.0, 1311.0], [833.0, 1303.0], [836.0, 1349.0], [694.0, 1357.0]]}, {"text": "ViT (ImageNet-21k)", "confidence": 0.9785481691360474, "text_region": [[1524.0, 1307.0], [1843.0, 1310.0], [1842.0, 1356.0], [1523.0, 1353.0]]}, {"text": "\u2605", "confidence": 0.984053909778595, "text_region": [[632.0, 1320.0], [669.0, 1320.0], [669.0, 1350.0], [632.0, 1350.0]]}, {"text": " Instagram", "confidence": 0.9641419649124146, "text_region": [[1227.0, 1317.0], [1397.0, 1317.0], [1397.0, 1353.0], [1227.0, 1353.0]]}, {"text": "\u25b3", "confidence": 0.6420583724975586, "text_region": [[1477.0, 1330.0], [1500.0, 1330.0], [1500.0, 1346.0], [1477.0, 1346.0]]}, {"text": "\u2606", "confidence": 0.9584766626358032, "text_region": [[629.0, 1360.0], [672.0, 1360.0], [672.0, 1402.0], [629.0, 1402.0]]}, {"text": "CLIP-ResNet", "confidence": 0.9988040328025818, "text_region": [[698.0, 1360.0], [891.0, 1360.0], [891.0, 1396.0], [698.0, 1396.0]]}, {"text": "SimCLRv2", "confidence": 0.999483585357666, "text_region": [[1231.0, 1356.0], [1400.0, 1356.0], [1400.0, 1402.0], [1231.0, 1402.0]]}, {"text": "BiT-M", "confidence": 0.9976778030395508, "text_region": [[1530.0, 1356.0], [1633.0, 1356.0], [1633.0, 1402.0], [1530.0, 1402.0]]}, {"text": "\u25a0", "confidence": 0.6018475890159607, "text_region": [[1470.0, 1366.0], [1500.0, 1366.0], [1500.0, 1396.0], [1470.0, 1396.0]]}, {"text": "EfficientNet-NoisyStudent", "confidence": 0.9966177344322205, "text_region": [[692.0, 1396.0], [1101.0, 1399.0], [1101.0, 1446.0], [692.0, 1442.0]]}, {"text": "BYOL", "confidence": 0.9993261098861694, "text_region": [[1230.0, 1392.0], [1325.0, 1400.0], [1321.0, 1446.0], [1226.0, 1438.0]]}, {"text": "BiT-S", "confidence": 0.9942800402641296, "text_region": [[1530.0, 1399.0], [1630.0, 1399.0], [1630.0, 1445.0], [1530.0, 1445.0]]}, {"text": "+", "confidence": 0.9527220129966736, "text_region": [[642.0, 1416.0], [665.0, 1416.0], [665.0, 1439.0], [642.0, 1439.0]]}, {"text": "\u25bc", "confidence": 0.7549821734428406, "text_region": [[1470.0, 1409.0], [1503.0, 1409.0], [1503.0, 1436.0], [1470.0, 1436.0]]}, {"text": "EfficientNet", "confidence": 0.999871015548706, "text_region": [[702.0, 1449.0], [885.0, 1449.0], [885.0, 1485.0], [702.0, 1485.0]]}, {"text": "MoCo", "confidence": 0.9985885620117188, "text_region": [[1227.0, 1445.0], [1330.0, 1445.0], [1330.0, 1492.0], [1227.0, 1492.0]]}, {"text": "ResNet", "confidence": 0.9997127652168274, "text_region": [[1530.0, 1445.0], [1656.0, 1445.0], [1656.0, 1492.0], [1530.0, 1492.0]]}, {"text": "+", "confidence": 0.922235906124115, "text_region": [[642.0, 1459.0], [665.0, 1459.0], [665.0, 1482.0], [642.0, 1482.0]]}, {"text": "+", "confidence": 0.800190269947052, "text_region": [[1470.0, 1455.0], [1503.0, 1455.0], [1503.0, 1482.0], [1470.0, 1482.0]]}], "img_idx": 0, "score": 0.7574484348297119}
{"type": "figure", "bbox": [1195, 275, 2172, 1246], "res": [{"text": "Linear probe average over 26 datasets", "confidence": 0.9881391525268555, "text_region": [[1420.0, 277.0], [2022.0, 277.0], [2022.0, 320.0], [1420.0, 320.0]]}, {"text": "90", "confidence": 0.9993975758552551, "text_region": [[1261.0, 310.0], [1304.0, 310.0], [1304.0, 337.0], [1261.0, 337.0]]}, {"text": "85", "confidence": 0.902347207069397, "text_region": [[1257.0, 484.0], [1280.0, 468.0], [1294.0, 486.0], [1271.0, 503.0]]}, {"text": "\u2606", "confidence": 0.9185256361961365, "text_region": [[1567.0, 766.0], [1597.0, 766.0], [1597.0, 795.0], [1567.0, 795.0]]}, {"text": "70", "confidence": 0.9996267557144165, "text_region": [[1454.0, 1152.0], [1497.0, 1152.0], [1497.0, 1181.0], [1454.0, 1181.0]]}, {"text": "75", "confidence": 0.9997031688690186, "text_region": [[1616.0, 1145.0], [1663.0, 1145.0], [1663.0, 1185.0], [1616.0, 1185.0]]}, {"text": "80", "confidence": 0.9917396306991577, "text_region": [[1779.0, 1145.0], [1829.0, 1145.0], [1829.0, 1185.0], [1779.0, 1185.0]]}, {"text": "65", "confidence": 0.9999073147773743, "text_region": [[1291.0, 1155.0], [1324.0, 1155.0], [1324.0, 1181.0], [1291.0, 1181.0]]}, {"text": "85", "confidence": 0.9992822408676147, "text_region": [[1942.0, 1148.0], [1992.0, 1148.0], [1992.0, 1188.0], [1942.0, 1188.0]]}, {"text": "90", "confidence": 0.9992556571960449, "text_region": [[2115.0, 1152.0], [2152.0, 1152.0], [2152.0, 1181.0], [2115.0, 1181.0]]}, {"text": "ImageNet Score (%) ", "confidence": 0.9793738126754761, "text_region": [[1583.0, 1178.0], [1863.0, 1178.0], [1863.0, 1221.0], [1583.0, 1221.0]]}], "img_idx": 0, "score": 0.7443298101425171}
{"type": "header", "bbox": [2129, 193, 2165, 218], "res": [{"text": "14", "confidence": 0.9999827146530151, "text_region": [[2129.0, 191.0], [2169.0, 191.0], [2169.0, 221.0], [2129.0, 221.0]]}], "img_idx": 0, "score": 0.9103356003761292}
{"type": "header", "bbox": [228, 193, 1125, 225], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.991613507270813, "text_region": [[220.0, 188.0], [1374.0, 188.0], [1374.0, 231.0], [220.0, 231.0]]}], "img_idx": 0, "score": 0.6809182167053223}
{"type": "reference", "bbox": [219, 2712, 1160, 2824], "res": [{"text": "4We caution that a zero-shot model can still exploit spurious", "confidence": 0.9886297583580017, "text_region": [[266.0, 2703.0], [1161.0, 2709.0], [1161.0, 2756.0], [266.0, 2749.0]]}, {"text": " correlations that are shared between the pre-training and evaluation", "confidence": 0.9948078393936157, "text_region": [[213.0, 2752.0], [1167.0, 2752.0], [1167.0, 2798.0], [213.0, 2798.0]]}, {"text": " distributions.", "confidence": 0.9946282505989075, "text_region": [[214.0, 2788.0], [410.0, 2795.0], [408.0, 2832.0], [212.0, 2824.0]]}], "img_idx": 0, "score": 0.9662519693374634}
