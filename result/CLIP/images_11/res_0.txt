{"type": "text", "bbox": [1227, 1895, 2168, 2837], "res": [{"text": "and HatefulMemes), geo-localization and scene recognition", "confidence": 0.9892878532409668, "text_region": [[1224.0, 1891.0], [2172.0, 1891.0], [2172.0, 1937.0], [1224.0, 1937.0]]}, {"text": "(Country211, SUN397), and activity recognition in videos ", "confidence": 0.9906612634658813, "text_region": [[1224.0, 1937.0], [2172.0, 1937.0], [2172.0, 1983.0], [1224.0, 1983.0]]}, {"text": "(Kinetics700 and UCF101). In addition CLIP also does", "confidence": 0.980475127696991, "text_region": [[1224.0, 1987.0], [2172.0, 1987.0], [2172.0, 2030.0], [1224.0, 2030.0]]}, {"text": "much better on fine-grained car and traffic sign recognition", "confidence": 0.9907572269439697, "text_region": [[1224.0, 2033.0], [2172.0, 2033.0], [2172.0, 2079.0], [1224.0, 2079.0]]}, {"text": "(Stanford Cars and GTSRB). This may reflect a problem", "confidence": 0.9865989089012146, "text_region": [[1227.0, 2082.0], [2172.0, 2082.0], [2172.0, 2128.0], [1227.0, 2128.0]]}, {"text": "with overly narrow supervision in ImageNet. A result such", "confidence": 0.992062509059906, "text_region": [[1221.0, 2125.0], [2169.0, 2125.0], [2169.0, 2171.0], [1221.0, 2171.0]]}, {"text": "as the 14.7% improvement on GTSRB could be indicative", "confidence": 0.9877658486366272, "text_region": [[1224.0, 2178.0], [2172.0, 2178.0], [2172.0, 2221.0], [1224.0, 2221.0]]}, {"text": " of an issue with ImageNet-1K, which has only a single la-", "confidence": 0.9802118539810181, "text_region": [[1221.0, 2221.0], [2169.0, 2221.0], [2169.0, 2267.0], [1221.0, 2267.0]]}, {"text": " bel for all traffic and street signs. This could encourage", "confidence": 0.9833387732505798, "text_region": [[1217.0, 2264.0], [2175.0, 2267.0], [2175.0, 2323.0], [1217.0, 2320.0]]}, {"text": " a supervised representation to collapse intra-class details", "confidence": 0.9913882613182068, "text_region": [[1221.0, 2320.0], [2172.0, 2317.0], [2172.0, 2363.0], [1221.0, 2366.0]]}, {"text": "and hurt accuracy on a fine-grained downstream task. As", "confidence": 0.9969834089279175, "text_region": [[1224.0, 2369.0], [2172.0, 2369.0], [2172.0, 2416.0], [1224.0, 2416.0]]}, {"text": " mentioned, CLIP still underperforms the EfficientNet on", "confidence": 0.9920884370803833, "text_region": [[1221.0, 2412.0], [2165.0, 2412.0], [2165.0, 2458.0], [1221.0, 2458.0]]}, {"text": "several datasets. Unsurprisingly, the dataset that the Effi-", "confidence": 0.993782103061676, "text_region": [[1224.0, 2465.0], [2169.0, 2465.0], [2169.0, 2511.0], [1224.0, 2511.0]]}, {"text": " cientNet does best relative to CLIP on is the one it was", "confidence": 0.9789485335350037, "text_region": [[1221.0, 2508.0], [2172.0, 2511.0], [2172.0, 2558.0], [1221.0, 2554.0]]}, {"text": " trained on: ImageNet. The EffcientNet also slightly outper-", "confidence": 0.9760167002677917, "text_region": [[1217.0, 2551.0], [2179.0, 2554.0], [2178.0, 2610.0], [1217.0, 2607.0]]}, {"text": "forms CLIP on low-resolution datasets such as CIFAR10", "confidence": 0.9984737634658813, "text_region": [[1224.0, 2607.0], [2175.0, 2607.0], [2175.0, 2653.0], [1224.0, 2653.0]]}, {"text": "and CIFAR10o. We suspect this is at least partly due to the", "confidence": 0.9870718717575073, "text_region": [[1224.0, 2656.0], [2169.0, 2656.0], [2169.0, 2703.0], [1224.0, 2703.0]]}, {"text": "lack of scale-based data augmentation in CLIP. The Effi-", "confidence": 0.9885174632072449, "text_region": [[1224.0, 2703.0], [2175.0, 2703.0], [2175.0, 2749.0], [1224.0, 2749.0]]}, {"text": "cientNet also does slightly better on PatchCamelyon and", "confidence": 0.9962425231933594, "text_region": [[1224.0, 2752.0], [2175.0, 2752.0], [2175.0, 2795.0], [1224.0, 2795.0]]}, {"text": "CLEVRCounts, datasets where overall performance is still", "confidence": 0.9871610999107361, "text_region": [[1224.0, 2798.0], [2172.0, 2798.0], [2172.0, 2845.0], [1224.0, 2845.0]]}], "img_idx": 0, "score": 0.9960421919822693}
{"type": "text", "bbox": [219, 1895, 1159, 2545], "res": [{"text": "On this broader evaluation suite, the benefits of CLIP are", "confidence": 0.9934227466583252, "text_region": [[220.0, 1891.0], [1161.0, 1891.0], [1161.0, 1934.0], [220.0, 1934.0]]}, {"text": "more clear. All CLIP models, regardless of scale, outper-", "confidence": 0.9906674027442932, "text_region": [[220.0, 1937.0], [1167.0, 1937.0], [1167.0, 1983.0], [220.0, 1983.0]]}, {"text": "form all evaluated systems in terms of compute efficiency.", "confidence": 0.9889305233955383, "text_region": [[216.0, 1987.0], [1167.0, 1987.0], [1167.0, 2033.0], [216.0, 2033.0]]}, {"text": "The improvement in average score of the best model over", "confidence": 0.9958491325378418, "text_region": [[216.0, 2030.0], [1161.0, 2030.0], [1161.0, 2076.0], [216.0, 2076.0]]}, {"text": "previous systems increases from 2.6% to 5%. We also find", "confidence": 0.9982891082763672, "text_region": [[213.0, 2076.0], [1164.0, 2072.0], [1164.0, 2128.0], [213.0, 2132.0]]}, {"text": "that self-supervised systems do noticeably better on our", "confidence": 0.9927653670310974, "text_region": [[216.0, 2128.0], [1164.0, 2128.0], [1164.0, 2175.0], [216.0, 2175.0]]}, {"text": "broader evaluation suite. For instance, while SimCLRv2", "confidence": 0.9993723630905151, "text_region": [[220.0, 2178.0], [1161.0, 2178.0], [1161.0, 2221.0], [220.0, 2221.0]]}, {"text": "still underperforms BiT-M on average on the 12 datasets", "confidence": 0.9822663068771362, "text_region": [[216.0, 2221.0], [1161.0, 2221.0], [1161.0, 2267.0], [216.0, 2267.0]]}, {"text": "of Kornblith et al. (2019), SimCLRv2 outperforms BiT-M", "confidence": 0.9895037412643433, "text_region": [[216.0, 2270.0], [1164.0, 2270.0], [1164.0, 2317.0], [216.0, 2317.0]]}, {"text": "on our 27 dataset evaluation suite. These findings suggest", "confidence": 0.9805392622947693, "text_region": [[216.0, 2313.0], [1164.0, 2320.0], [1164.0, 2373.0], [216.0, 2366.0]]}, {"text": "continuing to expand task diversity and coverage in order", "confidence": 0.9960638284683228, "text_region": [[220.0, 2366.0], [1161.0, 2366.0], [1161.0, 2412.0], [220.0, 2412.0]]}, {"text": "to better understand the \u201cgeneral' performance of systems.", "confidence": 0.981636643409729, "text_region": [[216.0, 2412.0], [1161.0, 2412.0], [1161.0, 2458.0], [216.0, 2458.0]]}, {"text": "We suspect additional evaluation efforts along the lines of", "confidence": 0.9881361722946167, "text_region": [[210.0, 2455.0], [1168.0, 2459.0], [1167.0, 2515.0], [209.0, 2511.0]]}, {"text": "VTAB to be valuable.", "confidence": 0.9937518239021301, "text_region": [[216.0, 2508.0], [565.0, 2508.0], [565.0, 2551.0], [216.0, 2551.0]]}], "img_idx": 0, "score": 0.9835671186447144}
{"type": "text", "bbox": [222, 2590, 1158, 2860], "res": [{"text": " In addition to the aggregate analysis above, we visualize", "confidence": 0.9858005046844482, "text_region": [[213.0, 2581.0], [1164.0, 2581.0], [1164.0, 2637.0], [213.0, 2637.0]]}, {"text": " per-dataset differences in the performance of the best CLIP", "confidence": 0.9931507110595703, "text_region": [[213.0, 2630.0], [1164.0, 2627.0], [1164.0, 2673.0], [213.0, 2676.0]]}, {"text": "model and the best model in our evaluation suite across", "confidence": 0.9835967421531677, "text_region": [[216.0, 2676.0], [1164.0, 2680.0], [1164.0, 2726.0], [216.0, 2722.0]]}, {"text": "all 27 datasets in Figure 11. CLIP outperforms the Noisy", "confidence": 0.9931239485740662, "text_region": [[213.0, 2716.0], [1168.0, 2723.0], [1167.0, 2779.0], [213.0, 2772.0]]}, {"text": "Student EfficientNet-L2 on 21 of the 27 datasets. CLIP", "confidence": 0.9758398532867432, "text_region": [[216.0, 2775.0], [1164.0, 2775.0], [1164.0, 2822.0], [216.0, 2822.0]]}, {"text": "improves the most on tasks which require OCR (SST2", "confidence": 0.9849907159805298, "text_region": [[216.0, 2822.0], [1161.0, 2818.0], [1161.0, 2864.0], [216.0, 2868.0]]}], "img_idx": 0, "score": 0.982894778251648}
{"type": "text", "bbox": [217, 1552, 2168, 1816], "res": [{"text": "Figure 10. Linear probe performance of CLIP models in comparison with state-of-the-art computer vision models, including", "confidence": 0.994279146194458, "text_region": [[216.0, 1544.0], [2169.0, 1544.0], [2169.0, 1591.0], [216.0, 1591.0]]}, {"text": "EfficientNet (Tan & Le, 2019; Xie et al., 2020), MoCo (Chen et al., 2020d), Instagram-pretrained ResNeXt models (Mahajan et al., 2018;", "confidence": 0.9894864559173584, "text_region": [[220.0, 1587.0], [2172.0, 1587.0], [2172.0, 1634.0], [220.0, 1634.0]]}, {"text": "Touvron et al., 2019), BiT (Kolesnikov et al., 2019), ViT (Dosovitskiy et al., 2020), SimCLRv2 (Chen et al., 2020c), BYOL (Grill et al.", "confidence": 0.9935097098350525, "text_region": [[220.0, 1634.0], [2169.0, 1634.0], [2169.0, 1676.0], [220.0, 1676.0]]}, {"text": "2020), and the original ResNet models (He et al., 2016b). (Left) Scores are averaged over 12 datasets studied by Kornblith et al. (2019).", "confidence": 0.9868690967559814, "text_region": [[220.0, 1676.0], [2172.0, 1676.0], [2172.0, 1723.0], [220.0, 1723.0]]}, {"text": "(Right) Scores are averaged over 27 datasets that contain a wider variety of distributions. Dotted lines indicate models fine-tuned or", "confidence": 0.9831809401512146, "text_region": [[216.0, 1723.0], [2172.0, 1723.0], [2172.0, 1766.0], [216.0, 1766.0]]}, {"text": "evaluated on images at a higher-resolution than pre-training. See Table 10 for individual scores and Figure 20 for plots for each dataset.", "confidence": 0.9890430569648743, "text_region": [[216.0, 1762.0], [2169.0, 1762.0], [2169.0, 1808.0], [216.0, 1808.0]]}], "img_idx": 0, "score": 0.9699638485908508}
{"type": "text", "bbox": [302, 282, 1168, 313], "res": [{"text": "Linear probe average over Kornblith et al.'s 12 datasets", "confidence": 0.9817376136779785, "text_region": [[306.0, 277.0], [1171.0, 277.0], [1171.0, 320.0], [306.0, 320.0]]}], "img_idx": 0, "score": 0.5284427404403687}
{"type": "figure", "bbox": [201, 306, 1170, 1205], "res": [{"text": "Linear probe average over Kornblith et al.'s 12 datasets", "confidence": 0.9817376136779785, "text_region": [[306.0, 277.0], [1171.0, 277.0], [1171.0, 320.0], [306.0, 320.0]]}, {"text": "90", "confidence": 0.9993783831596375, "text_region": [[273.0, 350.0], [309.0, 350.0], [309.0, 380.0], [273.0, 380.0]]}, {"text": "L/14@336px", "confidence": 0.9966520071029663, "text_region": [[908.0, 343.0], [1108.0, 343.0], [1108.0, 380.0], [908.0, 380.0]]}, {"text": "L/14", "confidence": 0.9982242584228516, "text_region": [[848.0, 386.0], [928.0, 386.0], [928.0, 422.0], [848.0, 422.0]]}, {"text": ".\u2605", "confidence": 0.8581995964050293, "text_region": [[961.0, 380.0], [1034.0, 380.0], [1034.0, 419.0], [961.0, 419.0]]}, {"text": "RN50x64", "confidence": 0.9935907125473022, "text_region": [[971.0, 429.0], [1121.0, 429.0], [1121.0, 465.0], [971.0, 465.0]]}, {"text": "L2-475", "confidence": 0.9991142749786377, "text_region": [[910.0, 474.0], [1006.0, 483.0], [1002.0, 519.0], [907.0, 510.0]]}, {"text": "\u2606", "confidence": 0.9859898090362549, "text_region": [[1024.0, 469.0], [1071.0, 469.0], [1071.0, 508.0], [1024.0, 508.0]]}, {"text": "L2-800", "confidence": 0.950240433216095, "text_region": [[1068.0, 495.0], [1138.0, 495.0], [1138.0, 535.0], [1068.0, 535.0]]}, {"text": "85", "confidence": 0.999779462814331, "text_region": [[273.0, 545.0], [312.0, 556.0], [303.0, 587.0], [264.0, 576.0]]}, {"text": "B/16", "confidence": 0.9983737468719482, "text_region": [[685.0, 541.0], [755.0, 541.0], [755.0, 568.0], [685.0, 568.0]]}, {"text": "WRN50x16", "confidence": 0.9327571988105774, "text_region": [[895.0, 571.0], [1068.0, 571.0], [1068.0, 607.0], [895.0, 607.0]]}, {"text": "..FixRes", "confidence": 0.8602379560470581, "text_region": [[1041.0, 637.0], [1138.0, 637.0], [1138.0, 673.0], [1041.0, 673.0]]}, {"text": "RN50x4", "confidence": 0.9066267013549805, "text_region": [[648.0, 661.0], [787.0, 653.0], [789.0, 686.0], [650.0, 694.0]]}, {"text": "B/32", "confidence": 0.9970742464065552, "text_region": [[539.0, 683.0], [602.0, 683.0], [602.0, 710.0], [539.0, 710.0]]}, {"text": "x48", "confidence": 0.8661865592002869, "text_region": [[991.0, 680.0], [1041.0, 680.0], [1041.0, 703.0], [991.0, 703.0]]}, {"text": "R152", "confidence": 0.9975806474685669, "text_region": [[1053.0, 694.0], [1119.0, 685.0], [1122.0, 712.0], [1057.0, 720.0]]}, {"text": "R152x4", "confidence": 0.9786279797554016, "text_region": [[1014.0, 729.0], [1111.0, 729.0], [1111.0, 762.0], [1014.0, 762.0]]}, {"text": "RN50W", "confidence": 0.982282280921936, "text_region": [[549.0, 825.0], [662.0, 825.0], [662.0, 858.0], [549.0, 858.0]]}, {"text": "res50x1Y", "confidence": 0.9955704212188721, "text_region": [[505.0, 912.0], [617.0, 903.0], [620.0, 940.0], [507.0, 948.0]]}, {"text": "R152x4", "confidence": 0.9667783379554749, "text_region": [[1004.0, 937.0], [1111.0, 937.0], [1111.0, 970.0], [1004.0, 970.0]]}, {"text": "75", "confidence": 0.9999165534973145, "text_region": [[273.0, 970.0], [309.0, 970.0], [309.0, 1000.0], [273.0, 1000.0]]}, {"text": "MResNet152", "confidence": 0.9246363639831543, "text_region": [[695.0, 993.0], [848.0, 993.0], [848.0, 1030.0], [695.0, 1030.0]]}, {"text": "ResNet50t", "confidence": 0.9684721827507019, "text_region": [[488.0, 1047.0], [624.0, 1039.0], [626.0, 1072.0], [490.0, 1080.0]]}, {"text": "MoCo-v2?", "confidence": 0.9333539605140686, "text_region": [[494.0, 1090.0], [616.0, 1081.0], [620.0, 1131.0], [498.0, 1140.0]]}, {"text": "100", "confidence": 0.9851918816566467, "text_region": [[422.0, 1152.0], [482.0, 1152.0], [482.0, 1191.0], [422.0, 1191.0]]}, {"text": "101", "confidence": 0.9985173344612122, "text_region": [[669.0, 1152.0], [725.0, 1152.0], [725.0, 1188.0], [669.0, 1188.0]]}, {"text": "102", "confidence": 0.9910814166069031, "text_region": [[915.0, 1155.0], [975.0, 1155.0], [975.0, 1191.0], [915.0, 1191.0]]}, {"text": "Forward-pass GFLOPs/image ", "confidence": 0.9703816175460815, "text_region": [[545.0, 1188.0], [935.0, 1188.0], [935.0, 1231.0], [545.0, 1231.0]]}], "img_idx": 0, "score": 0.9554555416107178}
{"type": "figure", "bbox": [1205, 278, 2168, 1228], "res": [{"text": "Linear probe average over all 27 datasets", "confidence": 0.9957684874534607, "text_region": [[1407.0, 271.0], [2059.0, 274.0], [2059.0, 320.0], [1407.0, 317.0]]}, {"text": "L/14@336px", "confidence": 0.9980524182319641, "text_region": [[1909.0, 346.0], [2109.0, 346.0], [2109.0, 383.0], [1909.0, 383.0]]}, {"text": "85", "confidence": 0.9999123215675354, "text_region": [[1271.0, 393.0], [1307.0, 393.0], [1307.0, 419.0], [1271.0, 419.0]]}, {"text": ".\u2605", "confidence": 0.6732180118560791, "text_region": [[1979.0, 386.0], [2039.0, 386.0], [2039.0, 422.0], [1979.0, 422.0]]}, {"text": "L/14", "confidence": 0.9991851449012756, "text_region": [[1849.0, 409.0], [1919.0, 409.0], [1919.0, 436.0], [1849.0, 436.0]]}, {"text": "RN50x64", "confidence": 0.9917531609535217, "text_region": [[1962.0, 432.0], [2122.0, 432.0], [2122.0, 479.0], [1962.0, 479.0]]}, {"text": "\u2606", "confidence": 0.9954918026924133, "text_region": [[2016.0, 479.0], [2069.0, 479.0], [2069.0, 518.0], [2016.0, 518.0]]}, {"text": "RN50x16", "confidence": 0.9873528480529785, "text_region": [[1799.0, 518.0], [1949.0, 518.0], [1949.0, 554.0], [1799.0, 554.0]]}, {"text": "B/16", "confidence": 0.9985156059265137, "text_region": [[1680.0, 548.0], [1760.0, 548.0], [1760.0, 584.0], [1680.0, 584.0]]}, {"text": "\u2606", "confidence": 0.8215879797935486, "text_region": [[1896.0, 564.0], [1939.0, 564.0], [1939.0, 591.0], [1896.0, 591.0]]}, {"text": "L2-800", "confidence": 0.9976480007171631, "text_region": [[2024.0, 565.0], [2131.0, 583.0], [2123.0, 629.0], [2017.0, 612.0]]}, {"text": "RN50x4", "confidence": 0.9533341526985168, "text_region": [[1646.0, 647.0], [1776.0, 647.0], [1776.0, 683.0], [1646.0, 683.0]]}, {"text": "B7-NS", "confidence": 0.9808158874511719, "text_region": [[1829.0, 685.0], [1907.0, 698.0], [1903.0, 724.0], [1825.0, 712.0]]}, {"text": "S", "confidence": 0.5736444592475891, "text_region": [[1231.0, 706.0], [1261.0, 706.0], [1261.0, 742.0], [1231.0, 742.0]]}, {"text": "RN101", "confidence": 0.9665926694869995, "text_region": [[1572.0, 730.0], [1694.0, 722.0], [1697.0, 768.0], [1575.0, 776.0]]}, {"text": "6", "confidence": 0.6212925910949707, "text_region": [[1226.0, 747.0], [1257.0, 741.0], [1266.0, 788.0], [1235.0, 794.0]]}, {"text": "FixRes", "confidence": 0.9963400363922119, "text_region": [[2052.0, 742.0], [2142.0, 742.0], [2142.0, 776.0], [2052.0, 776.0]]}, {"text": "R152x3", "confidence": 0.9430320858955383, "text_region": [[2039.0, 772.0], [2142.0, 772.0], [2142.0, 805.0], [2039.0, 805.0]]}, {"text": "RN50", "confidence": 0.9736944437026978, "text_region": [[1543.0, 795.0], [1660.0, 795.0], [1660.0, 842.0], [1543.0, 842.0]]}, {"text": "\u00d748", "confidence": 0.7948923110961914, "text_region": [[1979.0, 795.0], [2039.0, 795.0], [2039.0, 822.0], [1979.0, 822.0]]}, {"text": "75", "confidence": 0.9994468688964844, "text_region": [[1241.0, 828.0], [1300.0, 828.0], [1300.0, 855.0], [1241.0, 855.0]]}, {"text": "R152x4", "confidence": 0.841881275177002, "text_region": [[1999.0, 825.0], [2109.0, 825.0], [2109.0, 858.0], [1999.0, 858.0]]}, {"text": "\u25b3viT-H/14", "confidence": 0.8721559047698975, "text_region": [[1982.0, 861.0], [2105.0, 861.0], [2105.0, 898.0], [1982.0, 898.0]]}, {"text": "res50x1l", "confidence": 0.8704772591590881, "text_region": [[1513.0, 924.0], [1620.0, 924.0], [1620.0, 947.0], [1513.0, 947.0]]}, {"text": "R152x4", "confidence": 0.9447402954101562, "text_region": [[1996.0, 949.0], [2104.0, 962.0], [2098.0, 1008.0], [1991.0, 995.0]]}, {"text": "MoCo-v2", "confidence": 0.9867329597473145, "text_region": [[1492.0, 1018.0], [1620.0, 1005.0], [1625.0, 1051.0], [1497.0, 1064.0]]}, {"text": "MResNet152", "confidence": 0.9333697557449341, "text_region": [[1700.0, 1016.0], [1846.0, 1016.0], [1846.0, 1049.0], [1700.0, 1049.0]]}, {"text": "70", "confidence": 0.9997541308403015, "text_region": [[1267.0, 1046.0], [1307.0, 1046.0], [1307.0, 1076.0], [1267.0, 1076.0]]}, {"text": "ResNet50t", "confidence": 0.9659901857376099, "text_region": [[1486.0, 1057.0], [1615.0, 1049.0], [1617.0, 1085.0], [1488.0, 1093.0]]}, {"text": "100", "confidence": 0.9873404502868652, "text_region": [[1420.0, 1152.0], [1483.0, 1152.0], [1483.0, 1191.0], [1420.0, 1191.0]]}, {"text": "101", "confidence": 0.9976150393486023, "text_region": [[1666.0, 1152.0], [1723.0, 1152.0], [1723.0, 1188.0], [1666.0, 1188.0]]}, {"text": "102", "confidence": 0.9980435371398926, "text_region": [[1916.0, 1152.0], [1972.0, 1152.0], [1972.0, 1191.0], [1916.0, 1191.0]]}, {"text": "Forward-pass GFLOPs/image", "confidence": 0.9992015957832336, "text_region": [[1540.0, 1185.0], [1936.0, 1188.0], [1936.0, 1234.0], [1540.0, 1231.0]]}], "img_idx": 0, "score": 0.9487798810005188}
{"type": "figure", "bbox": [517, 1325, 1940, 1499], "res": [{"text": "CLIP-ViT", "confidence": 0.9985879063606262, "text_region": [[619.0, 1323.0], [752.0, 1323.0], [752.0, 1360.0], [619.0, 1360.0]]}, {"text": " Instagram-pretrained", "confidence": 0.9771933555603027, "text_region": [[1151.0, 1317.0], [1497.0, 1320.0], [1496.0, 1366.0], [1151.0, 1363.0]]}, {"text": "\u2605", "confidence": 0.8975919485092163, "text_region": [[545.0, 1327.0], [595.0, 1327.0], [595.0, 1360.0], [545.0, 1360.0]]}, {"text": "I", "confidence": 0.6359125971794128, "text_region": [[1553.0, 1327.0], [1643.0, 1327.0], [1643.0, 1360.0], [1553.0, 1360.0]]}, {"text": "ViT (ImageNet-21k)", "confidence": 0.9710908532142639, "text_region": [[1626.0, 1323.0], [1939.0, 1323.0], [1939.0, 1366.0], [1626.0, 1366.0]]}, {"text": "\u2606]", "confidence": 0.6510933637619019, "text_region": [[536.0, 1366.0], [605.0, 1366.0], [605.0, 1406.0], [536.0, 1406.0]]}, {"text": "CLIP-ResNet", "confidence": 0.9973577857017517, "text_region": [[619.0, 1370.0], [812.0, 1370.0], [812.0, 1406.0], [619.0, 1406.0]]}, {"text": "SimCLRv2", "confidence": 0.9993511438369751, "text_region": [[1154.0, 1366.0], [1324.0, 1366.0], [1324.0, 1412.0], [1154.0, 1412.0]]}, {"text": "1", "confidence": 0.8751651048660278, "text_region": [[1553.0, 1376.0], [1626.0, 1376.0], [1626.0, 1402.0], [1553.0, 1402.0]]}, {"text": "BiT-M", "confidence": 0.9954789876937866, "text_region": [[1630.0, 1370.0], [1730.0, 1370.0], [1730.0, 1406.0], [1630.0, 1406.0]]}, {"text": " EfficientNet-NoisyStudent", "confidence": 0.9826398491859436, "text_region": [[612.0, 1406.0], [1025.0, 1409.0], [1024.0, 1455.0], [612.0, 1452.0]]}, {"text": "BYOL", "confidence": 0.999509334564209, "text_region": [[1151.0, 1409.0], [1251.0, 1409.0], [1251.0, 1455.0], [1151.0, 1455.0]]}, {"text": "\u2192 BiT-S", "confidence": 0.8883756399154663, "text_region": [[1543.0, 1409.0], [1723.0, 1409.0], [1723.0, 1455.0], [1543.0, 1455.0]]}, {"text": "+", "confidence": 0.9136790633201599, "text_region": [[542.0, 1422.0], [602.0, 1422.0], [602.0, 1445.0], [542.0, 1445.0]]}, {"text": "-+\u2014 EfficientNet", "confidence": 0.9031866192817688, "text_region": [[530.0, 1452.0], [809.0, 1459.0], [808.0, 1502.0], [528.0, 1495.0]]}, {"text": "MoCo", "confidence": 0.9980195760726929, "text_region": [[1148.0, 1455.0], [1254.0, 1455.0], [1254.0, 1502.0], [1148.0, 1502.0]]}, {"text": "\u2014\u2014 ResNet", "confidence": 0.9419640302658081, "text_region": [[1544.0, 1451.0], [1757.0, 1459.0], [1755.0, 1505.0], [1543.0, 1498.0]]}], "img_idx": 0, "score": 0.8864825963973999}
{"type": "figure_caption", "bbox": [557, 1195, 928, 1224], "res": [{"text": "Forward-pass GFLOPs/image ", "confidence": 0.9703816175460815, "text_region": [[545.0, 1188.0], [935.0, 1188.0], [935.0, 1231.0], [545.0, 1231.0]]}], "img_idx": 0, "score": 0.6671462059020996}
{"type": "header", "bbox": [2129, 192, 2164, 217], "res": [{"text": "12", "confidence": 0.9997252225875854, "text_region": [[2129.0, 191.0], [2165.0, 191.0], [2165.0, 221.0], [2129.0, 221.0]]}], "img_idx": 0, "score": 0.9074721336364746}
{"type": "header", "bbox": [228, 193, 1125, 224], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9919780492782593, "text_region": [[220.0, 188.0], [1374.0, 188.0], [1374.0, 231.0], [220.0, 231.0]]}], "img_idx": 0, "score": 0.6973171830177307}
