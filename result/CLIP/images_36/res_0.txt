{"type": "text", "bbox": [1229, 664, 2167, 986], "res": [{"text": "CLIP-RN Five ResNet-based contrastive CLIP models", "confidence": 0.9882348775863647, "text_region": [[1227.0, 660.0], [2172.0, 660.0], [2172.0, 703.0], [1227.0, 703.0]]}, {"text": "are included. As discussed in the paper, the first two models", "confidence": 0.9914229512214661, "text_region": [[1227.0, 710.0], [2169.0, 710.0], [2169.0, 756.0], [1227.0, 756.0]]}, {"text": "follow ResNet-50 and ResNet-101, and we use EfficientNet-", "confidence": 0.9907194972038269, "text_region": [[1224.0, 756.0], [2175.0, 756.0], [2175.0, 802.0], [1224.0, 802.0]]}, {"text": "style (Tan & Le, 2019) scaling for the next three models", "confidence": 0.9919962286949158, "text_region": [[1224.0, 802.0], [2165.0, 802.0], [2165.0, 848.0], [1224.0, 848.0]]}, {"text": "which simultaneously scale the model width, the number", "confidence": 0.9904413819313049, "text_region": [[1224.0, 851.0], [2169.0, 851.0], [2169.0, 898.0], [1224.0, 898.0]]}, {"text": "of layers, and the input resolution to obtain models with", "confidence": 0.9922394752502441, "text_region": [[1224.0, 901.0], [2172.0, 901.0], [2172.0, 947.0], [1224.0, 947.0]]}, {"text": "roughly 4x, 16x, and 64x computation.", "confidence": 0.9994064569473267, "text_region": [[1224.0, 950.0], [1853.0, 950.0], [1853.0, 993.0], [1224.0, 993.0]]}], "img_idx": 0, "score": 0.9942231178283691}
{"type": "text", "bbox": [1228, 282, 2165, 596], "res": [{"text": "the ResNet-50 architecture as in the smallest contrastive", "confidence": 0.9980872869491577, "text_region": [[1224.0, 271.0], [2169.0, 274.0], [2169.0, 320.0], [1224.0, 317.0]]}, {"text": "model. To do so, the output from the CNN is projected into", "confidence": 0.9908626675605774, "text_region": [[1224.0, 320.0], [2172.0, 323.0], [2172.0, 370.0], [1224.0, 366.0]]}, {"text": "four tokens, which are then fed as a prefix to a language", "confidence": 0.9944488406181335, "text_region": [[1227.0, 373.0], [2169.0, 373.0], [2169.0, 419.0], [1227.0, 419.0]]}, {"text": "model autoregressively predicting the text tokens. Apart", "confidence": 0.9929069876670837, "text_region": [[1227.0, 422.0], [2175.0, 422.0], [2175.0, 469.0], [1227.0, 469.0]]}, {"text": "from the training objective, the model was trained on the", "confidence": 0.9795941710472107, "text_region": [[1227.0, 469.0], [2169.0, 469.0], [2169.0, 515.0], [1227.0, 515.0]]}, {"text": "same dataset for the same number of epochs as other CLIP", "confidence": 0.9914489388465881, "text_region": [[1227.0, 515.0], [2169.0, 515.0], [2169.0, 561.0], [1227.0, 561.0]]}, {"text": "models.", "confidence": 0.9996267557144165, "text_region": [[1227.0, 561.0], [1357.0, 561.0], [1357.0, 607.0], [1227.0, 607.0]]}], "img_idx": 0, "score": 0.9935405254364014}
{"type": "text", "bbox": [1228, 1047, 2166, 1324], "res": [{"text": "CLIP-ViT We include four CLIP models that use the Vi-", "confidence": 0.9829368591308594, "text_region": [[1227.0, 1046.0], [2172.0, 1046.0], [2172.0, 1089.0], [1227.0, 1089.0]]}, {"text": " sion Transformer (Dosovitskiy et al., 2020) architecture as", "confidence": 0.9796380996704102, "text_region": [[1221.0, 1089.0], [2165.0, 1089.0], [2165.0, 1135.0], [1221.0, 1135.0]]}, {"text": " the image encoder. We include three models trained on 224-", "confidence": 0.9749129414558411, "text_region": [[1214.0, 1135.0], [2178.0, 1132.0], [2179.0, 1188.0], [1214.0, 1191.0]]}, {"text": "by-224 pixel images: ViT-B/32, ViT-B/16, ViT-L/14, and", "confidence": 0.997750461101532, "text_region": [[1224.0, 1188.0], [2172.0, 1188.0], [2172.0, 1234.0], [1224.0, 1234.0]]}, {"text": "the ViT-L/14 model fine-tuned on 336-by-336 pixel input", "confidence": 0.9867091774940491, "text_region": [[1221.0, 1234.0], [2172.0, 1238.0], [2172.0, 1284.0], [1221.0, 1280.0]]}, {"text": "images.", "confidence": 0.9999001622200012, "text_region": [[1223.0, 1283.0], [1358.0, 1291.0], [1355.0, 1337.0], [1220.0, 1329.0]]}], "img_idx": 0, "score": 0.9924190640449524}
{"type": "text", "bbox": [1229, 2204, 2167, 2480], "res": [{"text": "Vision Transformer (ViT) We also include four ViT", "confidence": 0.9704633355140686, "text_region": [[1231.0, 2201.0], [2169.0, 2201.0], [2169.0, 2244.0], [1231.0, 2244.0]]}, {"text": "(Dosovitskiy et al., 2020) checkpoints pretrained on the", "confidence": 0.9978955984115601, "text_region": [[1224.0, 2251.0], [2169.0, 2251.0], [2169.0, 2297.0], [1224.0, 2297.0]]}, {"text": "ImageNet-21k dataset, namely ViT-B/32, ViT-B/16, ViT-", "confidence": 0.9965148568153381, "text_region": [[1224.0, 2297.0], [2179.0, 2297.0], [2179.0, 2343.0], [1224.0, 2343.0]]}, {"text": "L/16, and ViT-H/14. We note that their best-performing", "confidence": 0.9896911978721619, "text_region": [[1221.0, 2343.0], [2172.0, 2346.0], [2172.0, 2393.0], [1221.0, 2389.0]]}, {"text": " models, trained on the JFT-300M dataset, are not available", "confidence": 0.9764425158500671, "text_region": [[1221.0, 2389.0], [2172.0, 2393.0], [2172.0, 2439.0], [1221.0, 2435.0]]}, {"text": "publicly.", "confidence": 0.9998055696487427, "text_region": [[1220.0, 2439.0], [1368.0, 2431.0], [1371.0, 2481.0], [1223.0, 2489.0]]}], "img_idx": 0, "score": 0.991058349609375}
{"type": "text", "bbox": [220, 628, 1161, 1659], "res": [{"text": "We use the 12 datasets from the well-studied evaluation", "confidence": 0.9993219971656799, "text_region": [[220.0, 624.0], [1164.0, 624.0], [1164.0, 667.0], [220.0, 667.0]]}, {"text": "suite introduced by (Kornblith et al., 2019) and add 15", "confidence": 0.9851647019386292, "text_region": [[213.0, 670.0], [1164.0, 667.0], [1164.0, 713.0], [213.0, 716.0]]}, {"text": "additional datasets in order to assess the performance of", "confidence": 0.9975264668464661, "text_region": [[216.0, 719.0], [1164.0, 719.0], [1164.0, 766.0], [216.0, 766.0]]}, {"text": "models on a wider variety of distributions and tasks. These", "confidence": 0.9925738573074341, "text_region": [[216.0, 769.0], [1164.0, 769.0], [1164.0, 812.0], [216.0, 812.0]]}, {"text": "datasets include MNIST, the Facial Expression Recognition", "confidence": 0.9987469911575317, "text_region": [[220.0, 818.0], [1164.0, 818.0], [1164.0, 861.0], [220.0, 861.0]]}, {"text": "2013 dataset (Goodfellow et al., 2015), STL-10 (Coates", "confidence": 0.9943526983261108, "text_region": [[220.0, 861.0], [1161.0, 861.0], [1161.0, 908.0], [220.0, 908.0]]}, {"text": "et al., 2011), EuroSAT (Helber et al., 2019), the NWPU-", "confidence": 0.9745681285858154, "text_region": [[220.0, 911.0], [1161.0, 911.0], [1161.0, 954.0], [220.0, 954.0]]}, {"text": "RESISC45 dataset (Cheng et al., 2017), the German Traf-", "confidence": 0.9919302463531494, "text_region": [[216.0, 954.0], [1171.0, 954.0], [1171.0, 1010.0], [216.0, 1010.0]]}, {"text": "fic Sign Recognition Benchmark (GTSRB) dataset (Stal-", "confidence": 0.9819239377975464, "text_region": [[216.0, 1007.0], [1167.0, 1007.0], [1167.0, 1053.0], [216.0, 1053.0]]}, {"text": "lkamp et al., 2011), the KITTI dataset (Geiger et al., 2012),", "confidence": 0.9868310689926147, "text_region": [[220.0, 1053.0], [1167.0, 1053.0], [1167.0, 1099.0], [220.0, 1099.0]]}, {"text": "PatchCamelyon (Veeling et al., 2018), the UCF101 action", "confidence": 0.9853612184524536, "text_region": [[220.0, 1102.0], [1167.0, 1102.0], [1167.0, 1148.0], [220.0, 1148.0]]}, {"text": "recognition dataset (Soomro et al., 2012), Kinetics 700 (Car-", "confidence": 0.9911388754844666, "text_region": [[216.0, 1148.0], [1167.0, 1145.0], [1168.0, 1191.0], [216.0, 1195.0]]}, {"text": "reira et al., 2019), 2,500 random samples of the CLEVR", "confidence": 0.9808594584465027, "text_region": [[220.0, 1198.0], [1164.0, 1198.0], [1164.0, 1244.0], [220.0, 1244.0]]}, {"text": "dataset (Johnson et al., 2017), the Hateful Memes dataset", "confidence": 0.9890322685241699, "text_region": [[216.0, 1244.0], [1164.0, 1244.0], [1164.0, 1290.0], [216.0, 1290.0]]}, {"text": "(Kiela et al., 2020), and the ImageNet-1k dataset (Deng", "confidence": 0.9956814646720886, "text_region": [[216.0, 1294.0], [1164.0, 1294.0], [1164.0, 1340.0], [216.0, 1340.0]]}, {"text": "et al., 2012). For the two video datasets (UCF101 and Ki-", "confidence": 0.9867569208145142, "text_region": [[216.0, 1340.0], [1167.0, 1340.0], [1167.0, 1386.0], [216.0, 1386.0]]}, {"text": "netics700), we use the middle frame of each video clip as", "confidence": 0.9854164123535156, "text_region": [[220.0, 1389.0], [1164.0, 1389.0], [1164.0, 1436.0], [220.0, 1436.0]]}, {"text": "the input image. STL-10 and UCF101 have multiple pre-", "confidence": 0.9832417964935303, "text_region": [[213.0, 1429.0], [1171.0, 1432.0], [1171.0, 1488.0], [213.0, 1485.0]]}, {"text": "defined train/validation/test splits, 10 and 3 respectively, and", "confidence": 0.985180675983429, "text_region": [[216.0, 1485.0], [1164.0, 1485.0], [1164.0, 1531.0], [216.0, 1531.0]]}, {"text": "we report the average over all splits. Details on each dataset", "confidence": 0.9903731346130371, "text_region": [[216.0, 1534.0], [1164.0, 1534.0], [1164.0, 1581.0], [216.0, 1581.0]]}, {"text": "and the corresponding evaluation metrics are provided in", "confidence": 0.9977293014526367, "text_region": [[216.0, 1581.0], [1164.0, 1581.0], [1164.0, 1627.0], [216.0, 1627.0]]}, {"text": "Table 9.", "confidence": 0.9995226860046387, "text_region": [[216.0, 1627.0], [353.0, 1627.0], [353.0, 1673.0], [216.0, 1673.0]]}], "img_idx": 0, "score": 0.9898959994316101}
{"type": "text", "bbox": [1227, 1385, 2169, 1662], "res": [{"text": "EfficietNet We use the nine models (BO-B8) from the", "confidence": 0.9700711369514465, "text_region": [[1227.0, 1383.0], [2169.0, 1383.0], [2169.0, 1426.0], [1227.0, 1426.0]]}, {"text": "original EfficientNet paper (Tan & Le, 2019), as well as", "confidence": 0.9756922721862793, "text_region": [[1224.0, 1432.0], [2172.0, 1432.0], [2172.0, 1478.0], [1224.0, 1478.0]]}, {"text": "the noisy-student variants (B0-B7, L2-475, and L2-800)", "confidence": 0.9923319816589355, "text_region": [[1224.0, 1478.0], [2172.0, 1478.0], [2172.0, 1521.0], [1224.0, 1521.0]]}, {"text": "(Tan & Le, 2019). The largest models (L2-475 and L2-800)", "confidence": 0.9868878722190857, "text_region": [[1227.0, 1525.0], [2172.0, 1525.0], [2172.0, 1571.0], [1227.0, 1571.0]]}, {"text": "take the input resolutions of 475x475 and 800x800 pixels,", "confidence": 0.991847574710846, "text_region": [[1224.0, 1574.0], [2172.0, 1574.0], [2172.0, 1620.0], [1224.0, 1620.0]]}, {"text": "respectively.", "confidence": 0.9998003840446472, "text_region": [[1220.0, 1624.0], [1426.0, 1616.0], [1427.0, 1663.0], [1222.0, 1670.0]]}], "img_idx": 0, "score": 0.9888988733291626}
{"type": "text", "bbox": [1228, 2542, 2167, 2717], "res": [{"text": "SimCLRv2  The SimCLRv2 (Chen et al., 2020c) project", "confidence": 0.970808207988739, "text_region": [[1221.0, 2534.0], [2172.0, 2538.0], [2172.0, 2584.0], [1221.0, 2581.0]]}, {"text": "released pre-trained and fine-tuned models in various set-", "confidence": 0.9989590644836426, "text_region": [[1224.0, 2587.0], [2172.0, 2587.0], [2172.0, 2633.0], [1224.0, 2633.0]]}, {"text": "tings. We use the seven pretrain-only checkpoints with", "confidence": 0.9947490692138672, "text_region": [[1221.0, 2630.0], [2172.0, 2630.0], [2172.0, 2686.0], [1221.0, 2686.0]]}, {"text": "selective kernels.", "confidence": 0.9882395267486572, "text_region": [[1227.0, 2686.0], [1503.0, 2686.0], [1503.0, 2729.0], [1227.0, 2729.0]]}], "img_idx": 0, "score": 0.9873033165931702}
{"type": "text", "bbox": [220, 2157, 1159, 2480], "res": [{"text": "The Rendered SST2 dataset is designed to measure the opti", "confidence": 0.9968661069869995, "text_region": [[216.0, 2152.0], [1161.0, 2155.0], [1161.0, 2201.0], [216.0, 2198.0]]}, {"text": "cal character recognition capability of visual representations.", "confidence": 0.9913163781166077, "text_region": [[220.0, 2201.0], [1164.0, 2201.0], [1164.0, 2247.0], [220.0, 2247.0]]}, {"text": "To do so, we used the sentences from the Stanford Sentiment", "confidence": 0.9939274191856384, "text_region": [[216.0, 2251.0], [1164.0, 2251.0], [1164.0, 2297.0], [216.0, 2297.0]]}, {"text": "Treebank dataset (Socher et al., 2013) and rendered them", "confidence": 0.997696042060852, "text_region": [[216.0, 2297.0], [1164.0, 2297.0], [1164.0, 2343.0], [216.0, 2343.0]]}, {"text": "into images, with black texts on a white background, in a", "confidence": 0.996081531047821, "text_region": [[220.0, 2343.0], [1161.0, 2343.0], [1161.0, 2389.0], [220.0, 2389.0]]}, {"text": "448 \u00d7448 resolution. Two example images from this dataset", "confidence": 0.9853138327598572, "text_region": [[213.0, 2389.0], [1164.0, 2393.0], [1164.0, 2439.0], [213.0, 2435.0]]}, {"text": " are shown in Figure 19.", "confidence": 0.9710246920585632, "text_region": [[213.0, 2439.0], [595.0, 2439.0], [595.0, 2485.0], [213.0, 2485.0]]}], "img_idx": 0, "score": 0.9860360622406006}
{"type": "text", "bbox": [1228, 1725, 2167, 1902], "res": [{"text": "Instagram-pretrained ResNeXt We use the four models", "confidence": 0.981447160243988, "text_region": [[1224.0, 1719.0], [2172.0, 1719.0], [2172.0, 1766.0], [1224.0, 1766.0]]}, {"text": "(32x8d, 32x16d, 32x32d, 32x48d) released by (Mahajan", "confidence": 0.9968140721321106, "text_region": [[1221.0, 1762.0], [2172.0, 1766.0], [2172.0, 1812.0], [1221.0, 1808.0]]}, {"text": "et al., 2018), as well as their two FixRes variants which use", "confidence": 0.986423909664154, "text_region": [[1224.0, 1815.0], [2172.0, 1815.0], [2172.0, 1861.0], [1224.0, 1861.0]]}, {"text": "higher input resolutions (Touvron et al., 2019).", "confidence": 0.982313334941864, "text_region": [[1224.0, 1864.0], [1979.0, 1864.0], [1979.0, 1907.0], [1224.0, 1907.0]]}], "img_idx": 0, "score": 0.9824336171150208}
{"type": "text", "bbox": [1228, 1963, 2167, 2141], "res": [{"text": "Big Transfer (BiT) We use BiT-S and BiT-M models", "confidence": 0.9876351952552795, "text_region": [[1227.0, 1960.0], [2169.0, 1960.0], [2169.0, 2003.0], [1227.0, 2003.0]]}, {"text": "(Kolesnikov et al., 2019), trained on the ImageNet-1k and", "confidence": 0.9932013154029846, "text_region": [[1224.0, 2010.0], [2172.0, 2010.0], [2172.0, 2053.0], [1224.0, 2053.0]]}, {"text": "ImageNet-21k datasets. The model weights for BiT-L is not ", "confidence": 0.9899494647979736, "text_region": [[1224.0, 2056.0], [2175.0, 2056.0], [2175.0, 2102.0], [1224.0, 2102.0]]}, {"text": "publicly available.", "confidence": 0.999870777130127, "text_region": [[1224.0, 2102.0], [1523.0, 2102.0], [1523.0, 2148.0], [1224.0, 2148.0]]}], "img_idx": 0, "score": 0.9810189008712769}
{"type": "text", "bbox": [1229, 2783, 2165, 2867], "res": [{"text": "BYOL", "confidence": 0.9998272657394409, "text_region": [[1227.0, 2782.0], [1347.0, 2782.0], [1347.0, 2818.0], [1227.0, 2818.0]]}, {"text": "We use the recently released model weights of", "confidence": 0.9893419742584229, "text_region": [[1344.0, 2779.0], [2169.0, 2779.0], [2169.0, 2825.0], [1344.0, 2825.0]]}, {"text": " BYOL (Grill et al., 2020), specifically their 50x1 and 200x2", "confidence": 0.981415331363678, "text_region": [[1221.0, 2828.0], [2169.0, 2828.0], [2169.0, 2874.0], [1221.0, 2874.0]]}], "img_idx": 0, "score": 0.977557361125946}
{"type": "text", "bbox": [226, 360, 1155, 485], "res": [{"text": "We provide additional details for linear probe experiments", "confidence": 0.9881142973899841, "text_region": [[213.0, 346.0], [1164.0, 350.0], [1164.0, 406.0], [213.0, 403.0]]}, {"text": "presented in this paper, including the list of the datasets and", "confidence": 0.9952705502510071, "text_region": [[216.0, 403.0], [1164.0, 403.0], [1164.0, 449.0], [216.0, 449.0]]}, {"text": "models used for evaluation.", "confidence": 0.9859513640403748, "text_region": [[216.0, 452.0], [665.0, 452.0], [665.0, 495.0], [216.0, 495.0]]}], "img_idx": 0, "score": 0.9744295477867126}
{"type": "text", "bbox": [222, 2782, 1159, 2866], "res": [{"text": "LM RN5o  This is a multimodal model that uses an au-", "confidence": 0.9623403549194336, "text_region": [[220.0, 2779.0], [1167.0, 2779.0], [1167.0, 2825.0], [220.0, 2825.0]]}, {"text": "toregressive loss instead of a contrastive loss, while using", "confidence": 0.9914950132369995, "text_region": [[216.0, 2831.0], [1164.0, 2831.0], [1164.0, 2874.0], [216.0, 2874.0]]}], "img_idx": 0, "score": 0.9704324007034302}
{"type": "text", "bbox": [221, 2620, 1155, 2706], "res": [{"text": "In combination with the datasets listed above, we evaluate", "confidence": 0.9986901879310608, "text_region": [[216.0, 2617.0], [1164.0, 2617.0], [1164.0, 2663.0], [216.0, 2663.0]]}, {"text": "the following series of models using linear probes.", "confidence": 0.9996333122253418, "text_region": [[216.0, 2666.0], [1031.0, 2666.0], [1031.0, 2713.0], [216.0, 2713.0]]}], "img_idx": 0, "score": 0.954578697681427}
{"type": "text", "bbox": [220, 1704, 1162, 2123], "res": [{"text": "Additionally, we created two datasets that we call Coun-", "confidence": 0.9886611104011536, "text_region": [[220.0, 1696.0], [1161.0, 1696.0], [1161.0, 1742.0], [220.0, 1742.0]]}, {"text": "try211 and Rendered SST2. The Country211 dataset is", "confidence": 0.9951947331428528, "text_region": [[220.0, 1749.0], [1164.0, 1749.0], [1164.0, 1792.0], [220.0, 1792.0]]}, {"text": " designed to assess the geolocation capability of visual rep-", "confidence": 0.9862145781517029, "text_region": [[210.0, 1789.0], [1171.0, 1792.0], [1171.0, 1848.0], [209.0, 1845.0]]}, {"text": "resentations. We filtered the YFCC100m dataset (Thomee", "confidence": 0.9889481067657471, "text_region": [[216.0, 1845.0], [1161.0, 1845.0], [1161.0, 1888.0], [216.0, 1888.0]]}, {"text": "et al., 2016) to find 211 countries (defined as having an", "confidence": 0.9786142706871033, "text_region": [[213.0, 1884.0], [1164.0, 1888.0], [1164.0, 1944.0], [213.0, 1940.0]]}, {"text": "ISO-3166 country code) that have at least 300 photos with", "confidence": 0.9971345067024231, "text_region": [[220.0, 1937.0], [1161.0, 1937.0], [1161.0, 1983.0], [220.0, 1983.0]]}, {"text": "GPS coordinates, and we built a balanced dataset with 211", "confidence": 0.9886258840560913, "text_region": [[216.0, 1983.0], [1161.0, 1983.0], [1161.0, 2030.0], [216.0, 2030.0]]}, {"text": "categories, by sampling 200 photos for training and 100", "confidence": 0.9858028888702393, "text_region": [[216.0, 2036.0], [1167.0, 2036.0], [1167.0, 2082.0], [216.0, 2082.0]]}, {"text": "photos for testing, for each country.", "confidence": 0.9858460426330566, "text_region": [[216.0, 2082.0], [792.0, 2086.0], [791.0, 2132.0], [216.0, 2128.0]]}], "img_idx": 0, "score": 0.9468392133712769}
{"type": "title", "bbox": [220, 276, 775, 315], "res": [{"text": "A. Linear-probe evaluation", "confidence": 0.9957093596458435, "text_region": [[216.0, 264.0], [775.0, 264.0], [775.0, 320.0], [216.0, 320.0]]}], "img_idx": 0, "score": 0.9562079906463623}
{"type": "title", "bbox": [221, 554, 442, 583], "res": [{"text": "A.1. Datasets", "confidence": 0.9608966112136841, "text_region": [[223.0, 554.0], [446.0, 554.0], [446.0, 591.0], [223.0, 591.0]]}], "img_idx": 0, "score": 0.9408477544784546}
{"type": "title", "bbox": [223, 2544, 424, 2575], "res": [{"text": "A.2. Models", "confidence": 0.9968379735946655, "text_region": [[220.0, 2544.0], [422.0, 2544.0], [422.0, 2581.0], [220.0, 2581.0]]}], "img_idx": 0, "score": 0.936467170715332}
{"type": "header", "bbox": [2129, 192, 2162, 218], "res": [{"text": "37", "confidence": 0.9998747110366821, "text_region": [[2125.0, 188.0], [2172.0, 188.0], [2172.0, 228.0], [2125.0, 228.0]]}], "img_idx": 0, "score": 0.9085198044776917}
{"type": "header", "bbox": [292, 193, 1189, 224], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9977167248725891, "text_region": [[220.0, 188.0], [1370.0, 188.0], [1370.0, 231.0], [220.0, 231.0]]}], "img_idx": 0, "score": 0.7135758399963379}
