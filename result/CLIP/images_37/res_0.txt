{"type": "text", "bbox": [218, 2064, 1163, 2866], "res": [{"text": "We use image features taken from the penultimate layer of", "confidence": 0.984584629535675, "text_region": [[213.0, 2059.0], [1168.0, 2063.0], [1167.0, 2109.0], [213.0, 2105.0]]}, {"text": "each model, ignoring any classification layer provided. For", "confidence": 0.9979968070983887, "text_region": [[216.0, 2112.0], [1164.0, 2112.0], [1164.0, 2158.0], [216.0, 2158.0]]}, {"text": "CLIP-ViT models, we used the features before the linear", "confidence": 0.9992402791976929, "text_region": [[216.0, 2158.0], [1161.0, 2158.0], [1161.0, 2204.0], [216.0, 2204.0]]}, {"text": "projection to the embedding space, which corresponds to", "confidence": 0.9995116591453552, "text_region": [[220.0, 2204.0], [1161.0, 2204.0], [1161.0, 2251.0], [220.0, 2251.0]]}, {"text": "I-f in Figure 3. We train a logistic regression classifier", "confidence": 0.984686553478241, "text_region": [[223.0, 2254.0], [1161.0, 2254.0], [1161.0, 2300.0], [223.0, 2300.0]]}, {"text": "using scikit-learn's L-BFGS implementation, with maxi-", "confidence": 0.9923491477966309, "text_region": [[213.0, 2297.0], [1171.0, 2293.0], [1171.0, 2350.0], [213.0, 2353.0]]}, {"text": "mum 1,oo0 iterations, and report the corresponding met-", "confidence": 0.9775071144104004, "text_region": [[220.0, 2350.0], [1167.0, 2350.0], [1167.0, 2396.0], [220.0, 2396.0]]}, {"text": "ric for each dataset. We determine the L2 regularization", "confidence": 0.9915657043457031, "text_region": [[220.0, 2396.0], [1164.0, 2396.0], [1164.0, 2442.0], [220.0, 2442.0]]}, {"text": "strength X using a hyperparameter sweep on the validation", "confidence": 0.9931305050849915, "text_region": [[216.0, 2445.0], [1164.0, 2445.0], [1164.0, 2492.0], [216.0, 2492.0]]}, {"text": "sets over the range between 10-6 and 106, with 96 log-", "confidence": 0.9862727522850037, "text_region": [[210.0, 2482.0], [1171.0, 2482.0], [1171.0, 2548.0], [210.0, 2548.0]]}, {"text": "arithmically spaced steps. To save compute required for", "confidence": 0.9907371401786804, "text_region": [[216.0, 2541.0], [1161.0, 2541.0], [1161.0, 2587.0], [216.0, 2587.0]]}, {"text": "the sweeps, we perform a parametric binary search that", "confidence": 0.9826737642288208, "text_region": [[216.0, 2590.0], [1164.0, 2590.0], [1164.0, 2637.0], [216.0, 2637.0]]}, {"text": "starts with \u5165 = [10-6, 10-4, 10-2, 1, 102, 104, 10%] and it-", "confidence": 0.957351565361023, "text_region": [[209.0, 2630.0], [1167.0, 2627.0], [1168.0, 2683.0], [210.0, 2686.0]]}, {"text": " eratively halves the interval around the peak until it reaches", "confidence": 0.9904754161834717, "text_region": [[213.0, 2676.0], [1168.0, 2680.0], [1167.0, 2736.0], [213.0, 2732.0]]}, {"text": "a resolution of 8 steps per decade. The hyperparameter", "confidence": 0.9921998381614685, "text_region": [[213.0, 2729.0], [1164.0, 2729.0], [1164.0, 2785.0], [213.0, 2785.0]]}, {"text": " sweeps are performed on a validation split of each dataset.", "confidence": 0.9950512647628784, "text_region": [[209.0, 2775.0], [1167.0, 2772.0], [1168.0, 2828.0], [210.0, 2831.0]]}, {"text": "For the datasets that contain a validation split in addition to", "confidence": 0.9918409585952759, "text_region": [[216.0, 2828.0], [1164.0, 2828.0], [1164.0, 2874.0], [216.0, 2874.0]]}], "img_idx": 0, "score": 0.9947868585586548}
{"type": "text", "bbox": [1227, 1705, 2170, 2214], "res": [{"text": " The individual linear probe scores are provided in Table 10", "confidence": 0.9907664060592651, "text_region": [[1221.0, 1696.0], [2169.0, 1696.0], [2169.0, 1742.0], [1221.0, 1742.0]]}, {"text": " and plotted in Figure 20. The best-performing CLIP model, ", "confidence": 0.9712229371070862, "text_region": [[1217.0, 1742.0], [2178.0, 1739.0], [2179.0, 1795.0], [1217.0, 1799.0]]}, {"text": "using ViT-L/14 archiecture and 336-by-336 pixel images,", "confidence": 0.9919343590736389, "text_region": [[1224.0, 1795.0], [2179.0, 1795.0], [2179.0, 1841.0], [1224.0, 1841.0]]}, {"text": "achieved the state of the art in 21 of the 27 datasets, i.e.", "confidence": 0.9902592897415161, "text_region": [[1224.0, 1845.0], [2172.0, 1845.0], [2172.0, 1888.0], [1224.0, 1888.0]]}, {"text": "included in the Clopper-Pearson 99.5% confidence interval", "confidence": 0.9833438992500305, "text_region": [[1224.0, 1891.0], [2172.0, 1891.0], [2172.0, 1937.0], [1224.0, 1937.0]]}, {"text": "around each dataset's top score. For many datasets, CLIP", "confidence": 0.9983463287353516, "text_region": [[1224.0, 1940.0], [2169.0, 1940.0], [2169.0, 1987.0], [1224.0, 1987.0]]}, {"text": " performs significantly better than other models, demonstrat-", "confidence": 0.9736326336860657, "text_region": [[1221.0, 1983.0], [2172.0, 1980.0], [2172.0, 2036.0], [1221.0, 2039.0]]}, {"text": "ing the advantage of natural language supervision over tradi-", "confidence": 0.9945629239082336, "text_region": [[1224.0, 2036.0], [2179.0, 2036.0], [2179.0, 2082.0], [1224.0, 2082.0]]}, {"text": "tional pre-training approaches based on image classification.", "confidence": 0.9918947219848633, "text_region": [[1224.0, 2086.0], [2175.0, 2086.0], [2175.0, 2132.0], [1224.0, 2132.0]]}, {"text": "See Section 3.2 for more discussions on the linear probe", "confidence": 0.9805381894111633, "text_region": [[1224.0, 2132.0], [2169.0, 2132.0], [2169.0, 2178.0], [1224.0, 2178.0]]}, {"text": "results.", "confidence": 0.9714900851249695, "text_region": [[1220.0, 2182.0], [1345.0, 2174.0], [1348.0, 2220.0], [1223.0, 2228.0]]}], "img_idx": 0, "score": 0.9938523769378662}
{"type": "text", "bbox": [1227, 1244, 2166, 1565], "res": [{"text": "a test split, we use the provided validation set to perform", "confidence": 0.999363362789154, "text_region": [[1224.0, 1238.0], [2172.0, 1238.0], [2172.0, 1284.0], [1224.0, 1284.0]]}, {"text": "the hyperparameter search, and for the datasets that do not", "confidence": 0.9826631546020508, "text_region": [[1221.0, 1280.0], [2175.0, 1280.0], [2175.0, 1336.0], [1221.0, 1336.0]]}, {"text": "provide a validation split or have not published labels for", "confidence": 0.9933709502220154, "text_region": [[1224.0, 1333.0], [2172.0, 1333.0], [2172.0, 1379.0], [1224.0, 1379.0]]}, {"text": "the test data, we split the training dataset to perform the", "confidence": 0.9852582812309265, "text_region": [[1224.0, 1383.0], [2169.0, 1383.0], [2169.0, 1429.0], [1224.0, 1429.0]]}, {"text": "hyperparameter search. For the final result, we combine the", "confidence": 0.9986777901649475, "text_region": [[1224.0, 1429.0], [2169.0, 1429.0], [2169.0, 1475.0], [1224.0, 1475.0]]}, {"text": "validation split back with the training split and report the", "confidence": 0.9820501804351807, "text_region": [[1221.0, 1468.0], [2172.0, 1475.0], [2172.0, 1528.0], [1221.0, 1521.0]]}, {"text": "performance on the unused split.", "confidence": 0.9840061664581299, "text_region": [[1224.0, 1525.0], [1750.0, 1525.0], [1750.0, 1571.0], [1224.0, 1571.0]]}], "img_idx": 0, "score": 0.9914512038230896}
{"type": "text", "bbox": [220, 1546, 1161, 1724], "res": [{"text": "VirTex  We use the pretrained model of VirTex (Desai &", "confidence": 0.9736053347587585, "text_region": [[213.0, 1534.0], [1168.0, 1538.0], [1167.0, 1594.0], [213.0, 1591.0]]}, {"text": "Johnson, 2020). We note that VirTex has a similar model", "confidence": 0.9864547848701477, "text_region": [[216.0, 1591.0], [1164.0, 1591.0], [1164.0, 1637.0], [216.0, 1637.0]]}, {"text": "design to CLIP-AR but is trained on a 1000x smaller dataset ", "confidence": 0.9679499268531799, "text_region": [[216.0, 1640.0], [1167.0, 1640.0], [1167.0, 1686.0], [216.0, 1686.0]]}, {"text": "of high-quality captions from MSCOCO.", "confidence": 0.9908210039138794, "text_region": [[216.0, 1686.0], [878.0, 1686.0], [878.0, 1732.0], [216.0, 1732.0]]}], "img_idx": 0, "score": 0.9828174710273743}
{"type": "text", "bbox": [219, 1797, 1159, 1924], "res": [{"text": "ResNet We add the original ResNet checkpoints released", "confidence": 0.9912078380584717, "text_region": [[216.0, 1792.0], [1161.0, 1792.0], [1161.0, 1838.0], [216.0, 1838.0]]}, {"text": "by (He et al., 2016b), namely ResNet-50, ResNet-101, and ", "confidence": 0.9896500706672668, "text_region": [[216.0, 1838.0], [1167.0, 1838.0], [1167.0, 1884.0], [216.0, 1884.0]]}, {"text": "ResNet152", "confidence": 0.9991390705108643, "text_region": [[216.0, 1888.0], [402.0, 1888.0], [402.0, 1924.0], [216.0, 1924.0]]}], "img_idx": 0, "score": 0.9812373518943787}
{"type": "text", "bbox": [219, 1345, 1161, 1478], "res": [{"text": "Momentum Contrast (MoCo) We include the MoCo-v1", "confidence": 0.9768869280815125, "text_region": [[216.0, 1343.0], [1157.0, 1343.0], [1157.0, 1386.0], [216.0, 1386.0]]}, {"text": "(He et al., 2020) and the MoCo-v2 (Chen et al., 2020d)", "confidence": 0.9811404943466187, "text_region": [[216.0, 1389.0], [1167.0, 1389.0], [1167.0, 1432.0], [216.0, 1432.0]]}, {"text": "checkpoints.", "confidence": 0.9998270869255066, "text_region": [[223.0, 1442.0], [422.0, 1442.0], [422.0, 1478.0], [223.0, 1478.0]]}], "img_idx": 0, "score": 0.9772955775260925}
{"type": "title", "bbox": [223, 1992, 485, 2022], "res": [{"text": "A.3. Evaluation", "confidence": 0.9935832619667053, "text_region": [[220.0, 1987.0], [492.0, 1987.0], [492.0, 2033.0], [220.0, 2033.0]]}], "img_idx": 0, "score": 0.9443272352218628}
{"type": "title", "bbox": [219, 1244, 419, 1278], "res": [{"text": "checkpoints.", "confidence": 0.9998667240142822, "text_region": [[216.0, 1238.0], [422.0, 1238.0], [422.0, 1284.0], [216.0, 1284.0]]}], "img_idx": 0, "score": 0.8574624061584473}
{"type": "title", "bbox": [1236, 1630, 1426, 1660], "res": [{"text": "A.4. Results", "confidence": 0.9651870131492615, "text_region": [[1224.0, 1630.0], [1430.0, 1630.0], [1430.0, 1667.0], [1224.0, 1667.0]]}], "img_idx": 0, "score": 0.8463736176490784}
{"type": "figure", "bbox": [340, 261, 1049, 1093], "res": [{"text": "Montias ... pumps a lot of energy into his", "confidence": 0.9731996655464172, "text_region": [[373.0, 558.0], [1001.0, 558.0], [1001.0, 604.0], [373.0, 604.0]]}, {"text": "nicelynuancednarrativeandsurrounds", "confidence": 0.9973006248474121, "text_region": [[373.0, 594.0], [981.0, 594.0], [981.0, 627.0], [373.0, 627.0]]}, {"text": "himself with a cast of quirky --but not", "confidence": 0.9681375026702881, "text_region": [[369.0, 624.0], [971.0, 624.0], [971.0, 667.0], [369.0, 667.0]]}, {"text": "stereotyped -- street characters.", "confidence": 0.9937213659286499, "text_region": [[366.0, 657.0], [875.0, 657.0], [875.0, 700.0], [366.0, 700.0]]}], "img_idx": 0, "score": 0.8414232730865479}
{"type": "figure", "bbox": [1234, 426, 2016, 1088], "res": [{"text": "It's clear the filmmakers weren't sure where", "confidence": 0.9509101510047913, "text_region": [[1264.0, 558.0], [1956.0, 558.0], [1956.0, 601.0], [1264.0, 601.0]]}, {"text": "they wanted their story to.go, and even more", "confidence": 0.9624789953231812, "text_region": [[1264.0, 591.0], [1969.0, 591.0], [1969.0, 634.0], [1264.0, 634.0]]}, {"text": "clear that they lack the skilsto get us to", "confidence": 0.9377952814102173, "text_region": [[1258.0, 610.0], [1906.0, 617.0], [1905.0, 673.0], [1257.0, 666.0]]}, {"text": "this undetermined destination.", "confidence": 0.976227343082428, "text_region": [[1261.0, 650.0], [1746.0, 653.0], [1746.0, 700.0], [1260.0, 696.0]]}], "img_idx": 0, "score": 0.5283737182617188}
{"type": "figure_caption", "bbox": [722, 1118, 1662, 1150], "res": [{"text": "Figure 19. Two example images from the Rendered SST2 dataset", "confidence": 0.9929386377334595, "text_region": [[725.0, 1112.0], [1670.0, 1112.0], [1670.0, 1155.0], [725.0, 1155.0]]}], "img_idx": 0, "score": 0.8993247151374817}
{"type": "header", "bbox": [2130, 192, 2164, 218], "res": [{"text": "38", "confidence": 0.9995905160903931, "text_region": [[2125.0, 191.0], [2165.0, 191.0], [2165.0, 221.0], [2125.0, 221.0]]}], "img_idx": 0, "score": 0.9027175903320312}
{"type": "header", "bbox": [485, 194, 1367, 224], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9977167248725891, "text_region": [[220.0, 188.0], [1370.0, 188.0], [1370.0, 231.0], [220.0, 231.0]]}], "img_idx": 0, "score": 0.6767966747283936}
