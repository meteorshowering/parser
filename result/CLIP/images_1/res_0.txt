{"type": "text", "bbox": [1227, 1699, 2169, 2832], "res": [{"text": "A crucial difference between these weakly supervised mod-", "confidence": 0.9986265897750854, "text_region": [[1227.0, 1693.0], [2175.0, 1693.0], [2175.0, 1739.0], [1227.0, 1739.0]]}, {"text": "els and recent explorations of learning image representations", "confidence": 0.9928800463676453, "text_region": [[1227.0, 1742.0], [2172.0, 1742.0], [2172.0, 1789.0], [1227.0, 1789.0]]}, {"text": "directly from natural language is scale. While Mahajan et al.", "confidence": 0.991533100605011, "text_region": [[1224.0, 1785.0], [2179.0, 1785.0], [2179.0, 1841.0], [1224.0, 1841.0]]}, {"text": "(2018) and Kolesnikov et al. (2019) trained their models for", "confidence": 0.9983486533164978, "text_region": [[1227.0, 1838.0], [2172.0, 1838.0], [2172.0, 1881.0], [1227.0, 1881.0]]}, {"text": " accelerator years on millions to billions of images, VirTex,", "confidence": 0.9960227012634277, "text_region": [[1221.0, 1881.0], [2169.0, 1881.0], [2169.0, 1927.0], [1221.0, 1927.0]]}, {"text": "ICMLM, and ConVIRT trained for accelerator days on one", "confidence": 0.9983486533164978, "text_region": [[1224.0, 1934.0], [2169.0, 1934.0], [2169.0, 1977.0], [1224.0, 1977.0]]}, {"text": "to two hundred thousand images. In this work, we close", "confidence": 0.9989902973175049, "text_region": [[1221.0, 1977.0], [2172.0, 1977.0], [2172.0, 2033.0], [1221.0, 2033.0]]}, {"text": " this gap and study the behaviors of image classifiers trained", "confidence": 0.988738477230072, "text_region": [[1217.0, 2020.0], [2172.0, 2023.0], [2172.0, 2079.0], [1217.0, 2076.0]]}, {"text": "with natural language supervision at large scale. Enabled", "confidence": 0.9868243932723999, "text_region": [[1221.0, 2072.0], [2175.0, 2072.0], [2175.0, 2128.0], [1221.0, 2128.0]]}, {"text": "by the large amounts of publicly available data of this form", "confidence": 0.9822656512260437, "text_region": [[1224.0, 2122.0], [2169.0, 2122.0], [2169.0, 2168.0], [1224.0, 2168.0]]}, {"text": "on the internet, we create a new dataset of 400 million (im-", "confidence": 0.9791514873504639, "text_region": [[1224.0, 2171.0], [2172.0, 2171.0], [2172.0, 2218.0], [1224.0, 2218.0]]}, {"text": "age, text) pairs and demonstrate that a simplified version of", "confidence": 0.9894000291824341, "text_region": [[1221.0, 2218.0], [2175.0, 2211.0], [2175.0, 2267.0], [1221.0, 2274.0]]}, {"text": "ConVIRT trained from scratch, which we call CLIP, for Con-", "confidence": 0.9809747338294983, "text_region": [[1227.0, 2267.0], [2172.0, 2267.0], [2172.0, 2310.0], [1227.0, 2310.0]]}, {"text": "trastive Language-Image Pre-training, is an efficient method", "confidence": 0.9996563792228699, "text_region": [[1227.0, 2317.0], [2172.0, 2317.0], [2172.0, 2363.0], [1227.0, 2363.0]]}, {"text": "of learning from natural language supervision. We study", "confidence": 0.9933831095695496, "text_region": [[1224.0, 2363.0], [2169.0, 2363.0], [2169.0, 2409.0], [1224.0, 2409.0]]}, {"text": "the scalability of CLIP by training a series of eight models", "confidence": 0.9965541958808899, "text_region": [[1224.0, 2409.0], [2169.0, 2409.0], [2169.0, 2455.0], [1224.0, 2455.0]]}, {"text": "spanning almost 2 orders of magnitude of compute and ob-", "confidence": 0.9985384345054626, "text_region": [[1224.0, 2458.0], [2175.0, 2458.0], [2175.0, 2505.0], [1224.0, 2505.0]]}, {"text": "serve that transfer performance is a smoothly predictable", "confidence": 0.9954378008842468, "text_region": [[1227.0, 2508.0], [2169.0, 2508.0], [2169.0, 2554.0], [1227.0, 2554.0]]}, {"text": "function of compute (Hestness et al., 2017; Kaplan et al.,", "confidence": 0.988664448261261, "text_region": [[1224.0, 2554.0], [2172.0, 2554.0], [2172.0, 2600.0], [1224.0, 2600.0]]}, {"text": "2020). We find that CLIP, similar to the GPT family, learns", "confidence": 0.9914465546607971, "text_region": [[1227.0, 2600.0], [2169.0, 2600.0], [2169.0, 2647.0], [1227.0, 2647.0]]}, {"text": "to perform a wide set of tasks during pre-training including", "confidence": 0.9988529682159424, "text_region": [[1221.0, 2647.0], [2175.0, 2647.0], [2175.0, 2703.0], [1221.0, 2703.0]]}, {"text": " OCR, geo-localization, action recognition, and many others.", "confidence": 0.9819357395172119, "text_region": [[1221.0, 2693.0], [2179.0, 2693.0], [2179.0, 2749.0], [1221.0, 2749.0]]}, {"text": "We measure this by benchmarking the zero-shot transfer", "confidence": 0.9923678040504456, "text_region": [[1224.0, 2742.0], [2169.0, 2746.0], [2169.0, 2792.0], [1224.0, 2788.0]]}, {"text": "performance of CLIP on over 30 existing datasets and find", "confidence": 0.9927427768707275, "text_region": [[1224.0, 2795.0], [2168.0, 2788.0], [2169.0, 2835.0], [1224.0, 2841.0]]}], "img_idx": 0, "score": 0.9965357184410095}
{"type": "text", "bbox": [219, 1292, 1162, 1709], "res": [{"text": "classification datasets by scoring target classes based on", "confidence": 0.9916648268699646, "text_region": [[220.0, 1287.0], [1161.0, 1287.0], [1161.0, 1333.0], [220.0, 1333.0]]}, {"text": "their dictionary of learned visual n-grams and predicting the", "confidence": 0.9947693347930908, "text_region": [[220.0, 1333.0], [1161.0, 1333.0], [1161.0, 1379.0], [220.0, 1379.0]]}, {"text": " one with the highest score. Adopting more recent architec-", "confidence": 0.9833613634109497, "text_region": [[216.0, 1383.0], [1161.0, 1383.0], [1161.0, 1429.0], [216.0, 1429.0]]}, {"text": " tures and pre-training approaches, VirTex (Desai & Johnson,", "confidence": 0.9817060828208923, "text_region": [[209.0, 1426.0], [1167.0, 1422.0], [1168.0, 1478.0], [210.0, 1482.0]]}, {"text": "2020), ICMLM (Bulent Sariyildiz et al., 2020), and Con-", "confidence": 0.995611310005188, "text_region": [[220.0, 1478.0], [1167.0, 1478.0], [1167.0, 1521.0], [220.0, 1521.0]]}, {"text": "VIRT (Zhang et al., 2020) have recently demonstrated the", "confidence": 0.9996508359909058, "text_region": [[216.0, 1521.0], [1164.0, 1521.0], [1164.0, 1577.0], [216.0, 1577.0]]}, {"text": "potential of transformer-based language modeling, masked", "confidence": 0.9912007451057434, "text_region": [[220.0, 1574.0], [1164.0, 1574.0], [1164.0, 1620.0], [220.0, 1620.0]]}, {"text": "language modeling, and contrastive objectives to learn im-", "confidence": 0.9974403381347656, "text_region": [[213.0, 1617.0], [1171.0, 1614.0], [1171.0, 1670.0], [213.0, 1673.0]]}, {"text": " age representations from text.", "confidence": 0.98710036277771, "text_region": [[213.0, 1667.0], [702.0, 1663.0], [702.0, 1719.0], [213.0, 1723.0]]}], "img_idx": 0, "score": 0.9905017614364624}
{"type": "text", "bbox": [1227, 1292, 2169, 1661], "res": [{"text": "mises. Both works carefully design, and in the process limit,", "confidence": 0.9838019013404846, "text_region": [[1227.0, 1287.0], [2172.0, 1287.0], [2172.0, 1333.0], [1227.0, 1333.0]]}, {"text": "their supervision to 1000 and 18291 classes respectively.", "confidence": 0.9842892289161682, "text_region": [[1224.0, 1333.0], [2169.0, 1333.0], [2169.0, 1379.0], [1224.0, 1379.0]]}, {"text": "Natural language is able to express, and therefore supervise,", "confidence": 0.9849435687065125, "text_region": [[1224.0, 1383.0], [2172.0, 1383.0], [2172.0, 1429.0], [1224.0, 1429.0]]}, {"text": "a much wider set of visual concepts through its general-", "confidence": 0.9855338335037231, "text_region": [[1217.0, 1422.0], [2182.0, 1426.0], [2182.0, 1482.0], [1217.0, 1478.0]]}, {"text": "ity. Both approaches also use static softmax classifiers to", "confidence": 0.9967514276504517, "text_region": [[1224.0, 1478.0], [2172.0, 1478.0], [2172.0, 1525.0], [1224.0, 1525.0]]}, {"text": " perform prediction and lack a mechanism for dynamic out-", "confidence": 0.9931820034980774, "text_region": [[1221.0, 1521.0], [2175.0, 1518.0], [2175.0, 1574.0], [1221.0, 1577.0]]}, {"text": "puts. This severely curtails their flexibility and limits their", "confidence": 0.9827852249145508, "text_region": [[1227.0, 1574.0], [2172.0, 1574.0], [2172.0, 1620.0], [1227.0, 1620.0]]}, {"text": "\"zero-shot'\u2019 capabilities.", "confidence": 0.951500654220581, "text_region": [[1221.0, 1610.0], [1613.0, 1614.0], [1613.0, 1670.0], [1220.0, 1666.0]]}], "img_idx": 0, "score": 0.9902606010437012}
{"type": "text", "bbox": [219, 1745, 1161, 2848], "res": [{"text": "While exciting as proofs of concept, using natural language", "confidence": 0.9974225759506226, "text_region": [[220.0, 1742.0], [1164.0, 1742.0], [1164.0, 1789.0], [220.0, 1789.0]]}, {"text": "supervision for image representation learning is still rare.", "confidence": 0.9922240972518921, "text_region": [[216.0, 1785.0], [1164.0, 1785.0], [1164.0, 1841.0], [216.0, 1841.0]]}, {"text": "This is likely because demonstrated performance on com-", "confidence": 0.9956157803535461, "text_region": [[220.0, 1838.0], [1164.0, 1838.0], [1164.0, 1881.0], [220.0, 1881.0]]}, {"text": "mon benchmarks is much lower than alternative approaches.", "confidence": 0.9933018088340759, "text_region": [[213.0, 1878.0], [1168.0, 1881.0], [1167.0, 1937.0], [213.0, 1934.0]]}, {"text": "For example, Li et al. (2017) reach only 11.5% accuracy", "confidence": 0.9864161610603333, "text_region": [[213.0, 1927.0], [1161.0, 1934.0], [1161.0, 1980.0], [213.0, 1973.0]]}, {"text": "on ImageNet in a zero-shot setting. This is well below the ", "confidence": 0.9838763475418091, "text_region": [[216.0, 1980.0], [1167.0, 1980.0], [1167.0, 2026.0], [216.0, 2026.0]]}, {"text": " 88.4% accuracy of the current state of the art (Xie et al.,", "confidence": 0.9600156545639038, "text_region": [[210.0, 2020.0], [1168.0, 2023.0], [1167.0, 2079.0], [209.0, 2076.0]]}, {"text": "2020). It is even below the 50% accuracy of classic com-", "confidence": 0.9939366579055786, "text_region": [[220.0, 2076.0], [1167.0, 2076.0], [1167.0, 2122.0], [220.0, 2122.0]]}, {"text": " puter vision approaches (Deng et al., 2012). Instead, more", "confidence": 0.9952547550201416, "text_region": [[209.0, 2119.0], [1167.0, 2115.0], [1168.0, 2171.0], [210.0, 2175.0]]}, {"text": "narrowly scoped but well-targeted uses of weak supervision", "confidence": 0.998754620552063, "text_region": [[220.0, 2171.0], [1161.0, 2171.0], [1161.0, 2218.0], [220.0, 2218.0]]}, {"text": "have improved performance. Mahajan et al. (2018) showed", "confidence": 0.9910310506820679, "text_region": [[213.0, 2211.0], [1168.0, 2214.0], [1167.0, 2270.0], [213.0, 2267.0]]}, {"text": "that predicting ImageNet-related hashtags on Instagram im-", "confidence": 0.9987909197807312, "text_region": [[220.0, 2270.0], [1164.0, 2270.0], [1164.0, 2317.0], [220.0, 2317.0]]}, {"text": " ages is an effective pre-training task. When fine-tuned to", "confidence": 0.9758318066596985, "text_region": [[213.0, 2310.0], [1164.0, 2307.0], [1164.0, 2363.0], [213.0, 2366.0]]}, {"text": "ImageNet these pre-trained models increased accuracy by", "confidence": 0.9993740916252136, "text_region": [[220.0, 2360.0], [1157.0, 2360.0], [1157.0, 2406.0], [220.0, 2406.0]]}, {"text": "over 5% and improved the overall state of the art at the time.", "confidence": 0.9977760910987854, "text_region": [[220.0, 2409.0], [1164.0, 2409.0], [1164.0, 2455.0], [220.0, 2455.0]]}, {"text": "Kolesnikov et al. (2019) and Dosovitskiy et al. (2020) have", "confidence": 0.976875364780426, "text_region": [[220.0, 2458.0], [1164.0, 2458.0], [1164.0, 2505.0], [220.0, 2505.0]]}, {"text": "also demonstrated large gains on a broader set of transfer", "confidence": 0.9989758133888245, "text_region": [[213.0, 2498.0], [1168.0, 2501.0], [1167.0, 2558.0], [213.0, 2554.0]]}, {"text": "benchmarks by pre-training models to predict the classes of", "confidence": 0.999697744846344, "text_region": [[220.0, 2554.0], [1167.0, 2554.0], [1167.0, 2600.0], [220.0, 2600.0]]}, {"text": "the noisily labeled JFT-300M dataset.", "confidence": 0.9853813648223877, "text_region": [[220.0, 2600.0], [822.0, 2600.0], [822.0, 2647.0], [220.0, 2647.0]]}, {"text": "This line of work represents the current pragmatic middle", "confidence": 0.992605447769165, "text_region": [[220.0, 2673.0], [1164.0, 2673.0], [1164.0, 2719.0], [220.0, 2719.0]]}, {"text": "ground between learning from a limited amount of super-", "confidence": 0.9960190057754517, "text_region": [[220.0, 2722.0], [1164.0, 2722.0], [1164.0, 2769.0], [220.0, 2769.0]]}, {"text": "vised \u201cgold-labels\u201d and learning from practically unlimited", "confidence": 0.9862167239189148, "text_region": [[213.0, 2762.0], [1164.0, 2765.0], [1164.0, 2822.0], [213.0, 2818.0]]}, {"text": "amounts of raw text. However, it is not without compro-", "confidence": 0.9985916614532471, "text_region": [[220.0, 2818.0], [1161.0, 2818.0], [1161.0, 2864.0], [220.0, 2864.0]]}], "img_idx": 0, "score": 0.9888650178909302}
{"type": "text", "bbox": [220, 1018, 2181, 1179], "res": [{"text": "Figure 1. Summary of our approach. While standard image models jointly train an image feature extractor and a linear classifier to predict", "confidence": 0.9966143369674683, "text_region": [[220.0, 1010.0], [2172.0, 1010.0], [2172.0, 1056.0], [220.0, 1056.0]]}, {"text": "some label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training", "confidence": 0.9961947202682495, "text_region": [[220.0, 1056.0], [2172.0, 1056.0], [2172.0, 1099.0], [220.0, 1099.0]]}, {"text": "examples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the", "confidence": 0.9943363666534424, "text_region": [[220.0, 1096.0], [2172.0, 1096.0], [2172.0, 1142.0], [220.0, 1142.0]]}, {"text": "target dataset's classes.", "confidence": 0.9772089123725891, "text_region": [[216.0, 1142.0], [555.0, 1142.0], [555.0, 1185.0], [216.0, 1185.0]]}], "img_idx": 0, "score": 0.9644824266433716}
{"type": "text", "bbox": [1210, 274, 1720, 302], "res": [{"text": "(2) Create dataset classifier from label text", "confidence": 0.9816595911979675, "text_region": [[1214.0, 267.0], [1726.0, 267.0], [1726.0, 310.0], [1214.0, 310.0]]}], "img_idx": 0, "score": 0.90357905626297}
{"type": "text", "bbox": [225, 276, 548, 304], "res": [{"text": "(1) Contrastive pre-training", "confidence": 0.974700391292572, "text_region": [[216.0, 264.0], [556.0, 267.0], [555.0, 314.0], [216.0, 310.0]]}], "img_idx": 0, "score": 0.8341462016105652}
{"type": "figure", "bbox": [212, 311, 2183, 974], "res": [{"text": "(1) Contrastive pre-training", "confidence": 0.974700391292572, "text_region": [[216.0, 264.0], [556.0, 267.0], [555.0, 314.0], [216.0, 310.0]]}, {"text": "Text", "confidence": 0.9994959831237793, "text_region": [[542.0, 426.0], [585.0, 426.0], [585.0, 452.0], [542.0, 452.0]]}, {"text": "aussie pup", "confidence": 0.9968148469924927, "text_region": [[256.0, 439.0], [386.0, 439.0], [386.0, 472.0], [256.0, 472.0]]}, {"text": "Text", "confidence": 0.9960975050926208, "text_region": [[1690.0, 436.0], [1733.0, 436.0], [1733.0, 459.0], [1690.0, 459.0]]}, {"text": "Encoder", "confidence": 0.9976881742477417, "text_region": [[526.0, 455.0], [602.0, 455.0], [602.0, 479.0], [526.0, 479.0]]}, {"text": "Encode", "confidence": 0.9951505661010742, "text_region": [[1683.0, 462.0], [1746.0, 462.0], [1746.0, 485.0], [1683.0, 485.0]]}, {"text": "(3) Use for :", "confidence": 0.9298754334449768, "text_region": [[1217.0, 634.0], [1350.0, 634.0], [1350.0, 667.0], [1217.0, 667.0]]}, {"text": "prediction", "confidence": 0.9962517619132996, "text_region": [[1460.0, 637.0], [1583.0, 637.0], [1583.0, 660.0], [1460.0, 660.0]]}, {"text": "2.T", "confidence": 0.7562409043312073, "text_region": [[1118.0, 690.0], [1154.0, 690.0], [1154.0, 713.0], [1118.0, 713.0]]}, {"text": "I3\u00b7T", "confidence": 0.82015460729599, "text_region": [[1114.0, 756.0], [1161.0, 756.0], [1161.0, 782.0], [1114.0, 782.0]]}, {"text": "IN\u00b7T", "confidence": 0.8449831008911133, "text_region": [[1114.0, 904.0], [1161.0, 904.0], [1161.0, 931.0], [1114.0, 931.0]]}], "img_idx": 0, "score": 0.9152885675430298}
{"type": "header", "bbox": [228, 194, 1124, 224], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9977167248725891, "text_region": [[220.0, 188.0], [1370.0, 188.0], [1370.0, 231.0], [220.0, 231.0]]}], "img_idx": 0, "score": 0.7008461952209473}
{"type": "header", "bbox": [2145, 191, 2163, 221], "res": [], "img_idx": 0, "score": 0.5884207487106323}
