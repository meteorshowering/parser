{"type": "text", "bbox": [218, 1685, 1159, 2866], "res": [{"text": "While we have extensively analyzed the task-learning ca-", "confidence": 0.9997512102127075, "text_region": [[210.0, 1673.0], [1168.0, 1676.0], [1167.0, 1733.0], [209.0, 1729.0]]}, {"text": " pabilities of CLIP through zero-shot transfer in the previ-", "confidence": 0.9802770018577576, "text_region": [[213.0, 1719.0], [1171.0, 1723.0], [1171.0, 1779.0], [213.0, 1775.0]]}, {"text": "ous section, it is more common to study the representation", "confidence": 0.9889151453971863, "text_region": [[220.0, 1775.0], [1164.0, 1775.0], [1164.0, 1822.0], [220.0, 1822.0]]}, {"text": "learning capabilities of a model. There exist many ways to", "confidence": 0.993712842464447, "text_region": [[216.0, 1818.0], [1167.0, 1818.0], [1167.0, 1874.0], [216.0, 1874.0]]}, {"text": "evaluate the quality of representations as well as disagree-", "confidence": 0.985421359539032, "text_region": [[220.0, 1874.0], [1164.0, 1874.0], [1164.0, 1917.0], [220.0, 1917.0]]}, {"text": "ments over what properties an \u201cideal\u2019 representation should", "confidence": 0.9792646765708923, "text_region": [[220.0, 1921.0], [1164.0, 1921.0], [1164.0, 1967.0], [220.0, 1967.0]]}, {"text": "have (Locatello et al., 2020). Fitting a linear classifier on", "confidence": 0.9913761615753174, "text_region": [[220.0, 1967.0], [1164.0, 1967.0], [1164.0, 2013.0], [220.0, 2013.0]]}, {"text": "a representation extracted from the model and measuring", "confidence": 0.9860661029815674, "text_region": [[213.0, 2010.0], [1167.0, 2010.0], [1167.0, 2066.0], [213.0, 2066.0]]}, {"text": "its performance on various datasets is a common approach.", "confidence": 0.9813449382781982, "text_region": [[216.0, 2066.0], [1164.0, 2066.0], [1164.0, 2109.0], [216.0, 2109.0]]}, {"text": "An alternative is measuring the performance of end-to-end", "confidence": 0.9931892156600952, "text_region": [[220.0, 2112.0], [1161.0, 2112.0], [1161.0, 2158.0], [220.0, 2158.0]]}, {"text": "fine-tuning of the model. This increases flexibility, and", "confidence": 0.9804838299751282, "text_region": [[220.0, 2158.0], [1164.0, 2158.0], [1164.0, 2204.0], [220.0, 2204.0]]}, {"text": "prior work has convincingly demonstrated that fine-tuning", "confidence": 0.9754257798194885, "text_region": [[216.0, 2201.0], [1167.0, 2201.0], [1167.0, 2257.0], [216.0, 2257.0]]}, {"text": "outperforms linear classification on most image classifi-", "confidence": 0.9901353716850281, "text_region": [[220.0, 2254.0], [1161.0, 2254.0], [1161.0, 2300.0], [220.0, 2300.0]]}, {"text": "cation datasets (Kornblith et al., 2019; Zhai et al., 2019).", "confidence": 0.9887006282806396, "text_region": [[216.0, 2300.0], [1167.0, 2300.0], [1167.0, 2346.0], [216.0, 2346.0]]}, {"text": "While the high performance of fine-tuning motivates its", "confidence": 0.9898267984390259, "text_region": [[216.0, 2346.0], [1161.0, 2346.0], [1161.0, 2392.0], [216.0, 2392.0]]}, {"text": "study for practical reasons, we still opt for linear classifier", "confidence": 0.9833073616027832, "text_region": [[216.0, 2393.0], [1161.0, 2389.0], [1161.0, 2445.0], [216.0, 2449.0]]}, {"text": "based evaluation for several reasons. Our work is focused", "confidence": 0.9978541731834412, "text_region": [[220.0, 2445.0], [1161.0, 2445.0], [1161.0, 2488.0], [220.0, 2488.0]]}, {"text": " on developing a high-performing task and dataset-agnostic", "confidence": 0.992263674736023, "text_region": [[216.0, 2495.0], [1161.0, 2495.0], [1161.0, 2541.0], [216.0, 2541.0]]}, {"text": "pre-training approach. Fine-tuning, because it adapts rep-", "confidence": 0.9939778447151184, "text_region": [[216.0, 2538.0], [1161.0, 2538.0], [1161.0, 2594.0], [216.0, 2594.0]]}, {"text": "resentations to each dataset during the fine-tuning phase,", "confidence": 0.9940495491027832, "text_region": [[213.0, 2581.0], [1171.0, 2584.0], [1171.0, 2640.0], [213.0, 2637.0]]}, {"text": "can compensate for and potentially mask failures to learn", "confidence": 0.9978702068328857, "text_region": [[220.0, 2640.0], [1164.0, 2640.0], [1164.0, 2683.0], [220.0, 2683.0]]}, {"text": "general and robust representations during the pre-training", "confidence": 0.9987555146217346, "text_region": [[220.0, 2686.0], [1164.0, 2686.0], [1164.0, 2732.0], [220.0, 2732.0]]}, {"text": "phase. Linear classifiers, because of their limited fexibility,", "confidence": 0.995252788066864, "text_region": [[220.0, 2732.0], [1167.0, 2732.0], [1167.0, 2779.0], [220.0, 2779.0]]}, {"text": "instead highlight these failures and provide clear feedback", "confidence": 0.9996119141578674, "text_region": [[223.0, 2779.0], [1161.0, 2779.0], [1161.0, 2825.0], [223.0, 2825.0]]}, {"text": "during development. For CLIP, training supervised linear", "confidence": 0.9994584321975708, "text_region": [[220.0, 2828.0], [1164.0, 2828.0], [1164.0, 2874.0], [220.0, 2874.0]]}], "img_idx": 0, "score": 0.9947947859764099}
{"type": "text", "bbox": [1227, 2143, 2167, 2848], "res": [{"text": "As Figure 21 qualitatively shows, CLIP models learn a wider", "confidence": 0.9941189289093018, "text_region": [[1224.0, 2135.0], [2172.0, 2138.0], [2172.0, 2185.0], [1224.0, 2181.0]]}, {"text": "set of tasks than has previously been demonstrated in a sin-", "confidence": 0.9968252778053284, "text_region": [[1224.0, 2188.0], [2175.0, 2188.0], [2175.0, 2234.0], [1224.0, 2234.0]]}, {"text": " gle computer vision model trained end-to-end from random", "confidence": 0.9827086925506592, "text_region": [[1224.0, 2237.0], [2169.0, 2237.0], [2169.0, 2284.0], [1224.0, 2284.0]]}, {"text": " initialization. These tasks include geo-localization, optical", "confidence": 0.9857069253921509, "text_region": [[1221.0, 2280.0], [2169.0, 2280.0], [2169.0, 2326.0], [1221.0, 2326.0]]}, {"text": " character recognition, facial emotion recognition, and action", "confidence": 0.9762236475944519, "text_region": [[1221.0, 2323.0], [2175.0, 2327.0], [2175.0, 2383.0], [1221.0, 2379.0]]}, {"text": "recognition. None of these tasks are measured in the evalua-", "confidence": 0.9954673647880554, "text_region": [[1224.0, 2379.0], [2175.0, 2379.0], [2175.0, 2426.0], [1224.0, 2426.0]]}, {"text": "tion suite of Kornblith et al. (2019). This could be argued", "confidence": 0.9780133366584778, "text_region": [[1224.0, 2425.0], [2172.0, 2429.0], [2172.0, 2475.0], [1224.0, 2472.0]]}, {"text": "to be a form of selection bias in Kornblith et al. (2019)'s", "confidence": 0.980095624923706, "text_region": [[1224.0, 2475.0], [2169.0, 2475.0], [2169.0, 2521.0], [1224.0, 2521.0]]}, {"text": "study towards tasks that overlap with ImageNet. To address", "confidence": 0.9894251227378845, "text_region": [[1224.0, 2524.0], [2165.0, 2524.0], [2165.0, 2571.0], [1224.0, 2571.0]]}, {"text": "this, we also measure performance on a broader 27 dataset", "confidence": 0.9948904514312744, "text_region": [[1224.0, 2571.0], [2172.0, 2571.0], [2172.0, 2617.0], [1224.0, 2617.0]]}, {"text": "evaluation suite. This evaluation suite, detailed in Appendix", "confidence": 0.9992772340774536, "text_region": [[1227.0, 2620.0], [2172.0, 2620.0], [2172.0, 2666.0], [1227.0, 2666.0]]}, {"text": "A includes datasets representing the aforementioned tasks,", "confidence": 0.9998739957809448, "text_region": [[1224.0, 2670.0], [2172.0, 2670.0], [2172.0, 2716.0], [1224.0, 2716.0]]}, {"text": "German Traffic Signs Recognition Benchmark (Stallkamp", "confidence": 0.9831095933914185, "text_region": [[1224.0, 2716.0], [2169.0, 2716.0], [2169.0, 2762.0], [1224.0, 2762.0]]}, {"text": "et al., 2011), as well as several other datasets adapted from", "confidence": 0.9854587912559509, "text_region": [[1224.0, 2762.0], [2172.0, 2762.0], [2172.0, 2808.0], [1224.0, 2808.0]]}, {"text": "VTAB (Zhai et al., 2019).", "confidence": 0.9734596014022827, "text_region": [[1224.0, 2808.0], [1636.0, 2808.0], [1636.0, 2854.0], [1224.0, 2854.0]]}], "img_idx": 0, "score": 0.9946156740188599}
{"type": "text", "bbox": [1227, 970, 2168, 2108], "res": [{"text": "Figure 10 summarizes our findings. To minimize selection", "confidence": 0.9912948608398438, "text_region": [[1224.0, 967.0], [2172.0, 967.0], [2172.0, 1013.0], [1224.0, 1013.0]]}, {"text": "effects that could raise concerns of confirmation or reporting", "confidence": 0.9811387658119202, "text_region": [[1224.0, 1013.0], [2169.0, 1016.0], [2168.0, 1063.0], [1224.0, 1059.0]]}, {"text": "bias, we first study performance on the 12 dataset evaluation", "confidence": 0.9965174198150635, "text_region": [[1221.0, 1059.0], [2175.0, 1059.0], [2175.0, 1115.0], [1221.0, 1115.0]]}, {"text": " suite from Kornblith et al. (2019). While small CLIP mod-", "confidence": 0.9801092743873596, "text_region": [[1221.0, 1112.0], [2175.0, 1109.0], [2175.0, 1155.0], [1221.0, 1158.0]]}, {"text": "els such as a ResNet-50 and ResNet-101 outperform other", "confidence": 0.9937285780906677, "text_region": [[1227.0, 1162.0], [2172.0, 1162.0], [2172.0, 1208.0], [1227.0, 1208.0]]}, {"text": "ResNets trained on ImageNet-1K (BiT-S and the originals),", "confidence": 0.9804495573043823, "text_region": [[1224.0, 1208.0], [2172.0, 1208.0], [2172.0, 1254.0], [1224.0, 1254.0]]}, {"text": "they underperform ResNets trained on ImageNet-21K (BiT-", "confidence": 0.9842004179954529, "text_region": [[1227.0, 1254.0], [2175.0, 1254.0], [2175.0, 1300.0], [1227.0, 1300.0]]}, {"text": "M). These small CLIP models also underperform models", "confidence": 0.9971513748168945, "text_region": [[1227.0, 1304.0], [2169.0, 1304.0], [2169.0, 1350.0], [1227.0, 1350.0]]}, {"text": "in the EfficientNet family with similar compute require-", "confidence": 0.9837347865104675, "text_region": [[1221.0, 1350.0], [2172.0, 1353.0], [2172.0, 1399.0], [1221.0, 1396.0]]}, {"text": "ments. However, models trained with CLIP scale very well", "confidence": 0.9984984993934631, "text_region": [[1227.0, 1399.0], [2172.0, 1399.0], [2172.0, 1445.0], [1227.0, 1445.0]]}, {"text": "and the largest model we trained (ResNet-50x64) slightly", "confidence": 0.9907277226448059, "text_region": [[1227.0, 1449.0], [2169.0, 1449.0], [2169.0, 1495.0], [1227.0, 1495.0]]}, {"text": "outperforms the best performing existing model (a Noisy", "confidence": 0.981705904006958, "text_region": [[1227.0, 1495.0], [2169.0, 1495.0], [2169.0, 1541.0], [1227.0, 1541.0]]}, {"text": " Student EffcientNet-L2) on both overall score and compute", "confidence": 0.9837724566459656, "text_region": [[1221.0, 1538.0], [2172.0, 1541.0], [2172.0, 1587.0], [1221.0, 1584.0]]}, {"text": "efficiency. We also find that CLIP vision transformers are", "confidence": 0.9910942316055298, "text_region": [[1224.0, 1591.0], [2165.0, 1591.0], [2165.0, 1637.0], [1224.0, 1637.0]]}, {"text": "about 3x more compute efficient than CLIP ResNets, which", "confidence": 0.9983557462692261, "text_region": [[1227.0, 1637.0], [2172.0, 1637.0], [2172.0, 1683.0], [1227.0, 1683.0]]}, {"text": "allows us to reach higher overall performance within our", "confidence": 0.9939783811569214, "text_region": [[1227.0, 1690.0], [2172.0, 1690.0], [2172.0, 1732.0], [1227.0, 1732.0]]}, {"text": "compute budget. These results qualitatively replicate the", "confidence": 0.9973383545875549, "text_region": [[1224.0, 1736.0], [2169.0, 1732.0], [2169.0, 1779.0], [1224.0, 1782.0]]}, {"text": "findings of Dosovitskiy et al. (2020) which reported that", "confidence": 0.9877458810806274, "text_region": [[1224.0, 1782.0], [2175.0, 1782.0], [2175.0, 1828.0], [1224.0, 1828.0]]}, {"text": "vision transformers are more compute efficient than con-", "confidence": 0.9899014234542847, "text_region": [[1224.0, 1832.0], [2179.0, 1832.0], [2179.0, 1878.0], [1224.0, 1878.0]]}, {"text": "vnets when trained on sufficiently large datasets. Our best ", "confidence": 0.9750751852989197, "text_region": [[1224.0, 1878.0], [2175.0, 1878.0], [2175.0, 1924.0], [1224.0, 1924.0]]}, {"text": "overall model is a ViT-L/14 that is fine-tuned at a higher res-", "confidence": 0.9957438707351685, "text_region": [[1224.0, 1924.0], [2179.0, 1924.0], [2179.0, 1970.0], [1224.0, 1970.0]]}, {"text": "olution of 336 pixels on our dataset for 1 additional epoch.", "confidence": 0.9913729429244995, "text_region": [[1224.0, 1973.0], [2175.0, 1973.0], [2175.0, 2020.0], [1224.0, 2020.0]]}, {"text": "This model outperforms the best existing model across this", "confidence": 0.9908676147460938, "text_region": [[1221.0, 2013.0], [2175.0, 2016.0], [2175.0, 2072.0], [1221.0, 2069.0]]}, {"text": "evaluation suite by an average of 2.6%.", "confidence": 0.9960706830024719, "text_region": [[1227.0, 2069.0], [1856.0, 2069.0], [1856.0, 2115.0], [1227.0, 2115.0]]}], "img_idx": 0, "score": 0.9931606650352478}
{"type": "text", "bbox": [220, 934, 1159, 1227], "res": [{"text": "Figure 9. Zero-shot CLIP performance scales smoothly as a", "confidence": 0.9989122152328491, "text_region": [[220.0, 927.0], [1164.0, 927.0], [1164.0, 973.0], [220.0, 973.0]]}, {"text": "function of model compute. Across 39 evals on 36 different", "confidence": 0.9779813289642334, "text_region": [[220.0, 973.0], [1167.0, 973.0], [1167.0, 1016.0], [220.0, 1016.0]]}, {"text": "datasets, average zero-shot error is well modeled by a log-log lin-", "confidence": 0.9892479181289673, "text_region": [[220.0, 1016.0], [1164.0, 1016.0], [1164.0, 1063.0], [220.0, 1063.0]]}, {"text": "ear trend across a 44x range of compute spanning 5 different CLIP", "confidence": 0.9831491708755493, "text_region": [[220.0, 1059.0], [1164.0, 1059.0], [1164.0, 1106.0], [220.0, 1106.0]]}, {"text": "models. Lightly shaded lines are performance on individual evals,", "confidence": 0.9951296448707581, "text_region": [[220.0, 1106.0], [1164.0, 1106.0], [1164.0, 1148.0], [220.0, 1148.0]]}, {"text": "showing that performance is much more varied despite the smooth", "confidence": 0.9826521873474121, "text_region": [[216.0, 1148.0], [1164.0, 1148.0], [1164.0, 1195.0], [216.0, 1195.0]]}, {"text": "overall trend.", "confidence": 0.9845690131187439, "text_region": [[216.0, 1195.0], [412.0, 1195.0], [412.0, 1231.0], [216.0, 1231.0]]}], "img_idx": 0, "score": 0.9915306568145752}
{"type": "text", "bbox": [218, 1317, 1160, 1541], "res": [{"text": "this is caused by high variance between individual training", "confidence": 0.9996227622032166, "text_region": [[216.0, 1313.0], [1161.0, 1313.0], [1161.0, 1360.0], [216.0, 1360.0]]}, {"text": "runs on sub-tasks (as documented in D'Amour et al. (2020))", "confidence": 0.9888293743133545, "text_region": [[216.0, 1360.0], [1164.0, 1356.0], [1164.0, 1402.0], [216.0, 1406.0]]}, {"text": "masking a steadily improving trend or whether performance", "confidence": 0.9830867052078247, "text_region": [[220.0, 1412.0], [1161.0, 1412.0], [1161.0, 1455.0], [220.0, 1455.0]]}, {"text": "is actually non-monotonic as a function of compute on some", "confidence": 0.9879765510559082, "text_region": [[216.0, 1459.0], [1161.0, 1459.0], [1161.0, 1505.0], [216.0, 1505.0]]}, {"text": "tasks.", "confidence": 0.999840497970581, "text_region": [[216.0, 1505.0], [316.0, 1505.0], [316.0, 1551.0], [216.0, 1551.0]]}], "img_idx": 0, "score": 0.9890394806861877}
{"type": "text", "bbox": [1226, 280, 2165, 935], "res": [{"text": "classifiers has the added benefit of being very similar to the", "confidence": 0.9948001503944397, "text_region": [[1224.0, 271.0], [2169.0, 274.0], [2168.0, 320.0], [1224.0, 317.0]]}, {"text": "approach used for its zero-shot classifiers which enables", "confidence": 0.9891252517700195, "text_region": [[1221.0, 320.0], [2172.0, 313.0], [2172.0, 369.0], [1221.0, 376.0]]}, {"text": "extensive comparisons and analysis in Section 3.1. Finally,", "confidence": 0.9838297963142395, "text_region": [[1227.0, 373.0], [2175.0, 373.0], [2175.0, 419.0], [1227.0, 419.0]]}, {"text": "we aim to compare CLIP to a comprehensive set of existing", "confidence": 0.9936888217926025, "text_region": [[1224.0, 419.0], [2172.0, 422.0], [2172.0, 469.0], [1224.0, 465.0]]}, {"text": "models across many tasks. Studying 66 different models on", "confidence": 0.999590277671814, "text_region": [[1227.0, 465.0], [2172.0, 465.0], [2172.0, 511.0], [1227.0, 511.0]]}, {"text": "27 different datasets requires tuning 1782 different evalua-", "confidence": 0.9837585687637329, "text_region": [[1224.0, 511.0], [2165.0, 511.0], [2165.0, 558.0], [1224.0, 558.0]]}, {"text": "tions. Fine-tuning opens up a much larger design and hyper-", "confidence": 0.988091230392456, "text_region": [[1221.0, 561.0], [2172.0, 561.0], [2172.0, 607.0], [1221.0, 607.0]]}, {"text": "parameter space, which makes it difficult to fairly evaluate", "confidence": 0.9993818998336792, "text_region": [[1224.0, 610.0], [2172.0, 610.0], [2172.0, 657.0], [1224.0, 657.0]]}, {"text": " and computationally expensive to compare a diverse set of", "confidence": 0.989192545413971, "text_region": [[1217.0, 653.0], [2175.0, 650.0], [2175.0, 706.0], [1217.0, 710.0]]}, {"text": "techniques as discussed in other large scale empirical studies", "confidence": 0.9989122152328491, "text_region": [[1224.0, 706.0], [2169.0, 706.0], [2169.0, 752.0], [1224.0, 752.0]]}, {"text": "(Lucic et al., 2018; Choi et al., 2019). By comparison, linear", "confidence": 0.9982765316963196, "text_region": [[1224.0, 752.0], [2172.0, 752.0], [2172.0, 799.0], [1224.0, 799.0]]}, {"text": "classifiers require minimal hyper-parameter tuning and have", "confidence": 0.9990623593330383, "text_region": [[1224.0, 802.0], [2172.0, 802.0], [2172.0, 848.0], [1224.0, 848.0]]}, {"text": "standardized implementations and evaluation procedures.", "confidence": 0.992475688457489, "text_region": [[1224.0, 851.0], [2175.0, 851.0], [2175.0, 898.0], [1224.0, 898.0]]}, {"text": "Please see Appendix A for further details on evaluation.", "confidence": 0.9902432560920715, "text_region": [[1224.0, 894.0], [2122.0, 894.0], [2122.0, 940.0], [1224.0, 940.0]]}], "img_idx": 0, "score": 0.9884077310562134}
{"type": "title", "bbox": [219, 1610, 717, 1647], "res": [{"text": " 3.2. Representation Learning", "confidence": 0.9806579947471619, "text_region": [[210.0, 1594.0], [725.0, 1601.0], [725.0, 1657.0], [209.0, 1650.0]]}], "img_idx": 0, "score": 0.9605806469917297}
{"type": "figure", "bbox": [199, 261, 1149, 881], "res": [{"text": "RN50", "confidence": 0.9980420470237732, "text_region": [[303.0, 273.0], [377.0, 282.0], [372.0, 321.0], [298.0, 312.0]]}, {"text": "45", "confidence": 0.9998827576637268, "text_region": [[259.0, 310.0], [296.0, 310.0], [296.0, 340.0], [259.0, 340.0]]}, {"text": "RN101", "confidence": 0.9992048144340515, "text_region": [[402.0, 343.0], [492.0, 343.0], [492.0, 380.0], [402.0, 380.0]]}, {"text": "RN50x4", "confidence": 0.9702851176261902, "text_region": [[572.0, 403.0], [679.0, 403.0], [679.0, 439.0], [572.0, 439.0]]}, {"text": "40", "confidence": 0.9995149374008179, "text_region": [[259.0, 422.0], [289.0, 422.0], [289.0, 452.0], [259.0, 452.0]]}, {"text": "RN50x16", "confidence": 0.9896677136421204, "text_region": [[855.0, 518.0], [961.0, 518.0], [961.0, 554.0], [855.0, 554.0]]}, {"text": "35", "confidence": 0.998315155506134, "text_region": [[264.0, 536.0], [296.0, 555.0], [276.0, 588.0], [244.0, 569.0]]}, {"text": "30", "confidence": 0.9995280504226685, "text_region": [[249.0, 696.0], [299.0, 696.0], [299.0, 736.0], [249.0, 736.0]]}, {"text": "RN50x64", "confidence": 0.9954837560653687, "text_region": [[1008.0, 719.0], [1124.0, 719.0], [1124.0, 756.0], [1008.0, 756.0]]}, {"text": "9.9", "confidence": 0.9998739361763, "text_region": [[379.0, 809.0], [436.0, 809.0], [436.0, 845.0], [379.0, 845.0]]}, {"text": "21.5", "confidence": 0.9997828006744385, "text_region": [[542.0, 805.0], [609.0, 805.0], [609.0, 845.0], [542.0, 845.0]]}, {"text": "75.3", "confidence": 0.9997825622558594, "text_region": [[815.0, 801.0], [886.0, 810.0], [881.0, 849.0], [810.0, 840.0]]}, {"text": "265.9", "confidence": 0.9999673962593079, "text_region": [[1078.0, 809.0], [1161.0, 809.0], [1161.0, 845.0], [1078.0, 845.0]]}, {"text": "6.1", "confidence": 0.9996874928474426, "text_region": [[286.0, 812.0], [329.0, 812.0], [329.0, 838.0], [286.0, 838.0]]}, {"text": "Model GFLOPs", "confidence": 0.9837564826011658, "text_region": [[622.0, 845.0], [805.0, 845.0], [805.0, 881.0], [622.0, 881.0]]}], "img_idx": 0, "score": 0.9529452919960022}
{"type": "header", "bbox": [2129, 193, 2159, 218], "res": [{"text": "11", "confidence": 0.9990428686141968, "text_region": [[2129.0, 191.0], [2165.0, 191.0], [2165.0, 221.0], [2129.0, 221.0]]}], "img_idx": 0, "score": 0.9057307243347168}
{"type": "header", "bbox": [485, 194, 1366, 225], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9977254271507263, "text_region": [[220.0, 188.0], [1370.0, 188.0], [1370.0, 231.0], [220.0, 231.0]]}], "img_idx": 0, "score": 0.6629839539527893}
