{"type": "text", "bbox": [1227, 2212, 2169, 2676], "res": [{"text": "We also tested CLIP's zero-shot performance for \u2018in the", "confidence": 0.9891840219497681, "text_region": [[1221.0, 2201.0], [2169.0, 2204.0], [2169.0, 2251.0], [1221.0, 2247.0]]}, {"text": "wild' identity detection using the CelebA dataset?. We did", "confidence": 0.972534716129303, "text_region": [[1227.0, 2254.0], [2172.0, 2254.0], [2172.0, 2300.0], [1227.0, 2300.0]]}, {"text": " this to evaluate the model's performance for identity detec-", "confidence": 0.9684220552444458, "text_region": [[1217.0, 2293.0], [2179.0, 2297.0], [2178.0, 2353.0], [1217.0, 2350.0]]}, {"text": "tion using just the publicly available data it was pre-trained", "confidence": 0.9863050580024719, "text_region": [[1224.0, 2350.0], [2172.0, 2350.0], [2172.0, 2396.0], [1224.0, 2396.0]]}, {"text": "on. While we tested this on a dataset of celebrities who have", "confidence": 0.9868894219398499, "text_region": [[1224.0, 2396.0], [2172.0, 2396.0], [2172.0, 2442.0], [1224.0, 2442.0]]}, {"text": "a larger number of images on the internet, we hypothesize", "confidence": 0.9982499480247498, "text_region": [[1224.0, 2445.0], [2169.0, 2445.0], [2169.0, 2492.0], [1224.0, 2492.0]]}, {"text": "that the number of images in the pre-training data needed", "confidence": 0.9934977889060974, "text_region": [[1221.0, 2488.0], [2175.0, 2488.0], [2175.0, 2544.0], [1221.0, 2544.0]]}, {"text": "for the model to associate faces with names will keep de-", "confidence": 0.986394464969635, "text_region": [[1224.0, 2541.0], [2172.0, 2541.0], [2172.0, 2587.0], [1224.0, 2587.0]]}, {"text": "creasing as models get more powerful (see Table 8), which", "confidence": 0.9973199367523193, "text_region": [[1227.0, 2590.0], [2172.0, 2590.0], [2172.0, 2637.0], [1227.0, 2637.0]]}, {"text": "has significant societal implications (Garvie, 2019). This", "confidence": 0.9972627758979797, "text_region": [[1221.0, 2637.0], [2169.0, 2633.0], [2169.0, 2680.0], [1221.0, 2683.0]]}], "img_idx": 0, "score": 0.989861011505127}
{"type": "text", "bbox": [1226, 1540, 2167, 1955], "res": [{"text": "the model to choose from. Additionally, we carried out a", "confidence": 0.9905255436897278, "text_region": [[1224.0, 1538.0], [2169.0, 1538.0], [2169.0, 1581.0], [1224.0, 1581.0]]}, {"text": "'stress test' where the class set included at least one more", "confidence": 0.9901530146598816, "text_region": [[1227.0, 1584.0], [2172.0, 1584.0], [2172.0, 1630.0], [1227.0, 1630.0]]}, {"text": "caption for something that was \u2018close\u2019 to the image (for", "confidence": 0.9893786311149597, "text_region": [[1227.0, 1634.0], [2172.0, 1634.0], [2172.0, 1680.0], [1227.0, 1680.0]]}, {"text": "example, \u2018parking lot with white car' vs. \u2018parking lot with", "confidence": 0.9719898700714111, "text_region": [[1227.0, 1683.0], [2169.0, 1683.0], [2169.0, 1726.0], [1227.0, 1726.0]]}, {"text": "red car'). We found that the model had a top-1 accuracy", "confidence": 0.9817661046981812, "text_region": [[1221.0, 1719.0], [2172.0, 1723.0], [2172.0, 1779.0], [1221.0, 1775.0]]}, {"text": "of 91.8% on the CCTV images for the initial evaluation.", "confidence": 0.9921656250953674, "text_region": [[1227.0, 1775.0], [2172.0, 1775.0], [2172.0, 1822.0], [1227.0, 1822.0]]}, {"text": "The accuracy dropped significantly to 51.1% for the second", "confidence": 0.9814507961273193, "text_region": [[1227.0, 1825.0], [2175.0, 1825.0], [2175.0, 1871.0], [1227.0, 1871.0]]}, {"text": "evaluation, with the model incorrectly choosing the \u2018close'", "confidence": 0.9699258208274841, "text_region": [[1227.0, 1874.0], [2169.0, 1874.0], [2169.0, 1917.0], [1227.0, 1917.0]]}, {"text": "answer 40.7% of the time.", "confidence": 0.9969666004180908, "text_region": [[1224.0, 1921.0], [1646.0, 1921.0], [1646.0, 1967.0], [1224.0, 1967.0]]}], "img_idx": 0, "score": 0.9898155927658081}
{"type": "text", "bbox": [220, 1540, 1160, 1766], "res": [{"text": "around such systems. Our inclusion of surveillance is not", "confidence": 0.9848291873931885, "text_region": [[216.0, 1538.0], [1164.0, 1538.0], [1164.0, 1581.0], [216.0, 1581.0]]}, {"text": "intended to indicate enthusiasm for this domain - rather, we", "confidence": 0.9844511151313782, "text_region": [[216.0, 1581.0], [1168.0, 1584.0], [1167.0, 1630.0], [216.0, 1627.0]]}, {"text": "think surveillance is an important domain to try to make", "confidence": 0.9950336813926697, "text_region": [[220.0, 1634.0], [1161.0, 1634.0], [1161.0, 1680.0], [220.0, 1680.0]]}, {"text": "predictions about given its societal implications (Zuboff,", "confidence": 0.9919018745422363, "text_region": [[220.0, 1676.0], [1164.0, 1676.0], [1164.0, 1723.0], [220.0, 1723.0]]}, {"text": "2015; Browne, 2015).", "confidence": 0.9920369982719421, "text_region": [[220.0, 1726.0], [569.0, 1726.0], [569.0, 1772.0], [220.0, 1772.0]]}], "img_idx": 0, "score": 0.9732165932655334}
{"type": "text", "bbox": [1229, 1996, 2168, 2174], "res": [{"text": " For fine-grained detection, the zero-shot model performed", "confidence": 0.9867641925811768, "text_region": [[1221.0, 1987.0], [2172.0, 1990.0], [2172.0, 2036.0], [1221.0, 2033.0]]}, {"text": "poorly, with results near random. Note that this experiment", "confidence": 0.9953634738922119, "text_region": [[1224.0, 2039.0], [2172.0, 2039.0], [2172.0, 2086.0], [1224.0, 2086.0]]}, {"text": "was targeted only towards detecting the presence or absence", "confidence": 0.9878857731819153, "text_region": [[1227.0, 2089.0], [2172.0, 2089.0], [2172.0, 2135.0], [1227.0, 2135.0]]}, {"text": "of small objects in image sequences.", "confidence": 0.9909439086914062, "text_region": [[1224.0, 2132.0], [1819.0, 2135.0], [1819.0, 2181.0], [1224.0, 2178.0]]}], "img_idx": 0, "score": 0.9659092426300049}
{"type": "text", "bbox": [218, 2163, 1160, 2672], "res": [{"text": "Given CLIP's fexible class construction, we tested 515", "confidence": 0.997435450553894, "text_region": [[220.0, 2158.0], [1161.0, 2158.0], [1161.0, 2204.0], [220.0, 2204.0]]}, {"text": "surveillance images captured from 12 different video se-", "confidence": 0.9992262125015259, "text_region": [[220.0, 2208.0], [1164.0, 2208.0], [1164.0, 2254.0], [220.0, 2254.0]]}, {"text": "quences on self-constructed general classes for coarse and", "confidence": 0.999289870262146, "text_region": [[213.0, 2251.0], [1167.0, 2247.0], [1168.0, 2303.0], [213.0, 2307.0]]}, {"text": "fine grained classification. Coarse classification required the", "confidence": 0.9954153299331665, "text_region": [[216.0, 2297.0], [1164.0, 2300.0], [1164.0, 2346.0], [216.0, 2343.0]]}, {"text": "model to correctly identify the main subject of the image (i.e.", "confidence": 0.9900281429290771, "text_region": [[220.0, 2350.0], [1164.0, 2350.0], [1164.0, 2396.0], [220.0, 2396.0]]}, {"text": " determine if the image was a picture of an empty parking", "confidence": 0.9933282732963562, "text_region": [[210.0, 2389.0], [1168.0, 2396.0], [1167.0, 2452.0], [209.0, 2445.0]]}, {"text": "lot, school campus, etc.). For fine-grained classification, the", "confidence": 0.9907073378562927, "text_region": [[220.0, 2445.0], [1164.0, 2445.0], [1164.0, 2492.0], [220.0, 2492.0]]}, {"text": "model had to choose between two options constructed to", "confidence": 0.9962341785430908, "text_region": [[216.0, 2492.0], [1164.0, 2492.0], [1164.0, 2538.0], [216.0, 2538.0]]}, {"text": " determine if the model could identify the presence/absence", "confidence": 0.997489333152771, "text_region": [[210.0, 2534.0], [1168.0, 2538.0], [1167.0, 2594.0], [209.0, 2590.0]]}, {"text": "of smaller features in the image such as a person standing", "confidence": 0.9984503388404846, "text_region": [[216.0, 2590.0], [1164.0, 2590.0], [1164.0, 2637.0], [216.0, 2637.0]]}, {"text": "in the corner.", "confidence": 0.9684059023857117, "text_region": [[214.0, 2633.0], [436.0, 2640.0], [435.0, 2687.0], [212.0, 2679.0]]}], "img_idx": 0, "score": 0.9179916381835938}
{"type": "text", "bbox": [221, 2714, 1160, 2844], "res": [{"text": "For coarse classification, we constructed the classes by hand-", "confidence": 0.9962998628616333, "text_region": [[216.0, 2709.0], [1164.0, 2709.0], [1164.0, 2752.0], [216.0, 2752.0]]}, {"text": " captioning the images ourselves to describe the contents ", "confidence": 0.9718484282493591, "text_region": [[213.0, 2752.0], [1167.0, 2749.0], [1168.0, 2805.0], [213.0, 2808.0]]}, {"text": "of the image and there were always at least 6 options for", "confidence": 0.9980505108833313, "text_region": [[216.0, 2805.0], [1164.0, 2805.0], [1164.0, 2851.0], [216.0, 2851.0]]}], "img_idx": 0, "score": 0.8875250220298767}
{"type": "text", "bbox": [218, 1807, 1160, 2120], "res": [{"text": "We measure the model's performance on classification of", "confidence": 0.9830650091171265, "text_region": [[216.0, 1798.0], [1167.0, 1798.0], [1167.0, 1845.0], [216.0, 1845.0]]}, {"text": "images from CCTV cameras and zero-shot celebrity identifi-", "confidence": 0.995016872882843, "text_region": [[220.0, 1848.0], [1161.0, 1848.0], [1161.0, 1894.0], [220.0, 1894.0]]}, {"text": "cation. We first tested model performance on low-resolution", "confidence": 0.9987114071846008, "text_region": [[220.0, 1894.0], [1161.0, 1894.0], [1161.0, 1940.0], [220.0, 1940.0]]}, {"text": "images captured from surveillance cameras (e.g. CCTV", "confidence": 0.9916846752166748, "text_region": [[220.0, 1944.0], [1161.0, 1944.0], [1161.0, 1990.0], [220.0, 1990.0]]}, {"text": "cameras). We used the VIRAT dataset (Oh et al., 2011) and", "confidence": 0.9979450702667236, "text_region": [[220.0, 1990.0], [1164.0, 1990.0], [1164.0, 2036.0], [220.0, 2036.0]]}, {"text": "data captured by Varadarajan & Odobez (2009), which both", "confidence": 0.9939627647399902, "text_region": [[216.0, 2039.0], [1164.0, 2039.0], [1164.0, 2086.0], [216.0, 2086.0]]}, {"text": " consist of real world outdoor scenes with non-actors.", "confidence": 0.9857832789421082, "text_region": [[213.0, 2086.0], [1064.0, 2089.0], [1064.0, 2135.0], [213.0, 2132.0]]}], "img_idx": 0, "score": 0.8816523551940918}
{"type": "text", "bbox": [325, 1224, 2122, 1394], "res": [{"text": "Figure 18. CLIP performance on Member of Congress images when given the combined returned label set for the images from Google", "confidence": 0.9898982048034668, "text_region": [[220.0, 1221.0], [2165.0, 1221.0], [2165.0, 1267.0], [220.0, 1267.0]]}, {"text": " Cloud Vision, Amazon Rekognition and Microsoft Azure Computer Vision. The 20 most gendered labels for men and women were", "confidence": 0.9830774068832397, "text_region": [[213.0, 1254.0], [2172.0, 1257.0], [2172.0, 1313.0], [213.0, 1310.0]]}, {"text": "identified with x2 tests with the threshold at O.5%. Labels are sorted by absolute frequencies. Bars denote the percentage of images for a", "confidence": 0.9853456020355225, "text_region": [[216.0, 1303.0], [2169.0, 1307.0], [2169.0, 1353.0], [216.0, 1350.0]]}, {"text": " certain label by gender.", "confidence": 0.9920637011528015, "text_region": [[214.0, 1343.0], [559.0, 1350.0], [558.0, 1403.0], [212.0, 1396.0]]}], "img_idx": 0, "score": 0.8192371129989624}
{"type": "text", "bbox": [1639, 285, 1869, 348], "res": [{"text": "Top labels,", "confidence": 0.999028205871582, "text_region": [[1666.0, 280.0], [1843.0, 280.0], [1843.0, 317.0], [1666.0, 317.0]]}, {"text": "images of men", "confidence": 0.9855495691299438, "text_region": [[1627.0, 310.0], [1876.0, 317.0], [1875.0, 363.0], [1626.0, 356.0]]}], "img_idx": 0, "score": 0.543090283870697}
{"type": "figure", "bbox": [293, 365, 2128, 1164], "res": [{"text": "woman", "confidence": 0.9998331069946289, "text_region": [[446.0, 376.0], [552.0, 376.0], [552.0, 409.0], [446.0, 409.0]]}, {"text": "man", "confidence": 0.9999272227287292, "text_region": [[1380.0, 376.0], [1447.0, 376.0], [1447.0, 412.0], [1380.0, 412.0]]}, {"text": "lady", "confidence": 0.9951894879341125, "text_region": [[479.0, 412.0], [545.0, 412.0], [545.0, 449.0], [479.0, 449.0]]}, {"text": "male", "confidence": 0.9997508525848389, "text_region": [[1374.0, 412.0], [1444.0, 412.0], [1444.0, 449.0], [1374.0, 449.0]]}, {"text": "female", "confidence": 0.999889075756073, "text_region": [[446.0, 442.0], [552.0, 442.0], [552.0, 479.0], [446.0, 479.0]]}, {"text": "face", "confidence": 0.9992622137069702, "text_region": [[1378.0, 434.0], [1446.0, 445.0], [1439.0, 484.0], [1372.0, 473.0]]}, {"text": "looking ", "confidence": 0.9633951783180237, "text_region": [[442.0, 479.0], [552.0, 479.0], [552.0, 515.0], [442.0, 515.0]]}, {"text": "player", "confidence": 0.9997603297233582, "text_region": [[1354.0, 479.0], [1447.0, 479.0], [1447.0, 521.0], [1354.0, 521.0]]}, {"text": "senior citizen ", "confidence": 0.9456871747970581, "text_region": [[366.0, 511.0], [552.0, 511.0], [552.0, 548.0], [366.0, 548.0]]}, {"text": "black ", "confidence": 0.9378053545951843, "text_region": [[1364.0, 515.0], [1450.0, 515.0], [1450.0, 551.0], [1364.0, 551.0]]}, {"text": "public speaking ", "confidence": 0.9824573993682861, "text_region": [[329.0, 551.0], [552.0, 551.0], [552.0, 584.0], [329.0, 584.0]]}, {"text": "head -", "confidence": 0.93538898229599, "text_region": [[1367.0, 548.0], [1457.0, 548.0], [1457.0, 584.0], [1367.0, 584.0]]}, {"text": " blonde", "confidence": 0.9320389628410339, "text_region": [[446.0, 584.0], [552.0, 584.0], [552.0, 620.0], [446.0, 620.0]]}, {"text": "facial expression ", "confidence": 0.9946978688240051, "text_region": [[1211.0, 584.0], [1450.0, 584.0], [1450.0, 620.0], [1211.0, 620.0]]}, {"text": "spokesperson", "confidence": 0.9998846054077148, "text_region": [[356.0, 617.0], [549.0, 617.0], [549.0, 653.0], [356.0, 653.0]]}, {"text": "suit -", "confidence": 0.9117353558540344, "text_region": [[1387.0, 620.0], [1457.0, 620.0], [1457.0, 657.0], [1387.0, 657.0]]}, {"text": "blazer", "confidence": 0.999772310256958, "text_region": [[459.0, 653.0], [552.0, 653.0], [552.0, 686.0], [459.0, 686.0]]}, {"text": "photo ", "confidence": 0.9234051704406738, "text_region": [[1360.0, 653.0], [1483.0, 653.0], [1483.0, 686.0], [1360.0, 686.0]]}, {"text": "laughing ", "confidence": 0.9861111044883728, "text_region": [[426.0, 690.0], [552.0, 690.0], [552.0, 726.0], [426.0, 726.0]]}, {"text": "military officer", "confidence": 0.9928538799285889, "text_region": [[1241.0, 690.0], [1447.0, 690.0], [1447.0, 723.0], [1241.0, 723.0]]}, {"text": "hot", "confidence": 0.9992640614509583, "text_region": [[492.0, 719.0], [552.0, 719.0], [552.0, 756.0], [492.0, 756.0]]}, {"text": "walking -", "confidence": 0.9452031254768372, "text_region": [[1334.0, 723.0], [1457.0, 723.0], [1457.0, 759.0], [1334.0, 759.0]]}, {"text": "magenta", "confidence": 0.9999362230300903, "text_region": [[422.0, 756.0], [552.0, 756.0], [552.0, 792.0], [422.0, 792.0]]}, {"text": "photograph -", "confidence": 0.9713402390480042, "text_region": [[1281.0, 756.0], [1457.0, 756.0], [1457.0, 792.0], [1281.0, 792.0]]}, {"text": "bob cut", "confidence": 0.943498432636261, "text_region": [[432.0, 792.0], [552.0, 792.0], [552.0, 825.0], [432.0, 825.0]]}, {"text": "elder", "confidence": 0.9979137182235718, "text_region": [[1364.0, 795.0], [1447.0, 795.0], [1447.0, 825.0], [1364.0, 825.0]]}, {"text": "black hair -", "confidence": 0.922377347946167, "text_region": [[409.0, 825.0], [555.0, 825.0], [555.0, 861.0], [409.0, 861.0]]}, {"text": "display ", "confidence": 0.9621609449386597, "text_region": [[1340.0, 825.0], [1470.0, 825.0], [1470.0, 861.0], [1340.0, 861.0]]}, {"text": " pixie cut", "confidence": 0.966815173625946, "text_region": [[422.0, 861.0], [552.0, 861.0], [552.0, 898.0], [422.0, 898.0]]}, {"text": "tie ", "confidence": 0.9274799823760986, "text_region": [[1397.0, 861.0], [1457.0, 861.0], [1457.0, 898.0], [1397.0, 898.0]]}, {"text": "pink", "confidence": 0.998611569404602, "text_region": [[472.0, 898.0], [552.0, 898.0], [552.0, 934.0], [472.0, 934.0]]}, {"text": "shoulder ", "confidence": 0.9590184092521667, "text_region": [[1320.0, 894.0], [1463.0, 894.0], [1463.0, 931.0], [1320.0, 931.0]]}, {"text": "bangs", "confidence": 0.9997167587280273, "text_region": [[458.0, 923.0], [553.0, 932.0], [550.0, 971.0], [455.0, 963.0]]}, {"text": "frown ", "confidence": 0.95489901304245, "text_region": [[1357.0, 927.0], [1477.0, 927.0], [1477.0, 964.0], [1357.0, 964.0]]}, {"text": "newsreader", "confidence": 0.9997822642326355, "text_region": [[382.0, 960.0], [552.0, 960.0], [552.0, 997.0], [382.0, 997.0]]}, {"text": "kid -", "confidence": 0.8922422528266907, "text_region": [[1394.0, 964.0], [1460.0, 964.0], [1460.0, 1000.0], [1394.0, 1000.0]]}, {"text": "purple ", "confidence": 0.9356470704078674, "text_region": [[456.0, 1000.0], [552.0, 1000.0], [552.0, 1036.0], [456.0, 1036.0]]}, {"text": "Women", "confidence": 0.9986094236373901, "text_region": [[1041.0, 1003.0], [1141.0, 1003.0], [1141.0, 1036.0], [1041.0, 1036.0]]}, {"text": "necktie \u2193", "confidence": 0.9069179892539978, "text_region": [[1340.0, 997.0], [1483.0, 997.0], [1483.0, 1033.0], [1340.0, 1033.0]]}, {"text": "Women", "confidence": 0.9925732612609863, "text_region": [[1932.0, 1003.0], [2036.0, 1003.0], [2036.0, 1036.0], [1932.0, 1036.0]]}, {"text": "blouse ", "confidence": 0.9937995076179504, "text_region": [[448.0, 1030.0], [581.0, 1026.0], [583.0, 1069.0], [450.0, 1073.0]]}, {"text": "Men", "confidence": 0.9985647201538086, "text_region": [[1041.0, 1033.0], [1104.0, 1033.0], [1104.0, 1069.0], [1041.0, 1069.0]]}, {"text": "yellow ", "confidence": 0.9256446957588196, "text_region": [[1347.0, 1033.0], [1460.0, 1033.0], [1460.0, 1069.0], [1347.0, 1069.0]]}, {"text": "Men", "confidence": 0.9971866011619568, "text_region": [[1932.0, 1033.0], [2002.0, 1033.0], [2002.0, 1069.0], [1932.0, 1069.0]]}, {"text": "20", "confidence": 0.9996820688247681, "text_region": [[659.0, 1089.0], [688.0, 1089.0], [688.0, 1119.0], [659.0, 1119.0]]}, {"text": "40", "confidence": 0.9993141889572144, "text_region": [[778.0, 1089.0], [808.0, 1089.0], [808.0, 1119.0], [778.0, 1119.0]]}, {"text": "60", "confidence": 0.999125599861145, "text_region": [[885.0, 1089.0], [921.0, 1089.0], [921.0, 1119.0], [885.0, 1119.0]]}, {"text": "80", "confidence": 0.9988353252410889, "text_region": [[998.0, 1086.0], [1031.0, 1086.0], [1031.0, 1119.0], [998.0, 1119.0]]}, {"text": "100", "confidence": 0.9967305064201355, "text_region": [[1111.0, 1086.0], [1148.0, 1086.0], [1148.0, 1119.0], [1111.0, 1119.0]]}, {"text": "0", "confidence": 0.859375536441803, "text_region": [[1447.0, 1086.0], [1467.0, 1086.0], [1467.0, 1115.0], [1447.0, 1115.0]]}, {"text": "20", "confidence": 0.9997315406799316, "text_region": [[1553.0, 1089.0], [1587.0, 1089.0], [1587.0, 1119.0], [1553.0, 1119.0]]}, {"text": "60", "confidence": 0.9993294477462769, "text_region": [[1779.0, 1089.0], [1819.0, 1089.0], [1819.0, 1119.0], [1779.0, 1119.0]]}, {"text": "80", "confidence": 0.9991029500961304, "text_region": [[1896.0, 1082.0], [1926.0, 1082.0], [1926.0, 1119.0], [1896.0, 1119.0]]}, {"text": "100", "confidence": 0.9990773797035217, "text_region": [[2002.0, 1082.0], [2049.0, 1082.0], [2049.0, 1122.0], [2002.0, 1122.0]]}, {"text": "40", "confidence": 0.9996443390846252, "text_region": [[1670.0, 1092.0], [1706.0, 1092.0], [1706.0, 1122.0], [1670.0, 1122.0]]}, {"text": "Frequency (%)", "confidence": 0.9994068741798401, "text_region": [[775.0, 1125.0], [945.0, 1125.0], [945.0, 1158.0], [775.0, 1158.0]]}, {"text": "Frequency (%)", "confidence": 0.9997678995132446, "text_region": [[1670.0, 1122.0], [1839.0, 1122.0], [1839.0, 1158.0], [1670.0, 1158.0]]}], "img_idx": 0, "score": 0.9635786414146423}
{"type": "header", "bbox": [2128, 191, 2166, 216], "res": [{"text": "24", "confidence": 0.999871015548706, "text_region": [[2129.0, 191.0], [2169.0, 191.0], [2169.0, 221.0], [2129.0, 221.0]]}], "img_idx": 0, "score": 0.9059763550758362}
{"type": "header", "bbox": [356, 193, 1253, 226], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9943146705627441, "text_region": [[220.0, 188.0], [1370.0, 188.0], [1370.0, 231.0], [220.0, 231.0]]}], "img_idx": 0, "score": 0.6683486700057983}
{"type": "reference", "bbox": [1227, 2718, 2168, 2829], "res": [{"text": "$Note: The CelebA dataset is more representative of faces with", "confidence": 0.9792797565460205, "text_region": [[1271.0, 2703.0], [2169.0, 2709.0], [2168.0, 2766.0], [1270.0, 2759.0]]}, {"text": "lighter skin tones. Due to the nature of the dataset, we were not ", "confidence": 0.9920971989631653, "text_region": [[1224.0, 2752.0], [2175.0, 2756.0], [2175.0, 2802.0], [1224.0, 2798.0]]}, {"text": "able to control for race, gender, age, etc.", "confidence": 0.9881270527839661, "text_region": [[1227.0, 2798.0], [1806.0, 2798.0], [1806.0, 2841.0], [1227.0, 2841.0]]}], "img_idx": 0, "score": 0.9307485222816467}
