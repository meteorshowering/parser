{"type": "text", "bbox": [218, 1900, 1158, 2556], "res": [{"text": "Taken together, these results suggest that the recent shift", "confidence": 0.9945003986358643, "text_region": [[220.0, 1894.0], [1164.0, 1894.0], [1164.0, 1940.0], [220.0, 1940.0]]}, {"text": "towards large-scale task and dataset agnostic pre-training", "confidence": 0.9893918037414551, "text_region": [[213.0, 1934.0], [1164.0, 1937.0], [1164.0, 1993.0], [213.0, 1990.0]]}, {"text": "combined with a reorientation towards zero-shot and few-", "confidence": 0.9933687448501587, "text_region": [[220.0, 1990.0], [1164.0, 1990.0], [1164.0, 2033.0], [220.0, 2033.0]]}, {"text": " shot benchmarking on broad evaluation suites (as advocated", "confidence": 0.9841445684432983, "text_region": [[213.0, 2033.0], [1168.0, 2036.0], [1167.0, 2082.0], [213.0, 2079.0]]}, {"text": "by Yogatama et al. (2019) and Linzen (2020) promotes the", "confidence": 0.9813618659973145, "text_region": [[213.0, 2076.0], [1168.0, 2079.0], [1167.0, 2135.0], [213.0, 2132.0]]}, {"text": "development of more robust systems and provides a more", "confidence": 0.9979156255722046, "text_region": [[216.0, 2132.0], [1164.0, 2132.0], [1164.0, 2178.0], [216.0, 2178.0]]}, {"text": "accurate assessment of performance. We are curious to see", "confidence": 0.9874308109283447, "text_region": [[216.0, 2181.0], [1164.0, 2181.0], [1164.0, 2224.0], [216.0, 2224.0]]}, {"text": "if the same results hold for zero-shot models in the field", "confidence": 0.9882239699363708, "text_region": [[216.0, 2228.0], [1164.0, 2228.0], [1164.0, 2274.0], [216.0, 2274.0]]}, {"text": "of NLP such as the GPT family. While Hendrycks et al.", "confidence": 0.9916714429855347, "text_region": [[216.0, 2277.0], [1164.0, 2277.0], [1164.0, 2323.0], [216.0, 2323.0]]}, {"text": "(2020b) has reported that pre-training improves relative ro-", "confidence": 0.9830451607704163, "text_region": [[216.0, 2320.0], [1171.0, 2320.0], [1171.0, 2376.0], [216.0, 2376.0]]}, {"text": "bustness on sentiment analysis, Miller et al. (2020)'s study", "confidence": 0.9872766137123108, "text_region": [[220.0, 2369.0], [1161.0, 2369.0], [1161.0, 2416.0], [220.0, 2416.0]]}, {"text": "of the robustness of question answering models under nat-", "confidence": 0.9938541650772095, "text_region": [[216.0, 2416.0], [1164.0, 2416.0], [1164.0, 2462.0], [216.0, 2462.0]]}, {"text": "ural distribution shift finds, similar to Taori et al. (2020),", "confidence": 0.9850699305534363, "text_region": [[216.0, 2468.0], [1167.0, 2468.0], [1167.0, 2511.0], [216.0, 2511.0]]}, {"text": "little evidence of effective robustness improvements to date.", "confidence": 0.9865603446960449, "text_region": [[216.0, 2515.0], [1164.0, 2515.0], [1164.0, 2561.0], [216.0, 2561.0]]}], "img_idx": 0, "score": 0.9915487170219421}
{"type": "text", "bbox": [218, 1273, 2172, 1576], "res": [{"text": "Figure 14. While supervised adaptation to ImageNet increases ImageNet accuracy by 9.2% , it slightly reduces average robustness.", "confidence": 0.9933128356933594, "text_region": [[220.0, 1270.0], [2175.0, 1270.0], [2175.0, 1313.0], [220.0, 1313.0]]}, {"text": "(Left) Customizing zero-shot CLIP to each dataset improves robustness compared to using a single static zero-shot ImageNet classifier", "confidence": 0.9923601746559143, "text_region": [[216.0, 1313.0], [2172.0, 1313.0], [2172.0, 1360.0], [216.0, 1360.0]]}, {"text": "and pooling predictions across similar classes as in Taori et al. (2020). CLIP models adapted to ImageNet have similar effective robustness", "confidence": 0.9908187389373779, "text_region": [[216.0, 1356.0], [2165.0, 1356.0], [2165.0, 1402.0], [216.0, 1402.0]]}, {"text": "as the best prior ImageNet models. (Right) Details of per dataset changes in accuracy for the two robustness interventions. Adapting to", "confidence": 0.9940712451934814, "text_region": [[220.0, 1402.0], [2172.0, 1402.0], [2172.0, 1445.0], [220.0, 1445.0]]}, {"text": "ImageNet increases accuracy on ImageNetV2 noticeably but trades off accuracy on several other distributions. Dataset specific zero-shot ", "confidence": 0.9928311705589294, "text_region": [[220.0, 1445.0], [2172.0, 1445.0], [2172.0, 1488.0], [220.0, 1488.0]]}, {"text": "classifiers can improve accuracy by a large amount but are limited to only a few datasets that include classes which don't perfectly align ", "confidence": 0.9860508441925049, "text_region": [[216.0, 1488.0], [2175.0, 1488.0], [2175.0, 1534.0], [216.0, 1534.0]]}, {"text": "with ImageNet categories.", "confidence": 0.999033510684967, "text_region": [[213.0, 1528.0], [599.0, 1535.0], [598.0, 1581.0], [213.0, 1574.0]]}], "img_idx": 0, "score": 0.9879981875419617}
{"type": "text", "bbox": [1226, 1681, 2167, 1954], "res": [{"text": "humans on one of our tasks. We wanted to get a sense of", "confidence": 0.9977810382843018, "text_region": [[1224.0, 1680.0], [2172.0, 1680.0], [2172.0, 1726.0], [1224.0, 1726.0]]}, {"text": "how strong human zero-shot performance is at these tasks,", "confidence": 0.9867597818374634, "text_region": [[1224.0, 1726.0], [2172.0, 1726.0], [2172.0, 1772.0], [1224.0, 1772.0]]}, {"text": "and how much human performance is improved if they are", "confidence": 0.9969075322151184, "text_region": [[1224.0, 1775.0], [2172.0, 1775.0], [2172.0, 1822.0], [1224.0, 1822.0]]}, {"text": "shown one or two image samples. This can help us to", "confidence": 0.9945380091667175, "text_region": [[1224.0, 1822.0], [2172.0, 1822.0], [2172.0, 1868.0], [1224.0, 1868.0]]}, {"text": " compare task difficulty for humans and CLIP, and identify", "confidence": 0.9782223105430603, "text_region": [[1221.0, 1865.0], [2172.0, 1861.0], [2172.0, 1917.0], [1221.0, 1921.0]]}, {"text": " correlations and differences between them.", "confidence": 0.9947533011436462, "text_region": [[1221.0, 1914.0], [1919.0, 1917.0], [1919.0, 1964.0], [1221.0, 1960.0]]}], "img_idx": 0, "score": 0.9866148233413696}
{"type": "text", "bbox": [219, 1682, 1159, 1862], "res": [{"text": "Across our experiments, high effective robustness seems to", "confidence": 0.993411660194397, "text_region": [[216.0, 1673.0], [1164.0, 1676.0], [1164.0, 1723.0], [216.0, 1719.0]]}, {"text": "result from minimizing the amount of distribution specific", "confidence": 0.9766608476638794, "text_region": [[220.0, 1726.0], [1161.0, 1726.0], [1161.0, 1772.0], [220.0, 1772.0]]}, {"text": "training data a model has access to, but this comes at a cost ", "confidence": 0.9828587770462036, "text_region": [[216.0, 1775.0], [1167.0, 1775.0], [1167.0, 1822.0], [216.0, 1822.0]]}, {"text": " of reducing dataset-specific performance.", "confidence": 0.9872862696647644, "text_region": [[213.0, 1818.0], [885.0, 1822.0], [885.0, 1868.0], [213.0, 1864.0]]}], "img_idx": 0, "score": 0.979324460029602}
{"type": "text", "bbox": [220, 2711, 1163, 2838], "res": [{"text": " How does CLIP compare to human performance and human", "confidence": 0.9806969165802002, "text_region": [[213.0, 2699.0], [1164.0, 2703.0], [1164.0, 2759.0], [213.0, 2755.0]]}, {"text": "learning? To get a better understanding of how well humans", "confidence": 0.9973041415214539, "text_region": [[220.0, 2756.0], [1161.0, 2756.0], [1161.0, 2802.0], [220.0, 2802.0]]}, {"text": "perform in similar evaluation settings to CLIP, we evaluated", "confidence": 0.9957828521728516, "text_region": [[216.0, 2805.0], [1164.0, 2805.0], [1164.0, 2848.0], [216.0, 2848.0]]}], "img_idx": 0, "score": 0.9747262597084045}
{"type": "text", "bbox": [1227, 1995, 2170, 2450], "res": [{"text": "We had five different humans look at each of 3669 images", "confidence": 0.9985185265541077, "text_region": [[1221.0, 1987.0], [2172.0, 1990.0], [2172.0, 2036.0], [1221.0, 2033.0]]}, {"text": "in the test split of the Oxford IIT Pets dataset (Parkhi et al.,", "confidence": 0.9879259467124939, "text_region": [[1221.0, 2033.0], [2169.0, 2033.0], [2169.0, 2079.0], [1221.0, 2079.0]]}, {"text": "2012) and select which of the 37 cat or dog breeds best", "confidence": 0.9903060793876648, "text_region": [[1224.0, 2079.0], [2172.0, 2082.0], [2172.0, 2129.0], [1224.0, 2125.0]]}, {"text": "matched the image (or \u2018I don't know\u2019 if they were com-", "confidence": 0.9828940629959106, "text_region": [[1224.0, 2132.0], [2175.0, 2132.0], [2175.0, 2178.0], [1224.0, 2178.0]]}, {"text": "pletely uncertain). In the zero-shot case the humans were", "confidence": 0.9955562353134155, "text_region": [[1224.0, 2181.0], [2169.0, 2181.0], [2169.0, 2224.0], [1224.0, 2224.0]]}, {"text": "given no examples of the breeds and asked to label them", "confidence": 0.9904300570487976, "text_region": [[1224.0, 2228.0], [2172.0, 2228.0], [2172.0, 2274.0], [1224.0, 2274.0]]}, {"text": "to the best of their ability without an internet search. In", "confidence": 0.992490291595459, "text_region": [[1224.0, 2277.0], [2172.0, 2277.0], [2172.0, 2323.0], [1224.0, 2323.0]]}, {"text": "the one-shot experiment the humans were given one sample", "confidence": 0.9864245653152466, "text_region": [[1224.0, 2326.0], [2172.0, 2326.0], [2172.0, 2369.0], [1224.0, 2369.0]]}, {"text": "image of each breed and in the two-shot experiment they", "confidence": 0.9976112246513367, "text_region": [[1224.0, 2373.0], [2172.0, 2373.0], [2172.0, 2419.0], [1224.0, 2419.0]]}, {"text": "were given two sample images of each breed.5", "confidence": 0.987278163433075, "text_region": [[1217.0, 2416.0], [1972.0, 2409.0], [1973.0, 2465.0], [1218.0, 2472.0]]}], "img_idx": 0, "score": 0.9389842748641968}
{"type": "text", "bbox": [1225, 2616, 2167, 2824], "res": [{"text": " accuracy of 94% on the STL-10 dataset (Coates et al., 2011)", "confidence": 0.991001546382904, "text_region": [[1221.0, 2587.0], [2175.0, 2584.0], [2175.0, 2630.0], [1221.0, 2633.0]]}, {"text": " 5There is not a perfect correspondence between the human", "confidence": 0.9707791805267334, "text_region": [[1271.0, 2653.0], [2175.0, 2660.0], [2175.0, 2716.0], [1270.0, 2709.0]]}, {"text": "few-shot tasks and the model's few-shot performance since the", "confidence": 0.9935758113861084, "text_region": [[1224.0, 2706.0], [2172.0, 2706.0], [2172.0, 2749.0], [1224.0, 2749.0]]}, {"text": "model cannot refer to sample images in the way that the humans", "confidence": 0.9914685487747192, "text_region": [[1224.0, 2746.0], [2172.0, 2746.0], [2172.0, 2792.0], [1224.0, 2792.0]]}, {"text": "can.", "confidence": 0.9997568130493164, "text_region": [[1224.0, 2788.0], [1294.0, 2788.0], [1294.0, 2828.0], [1224.0, 2828.0]]}], "img_idx": 0, "score": 0.8710135221481323}
{"type": "title", "bbox": [222, 2629, 1018, 2669], "res": [{"text": "4. Comparison to Human Performance", "confidence": 0.9875695705413818, "text_region": [[220.0, 2627.0], [1021.0, 2627.0], [1021.0, 2673.0], [220.0, 2673.0]]}], "img_idx": 0, "score": 0.9502511024475098}
{"type": "figure", "bbox": [227, 268, 2165, 1194], "res": [{"text": "Adapt to ImageNet", "confidence": 0.9852401614189148, "text_region": [[1510.0, 264.0], [1859.0, 264.0], [1859.0, 310.0], [1510.0, 310.0]]}, {"text": "ImageNet", "confidence": 0.9945896863937378, "text_region": [[1320.0, 320.0], [1473.0, 320.0], [1473.0, 356.0], [1320.0, 356.0]]}, {"text": "+9.2", "confidence": 0.9984761476516724, "text_region": [[1666.0, 312.0], [1744.0, 321.0], [1740.0, 361.0], [1662.0, 352.0]]}, {"text": "%", "confidence": 0.9981520771980286, "text_region": [[243.0, 340.0], [276.0, 340.0], [276.0, 376.0], [243.0, 376.0]]}, {"text": "ImageNetV2", "confidence": 0.9768940806388855, "text_region": [[1277.0, 360.0], [1472.0, 352.0], [1474.0, 399.0], [1278.0, 407.0]]}, {"text": "+5.8", "confidence": 0.9988259673118591, "text_region": [[1600.0, 366.0], [1666.0, 366.0], [1666.0, 393.0], [1600.0, 393.0]]}, {"text": "180-", "confidence": 0.7884629368782043, "text_region": [[246.0, 383.0], [333.0, 383.0], [333.0, 422.0], [246.0, 422.0]]}, {"text": "Youtube-BB", "confidence": 0.999687671661377, "text_region": [[1291.0, 399.0], [1483.0, 399.0], [1483.0, 436.0], [1291.0, 436.0]]}, {"text": "+0.6", "confidence": 0.9972577691078186, "text_region": [[1480.0, 399.0], [1567.0, 399.0], [1567.0, 436.0], [1480.0, 436.0]]}, {"text": "0.5", "confidence": 0.9996433854103088, "text_region": [[1407.0, 442.0], [1457.0, 442.0], [1457.0, 469.0], [1407.0, 469.0]]}, {"text": "imageNet Vid", "confidence": 0.9688830375671387, "text_region": [[1473.0, 439.0], [1689.0, 432.0], [1690.0, 478.0], [1475.0, 486.0]]}, {"text": "75", "confidence": 0.9998771548271179, "text_region": [[286.0, 469.0], [329.0, 469.0], [329.0, 498.0], [286.0, 498.0]]}, {"text": "1.9", "confidence": 0.9992310404777527, "text_region": [[1377.0, 485.0], [1440.0, 485.0], [1440.0, 511.0], [1377.0, 511.0]]}, {"text": "ImageNet-A ", "confidence": 0.9453355669975281, "text_region": [[1480.0, 479.0], [1666.0, 479.0], [1666.0, 525.0], [1480.0, 525.0]]}, {"text": "2.8", "confidence": 0.9997381567955017, "text_region": [[1360.0, 525.0], [1424.0, 525.0], [1424.0, 551.0], [1360.0, 551.0]]}, {"text": "ImageNet Sketch", "confidence": 0.9956110715866089, "text_region": [[1477.0, 518.0], [1743.0, 518.0], [1743.0, 564.0], [1477.0, 564.0]]}, {"text": "o", "confidence": 0.6219496130943298, "text_region": [[256.0, 540.0], [286.0, 553.0], [270.0, 591.0], [240.0, 578.0]]}, {"text": "70", "confidence": 0.9997469782829285, "text_region": [[286.0, 544.0], [319.0, 544.0], [319.0, 574.0], [286.0, 574.0]]}, {"text": "-3.8", "confidence": 0.9990758299827576, "text_region": [[1334.0, 561.0], [1404.0, 561.0], [1404.0, 597.0], [1334.0, 597.0]]}, {"text": "ObjectNet", "confidence": 0.9996592402458191, "text_region": [[1477.0, 558.0], [1640.0, 558.0], [1640.0, 604.0], [1477.0, 604.0]]}, {"text": "4.7", "confidence": 0.9997137188911438, "text_region": [[1324.0, 604.0], [1380.0, 604.0], [1380.0, 630.0], [1324.0, 630.0]]}, {"text": "ImageNet-R", "confidence": 0.9920169711112976, "text_region": [[1477.0, 601.0], [1660.0, 601.0], [1660.0, 637.0], [1477.0, 637.0]]}, {"text": "10", "confidence": 0.9993361234664917, "text_region": [[1254.0, 653.0], [1310.0, 653.0], [1310.0, 690.0], [1254.0, 690.0]]}, {"text": "0", "confidence": 0.5736365914344788, "text_region": [[1469.0, 649.0], [1497.0, 663.0], [1483.0, 691.0], [1455.0, 677.0]]}, {"text": "15", "confidence": 0.9998409748077393, "text_region": [[1763.0, 650.0], [1813.0, 650.0], [1813.0, 690.0], [1763.0, 690.0]]}, {"text": "20", "confidence": 0.9995647072792053, "text_region": [[1863.0, 650.0], [1919.0, 650.0], [1919.0, 690.0], [1863.0, 690.0]]}, {"text": "25", "confidence": 0.9998905062675476, "text_region": [[1969.0, 647.0], [2022.0, 647.0], [2022.0, 690.0], [1969.0, 690.0]]}, {"text": "-5", "confidence": 0.9645822048187256, "text_region": [[1357.0, 657.0], [1397.0, 657.0], [1397.0, 686.0], [1357.0, 686.0]]}, {"text": "5", "confidence": 0.9928436875343323, "text_region": [[1556.0, 658.0], [1589.0, 647.0], [1599.0, 678.0], [1566.0, 689.0]]}, {"text": "10", "confidence": 0.9995532035827637, "text_region": [[1663.0, 657.0], [1713.0, 657.0], [1713.0, 690.0], [1663.0, 690.0]]}, {"text": "30", "confidence": 0.9996061325073242, "text_region": [[2072.0, 650.0], [2122.0, 650.0], [2122.0, 690.0], [2072.0, 690.0]]}, {"text": "10", "confidence": 0.580144464969635, "text_region": [[253.0, 670.0], [283.0, 670.0], [283.0, 700.0], [253.0, 700.0]]}, {"text": "60", "confidence": 0.9993247985839844, "text_region": [[283.0, 670.0], [329.0, 670.0], [329.0, 710.0], [283.0, 710.0]]}, {"text": " Change from zero-shot ImageNet classifier accuracy (%) ", "confidence": 0.9602222442626953, "text_region": [[1254.0, 693.0], [2115.0, 693.0], [2115.0, 739.0], [1254.0, 739.0]]}, {"text": "55", "confidence": 0.9997698068618774, "text_region": [[286.0, 736.0], [326.0, 736.0], [326.0, 762.0], [286.0, 762.0]]}, {"text": "Adapt to class shift", "confidence": 0.9953716993331909, "text_region": [[1507.0, 749.0], [1859.0, 749.0], [1859.0, 795.0], [1507.0, 795.0]]}, {"text": "50", "confidence": 0.9997847676277161, "text_region": [[286.0, 799.0], [329.0, 799.0], [329.0, 825.0], [286.0, 825.0]]}, {"text": "Youtube-BB", "confidence": 0.9997847676277161, "text_region": [[1294.0, 805.0], [1483.0, 805.0], [1483.0, 842.0], [1294.0, 842.0]]}, {"text": "+26.9", "confidence": 0.9998559951782227, "text_region": [[2039.0, 805.0], [2132.0, 805.0], [2132.0, 842.0], [2039.0, 842.0]]}, {"text": "ImageNet Vid", "confidence": 0.9933354258537292, "text_region": [[1264.0, 848.0], [1473.0, 848.0], [1473.0, 884.0], [1264.0, 884.0]]}, {"text": "+8.3", "confidence": 0.9956701993942261, "text_region": [[1640.0, 848.0], [1723.0, 848.0], [1723.0, 884.0], [1640.0, 884.0]]}, {"text": "45", "confidence": 0.9998989105224609, "text_region": [[286.0, 858.0], [326.0, 858.0], [326.0, 888.0], [286.0, 888.0]]}, {"text": "ObjectNet", "confidence": 0.9992979764938354, "text_region": [[1313.0, 885.0], [1472.0, 877.0], [1474.0, 923.0], [1315.0, 931.0]]}, {"text": "+2.3", "confidence": 0.9984622001647949, "text_region": [[1520.0, 888.0], [1603.0, 888.0], [1603.0, 924.0], [1520.0, 924.0]]}, {"text": "40", "confidence": 0.9997971057891846, "text_region": [[286.0, 917.0], [326.0, 917.0], [326.0, 944.0], [286.0, 944.0]]}, {"text": "ldeal robust model (y =x)", "confidence": 0.9307864308357239, "text_region": [[891.0, 914.0], [1164.0, 914.0], [1164.0, 937.0], [891.0, 937.0]]}, {"text": "ImageNet Sketch", "confidence": 0.9935160279273987, "text_region": [[1204.0, 921.0], [1497.0, 921.0], [1497.0, 964.0], [1204.0, 964.0]]}, {"text": " Adaptive Zero-Shot CLIP", "confidence": 0.9731236100196838, "text_region": [[885.0, 937.0], [1157.0, 937.0], [1157.0, 980.0], [885.0, 980.0]]}, {"text": "ImageNet Zero-Shot CLIP", "confidence": 0.9789413213729858, "text_region": [[885.0, 964.0], [1161.0, 964.0], [1161.0, 1007.0], [885.0, 1007.0]]}, {"text": "ImageNet-R|o", "confidence": 0.9253897070884705, "text_region": [[1291.0, 967.0], [1500.0, 967.0], [1500.0, 1003.0], [1291.0, 1003.0]]}, {"text": "%", "confidence": 0.894772469997406, "text_region": [[256.0, 983.0], [286.0, 983.0], [286.0, 1013.0], [256.0, 1013.0]]}, {"text": "35", "confidence": 0.999531626701355, "text_region": [[279.0, 983.0], [319.0, 983.0], [319.0, 1010.0], [279.0, 1010.0]]}, {"text": " Logistic Regression CLIP ", "confidence": 0.9510316252708435, "text_region": [[885.0, 997.0], [1157.0, 997.0], [1157.0, 1040.0], [885.0, 1040.0]]}, {"text": "ImageNet-Alo", "confidence": 0.9559200406074524, "text_region": [[1291.0, 1007.0], [1500.0, 1007.0], [1500.0, 1043.0], [1291.0, 1043.0]]}, {"text": " Standard ImageNet training", "confidence": 0.973870575428009, "text_region": [[885.0, 1026.0], [1187.0, 1026.0], [1187.0, 1069.0], [885.0, 1069.0]]}, {"text": "30", "confidence": 0.9717334508895874, "text_region": [[248.0, 1048.0], [321.0, 1036.0], [328.0, 1075.0], [254.0, 1088.0]]}, {"text": "ImageNetV2]0", "confidence": 0.9429833889007568, "text_region": [[1277.0, 1043.0], [1503.0, 1043.0], [1503.0, 1089.0], [1277.0, 1089.0]]}, {"text": "Robustness intervention", "confidence": 0.997040867805481, "text_region": [[885.0, 1056.0], [1154.0, 1056.0], [1154.0, 1099.0], [885.0, 1099.0]]}, {"text": "ImageNetlo", "confidence": 0.9747764468193054, "text_region": [[1317.0, 1082.0], [1503.0, 1082.0], [1503.0, 1129.0], [1317.0, 1129.0]]}, {"text": "Trained with more data", "confidence": 0.9892635345458984, "text_region": [[888.0, 1092.0], [1141.0, 1092.0], [1141.0, 1125.0], [888.0, 1125.0]]}, {"text": "25", "confidence": 0.9998111724853516, "text_region": [[286.0, 1129.0], [323.0, 1129.0], [323.0, 1155.0], [286.0, 1155.0]]}, {"text": "-10", "confidence": 0.9934802055358887, "text_region": [[1244.0, 1138.0], [1307.0, 1138.0], [1307.0, 1175.0], [1244.0, 1175.0]]}, {"text": "15", "confidence": 0.9997929334640503, "text_region": [[1766.0, 1135.0], [1813.0, 1135.0], [1813.0, 1175.0], [1766.0, 1175.0]]}, {"text": "20", "confidence": 0.9997818470001221, "text_region": [[1866.0, 1135.0], [1916.0, 1135.0], [1916.0, 1175.0], [1866.0, 1175.0]]}, {"text": "25", "confidence": 0.9998958706855774, "text_region": [[1972.0, 1132.0], [2019.0, 1132.0], [2019.0, 1172.0], [1972.0, 1172.0]]}, {"text": "90", "confidence": 0.9992109537124634, "text_region": [[815.0, 1145.0], [861.0, 1145.0], [861.0, 1185.0], [815.0, 1185.0]]}, {"text": "-5", "confidence": 0.9829930067062378, "text_region": [[1354.0, 1142.0], [1397.0, 1142.0], [1397.0, 1172.0], [1354.0, 1172.0]]}, {"text": "0", "confidence": 0.9398828148841858, "text_region": [[1460.0, 1142.0], [1490.0, 1142.0], [1490.0, 1172.0], [1460.0, 1172.0]]}, {"text": "5", "confidence": 0.999943733215332, "text_region": [[1567.0, 1142.0], [1593.0, 1142.0], [1593.0, 1172.0], [1567.0, 1172.0]]}, {"text": "10", "confidence": 0.9993088841438293, "text_region": [[1663.0, 1142.0], [1703.0, 1142.0], [1703.0, 1172.0], [1663.0, 1172.0]]}, {"text": "30", "confidence": 0.9995186924934387, "text_region": [[2075.0, 1142.0], [2119.0, 1142.0], [2119.0, 1172.0], [2075.0, 1172.0]]}, {"text": "70", "confidence": 0.9995195865631104, "text_region": [[316.0, 1152.0], [353.0, 1152.0], [353.0, 1178.0], [316.0, 1178.0]]}, {"text": "75", "confidence": 0.9998719692230225, "text_region": [[412.0, 1155.0], [449.0, 1155.0], [449.0, 1181.0], [412.0, 1181.0]]}, {"text": "80", "confidence": 0.9996281862258911, "text_region": [[519.0, 1152.0], [555.0, 1152.0], [555.0, 1181.0], [519.0, 1181.0]]}, {"text": "85", "confidence": 0.999800980091095, "text_region": [[645.0, 1152.0], [682.0, 1152.0], [682.0, 1181.0], [645.0, 1181.0]]}, {"text": "95", "confidence": 0.9997851848602295, "text_region": [[1094.0, 1155.0], [1131.0, 1155.0], [1131.0, 1181.0], [1094.0, 1181.0]]}, {"text": " Change from zero-shot ImageNet classifier accuracy (%) ", "confidence": 0.9847950339317322, "text_region": [[1247.0, 1168.0], [2119.0, 1172.0], [2119.0, 1228.0], [1247.0, 1224.0]]}, {"text": " Average on class subsampled ImageNet (top-1, %)", "confidence": 0.9706798195838928, "text_region": [[426.0, 1178.0], [1101.0, 1178.0], [1101.0, 1221.0], [426.0, 1221.0]]}], "img_idx": 0, "score": 0.8653888702392578}
{"type": "figure_caption", "bbox": [1264, 1187, 2110, 1216], "res": [{"text": " Change from zero-shot ImageNet classifier accuracy (%) ", "confidence": 0.9847950339317322, "text_region": [[1247.0, 1168.0], [2119.0, 1172.0], [2119.0, 1228.0], [1247.0, 1224.0]]}], "img_idx": 0, "score": 0.8403348922729492}
{"type": "figure_caption", "bbox": [437, 1190, 1100, 1217], "res": [{"text": " Average on class subsampled ImageNet (top-1, %)", "confidence": 0.9706798195838928, "text_region": [[426.0, 1178.0], [1101.0, 1178.0], [1101.0, 1221.0], [426.0, 1221.0]]}], "img_idx": 0, "score": 0.6395657658576965}
{"type": "header", "bbox": [2129, 193, 2165, 218], "res": [{"text": "16", "confidence": 0.9998273849487305, "text_region": [[2129.0, 191.0], [2165.0, 191.0], [2165.0, 221.0], [2129.0, 221.0]]}], "img_idx": 0, "score": 0.905729353427887}
{"type": "header", "bbox": [485, 193, 1367, 225], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.986918568611145, "text_region": [[216.0, 188.0], [1374.0, 188.0], [1374.0, 231.0], [216.0, 231.0]]}], "img_idx": 0, "score": 0.6876739263534546}
