{"type": "text", "bbox": [1226, 1659, 2169, 2869], "res": [{"text": "ten than not and wins on 16 of the 27 datasets. Looking at", "confidence": 0.996218740940094, "text_region": [[1221.0, 1653.0], [2172.0, 1657.0], [2172.0, 1703.0], [1221.0, 1699.0]]}, {"text": "individual datasets reveals some interesting behavior. On", "confidence": 0.9979802370071411, "text_region": [[1224.0, 1703.0], [2172.0, 1703.0], [2172.0, 1749.0], [1224.0, 1749.0]]}, {"text": "fine-grained classification tasks, we observe a wide spread", "confidence": 0.985447883605957, "text_region": [[1221.0, 1749.0], [2165.0, 1749.0], [2165.0, 1795.0], [1221.0, 1795.0]]}, {"text": "in performance. On two of these datasets, Stanford Cars and ", "confidence": 0.9885051250457764, "text_region": [[1224.0, 1802.0], [2175.0, 1802.0], [2175.0, 1848.0], [1224.0, 1848.0]]}, {"text": "Food101, zero-shot CLIP outperforms logistic regression", "confidence": 0.993778645992279, "text_region": [[1224.0, 1848.0], [2172.0, 1848.0], [2172.0, 1894.0], [1224.0, 1894.0]]}, {"text": "on ResNet-50 features by over 20% while on two others,", "confidence": 0.9972729682922363, "text_region": [[1224.0, 1894.0], [2172.0, 1894.0], [2172.0, 1940.0], [1224.0, 1940.0]]}, {"text": " Flowers102 and FGVCAircraft, zero-shot CLIP underper-", "confidence": 0.9933921694755554, "text_region": [[1221.0, 1940.0], [2169.0, 1944.0], [2169.0, 1990.0], [1221.0, 1987.0]]}, {"text": "forms by over 10%. (", "confidence": 0.9530216455459595, "text_region": [[1220.0, 1990.0], [1606.0, 1986.0], [1607.0, 2033.0], [1221.0, 2036.0]]}, {"text": " On OxfordPets and Birdsnap, per-", "confidence": 0.9960366487503052, "text_region": [[1584.0, 1983.0], [2175.0, 1990.0], [2175.0, 2043.0], [1583.0, 2036.0]]}, {"text": "formance is much closer. We suspect these difference are", "confidence": 0.9971509575843811, "text_region": [[1224.0, 2039.0], [2169.0, 2039.0], [2169.0, 2086.0], [1224.0, 2086.0]]}, {"text": "primarily due to varying amounts of per-task supervision", "confidence": 0.9938187599182129, "text_region": [[1227.0, 2089.0], [2172.0, 2089.0], [2172.0, 2135.0], [1227.0, 2135.0]]}, {"text": "between WIT and ImageNet. On \u201cgeneral' object classifica-", "confidence": 0.9866312146186829, "text_region": [[1227.0, 2135.0], [2172.0, 2135.0], [2172.0, 2181.0], [1227.0, 2181.0]]}, {"text": "tion datasets such as ImageNet, CIFAR10/100, STL10, and", "confidence": 0.999284029006958, "text_region": [[1227.0, 2181.0], [2175.0, 2181.0], [2175.0, 2228.0], [1227.0, 2228.0]]}, {"text": "PascalVOC2007 performance is relatively similar with a", "confidence": 0.9867838621139526, "text_region": [[1227.0, 2228.0], [2172.0, 2228.0], [2172.0, 2274.0], [1227.0, 2274.0]]}, {"text": "slight advantage for zero-shot CLIP in all cases. On STL10,", "confidence": 0.9983518719673157, "text_region": [[1224.0, 2274.0], [2172.0, 2274.0], [2172.0, 2320.0], [1224.0, 2320.0]]}, {"text": "CLIP achieves 99.3% overall which appears to be a new", "confidence": 0.9916502237319946, "text_region": [[1227.0, 2326.0], [2172.0, 2326.0], [2172.0, 2373.0], [1227.0, 2373.0]]}, {"text": "state of the art despite not using any training examples. Zero-", "confidence": 0.9936848282814026, "text_region": [[1221.0, 2366.0], [2179.0, 2369.0], [2178.0, 2426.0], [1221.0, 2422.0]]}, {"text": "shot CLIP significantly outperforms a ResNet-50 on two", "confidence": 0.994626522064209, "text_region": [[1227.0, 2422.0], [2169.0, 2422.0], [2169.0, 2468.0], [1227.0, 2468.0]]}, {"text": "datasets measuring action recognition in videos. On Kinet-", "confidence": 0.9959654211997986, "text_region": [[1224.0, 2472.0], [2175.0, 2472.0], [2175.0, 2515.0], [1224.0, 2515.0]]}, {"text": "ics700, CLIP outperforms a ResNet-50 by 14.5%. Zero-", "confidence": 0.9941554665565491, "text_region": [[1217.0, 2511.0], [2175.0, 2508.0], [2175.0, 2564.0], [1217.0, 2567.0]]}, {"text": "shot CLIP also outperforms a ResNet-50's features by 7.7%", "confidence": 0.9949312806129456, "text_region": [[1224.0, 2564.0], [2172.0, 2564.0], [2172.0, 2610.0], [1224.0, 2610.0]]}, {"text": "on UCF101. We speculate this is due to natural language", "confidence": 0.9928063750267029, "text_region": [[1224.0, 2614.0], [2169.0, 2614.0], [2169.0, 2660.0], [1224.0, 2660.0]]}, {"text": " providing wider supervision for visual concepts involving", "confidence": 0.9869030714035034, "text_region": [[1221.0, 2656.0], [2175.0, 2656.0], [2175.0, 2713.0], [1221.0, 2713.0]]}, {"text": "verbs, compared to the noun-centric object supervision in", "confidence": 0.9983325600624084, "text_region": [[1224.0, 2709.0], [2172.0, 2709.0], [2172.0, 2756.0], [1224.0, 2756.0]]}, {"text": " ImageNet.", "confidence": 0.9501606225967407, "text_region": [[1217.0, 2752.0], [1404.0, 2752.0], [1404.0, 2808.0], [1217.0, 2808.0]]}, {"text": "Looking at where zero-shot CLIP notably underperforms,", "confidence": 0.9856442809104919, "text_region": [[1224.0, 2828.0], [2172.0, 2828.0], [2172.0, 2874.0], [1224.0, 2874.0]]}], "img_idx": 0, "score": 0.9962717890739441}
{"type": "text", "bbox": [218, 1067, 1160, 2063], "res": [{"text": "We also experimented with ensembling over multiple zero-", "confidence": 0.9869542717933655, "text_region": [[213.0, 1059.0], [1164.0, 1063.0], [1164.0, 1109.0], [213.0, 1105.0]]}, {"text": " shot classifiers as another way of improving performance.", "confidence": 0.991209089756012, "text_region": [[213.0, 1109.0], [1164.0, 1112.0], [1164.0, 1158.0], [213.0, 1155.0]]}, {"text": "These classifiers are computed by using different context", "confidence": 0.9976202249526978, "text_region": [[216.0, 1162.0], [1167.0, 1162.0], [1167.0, 1208.0], [216.0, 1208.0]]}, {"text": "prompts suchas\u2018A photo of a big {label}\u201dand", "confidence": 0.9771662950515747, "text_region": [[213.0, 1211.0], [1164.0, 1204.0], [1164.0, 1251.0], [213.0, 1257.0]]}, {"text": "\"A photo of a small {label}\".We constructthe", "confidence": 0.9685130715370178, "text_region": [[220.0, 1251.0], [1161.0, 1251.0], [1161.0, 1297.0], [220.0, 1297.0]]}, {"text": " ensemble over the embedding space instead of probability", "confidence": 0.9874777793884277, "text_region": [[213.0, 1300.0], [1164.0, 1300.0], [1164.0, 1356.0], [213.0, 1356.0]]}, {"text": "space. This allows us to cache a single set of averaged text", "confidence": 0.9978747963905334, "text_region": [[216.0, 1353.0], [1161.0, 1353.0], [1161.0, 1399.0], [216.0, 1399.0]]}, {"text": "embeddings so that the compute cost of the ensemble is the", "confidence": 0.9698942303657532, "text_region": [[213.0, 1399.0], [1164.0, 1396.0], [1164.0, 1442.0], [213.0, 1445.0]]}, {"text": "same as using a single classifier when amortized over many", "confidence": 0.9990171790122986, "text_region": [[216.0, 1449.0], [1164.0, 1449.0], [1164.0, 1495.0], [216.0, 1495.0]]}, {"text": "predictions. We've observed ensembling across many gen-", "confidence": 0.9993782639503479, "text_region": [[216.0, 1495.0], [1164.0, 1495.0], [1164.0, 1541.0], [216.0, 1541.0]]}, {"text": "erated zero-shot classifiers to reliably improve performance", "confidence": 0.9943209290504456, "text_region": [[216.0, 1541.0], [1164.0, 1541.0], [1164.0, 1587.0], [216.0, 1587.0]]}, {"text": "and use it for the majority of datasets. On ImageNet, we", "confidence": 0.9954819679260254, "text_region": [[216.0, 1591.0], [1161.0, 1591.0], [1161.0, 1637.0], [216.0, 1637.0]]}, {"text": "ensemble 80 different context prompts and this improves", "confidence": 0.9993948936462402, "text_region": [[220.0, 1640.0], [1161.0, 1640.0], [1161.0, 1686.0], [220.0, 1686.0]]}, {"text": "performance by an additional 3.5% over the single default", "confidence": 0.9988304376602173, "text_region": [[220.0, 1683.0], [1161.0, 1683.0], [1161.0, 1729.0], [220.0, 1729.0]]}, {"text": "prompt discussed above. When considered together, prompt", "confidence": 0.9898902177810669, "text_region": [[220.0, 1736.0], [1161.0, 1736.0], [1161.0, 1782.0], [220.0, 1782.0]]}, {"text": "engineering and ensembling improve ImageNet accuracy", "confidence": 0.987179696559906, "text_region": [[220.0, 1782.0], [1161.0, 1782.0], [1161.0, 1828.0], [220.0, 1828.0]]}, {"text": "by almost 5%. In Figure 4 we visualize how prompt engi-", "confidence": 0.9986267685890198, "text_region": [[213.0, 1822.0], [1171.0, 1825.0], [1171.0, 1881.0], [213.0, 1878.0]]}, {"text": "neering and ensembling change the performance of a set of", "confidence": 0.9906582832336426, "text_region": [[220.0, 1878.0], [1167.0, 1878.0], [1167.0, 1924.0], [220.0, 1924.0]]}, {"text": "CLIP models compared to the contextless baseline approach", "confidence": 0.9872226119041443, "text_region": [[216.0, 1917.0], [1161.0, 1921.0], [1161.0, 1977.0], [216.0, 1973.0]]}, {"text": "of directly embedding the class name as done in Li et al.", "confidence": 0.9939999580383301, "text_region": [[216.0, 1973.0], [1167.0, 1973.0], [1167.0, 2020.0], [216.0, 2020.0]]}, {"text": "(2017).", "confidence": 0.9998268485069275, "text_region": [[216.0, 2020.0], [343.0, 2020.0], [343.0, 2066.0], [216.0, 2066.0]]}], "img_idx": 0, "score": 0.9935653805732727}
{"type": "text", "bbox": [220, 2193, 1160, 2824], "res": [{"text": "Since task-agnostic zero-shot classifiers for computer vision", "confidence": 0.9971339106559753, "text_region": [[220.0, 2191.0], [1161.0, 2191.0], [1161.0, 2237.0], [220.0, 2237.0]]}, {"text": "have been understudied, CLIP provides a promising oppor-", "confidence": 0.9891371726989746, "text_region": [[213.0, 2227.0], [1168.0, 2234.0], [1167.0, 2294.0], [213.0, 2287.0]]}, {"text": "tunity to gain a better understanding of this type of model.", "confidence": 0.9950165152549744, "text_region": [[216.0, 2287.0], [1164.0, 2287.0], [1164.0, 2333.0], [216.0, 2333.0]]}, {"text": "In this section, we conduct a study of various properties of", "confidence": 0.9806448817253113, "text_region": [[216.0, 2330.0], [1167.0, 2330.0], [1167.0, 2386.0], [216.0, 2386.0]]}, {"text": " CLIP's zero-shot classifiers. As a first question, we look", "confidence": 0.9699483513832092, "text_region": [[210.0, 2373.0], [1168.0, 2376.0], [1167.0, 2432.0], [209.0, 2429.0]]}, {"text": "simply at how well zero-shot classifiers perform. To con-", "confidence": 0.9912853837013245, "text_region": [[216.0, 2432.0], [1167.0, 2432.0], [1167.0, 2478.0], [216.0, 2478.0]]}, {"text": "textualize this, we compare to the performance of a simple", "confidence": 0.9955589175224304, "text_region": [[220.0, 2478.0], [1161.0, 2478.0], [1161.0, 2524.0], [220.0, 2524.0]]}, {"text": " off-the-shelf baseline: ftting a fully supervised, regularized,", "confidence": 0.9855225682258606, "text_region": [[210.0, 2518.0], [1168.0, 2521.0], [1167.0, 2577.0], [209.0, 2574.0]]}, {"text": "logistic regression classifier on the features of the canonical", "confidence": 0.9974258542060852, "text_region": [[220.0, 2574.0], [1161.0, 2574.0], [1161.0, 2620.0], [220.0, 2620.0]]}, {"text": "ResNet-50. In Figure 5 we show this comparison across 27", "confidence": 0.993066132068634, "text_region": [[220.0, 2620.0], [1164.0, 2620.0], [1164.0, 2666.0], [220.0, 2666.0]]}, {"text": "datasets. Please see Appendix A for details of datasets and", "confidence": 0.9975569844245911, "text_region": [[216.0, 2670.0], [1164.0, 2670.0], [1164.0, 2716.0], [216.0, 2716.0]]}, {"text": " setup.", "confidence": 0.940451979637146, "text_region": [[212.0, 2711.0], [321.0, 2720.0], [316.0, 2770.0], [208.0, 2761.0]]}, {"text": " Zero-shot CLIP outperforms this baseline slightly more of-", "confidence": 0.9951661825180054, "text_region": [[213.0, 2785.0], [1168.0, 2789.0], [1167.0, 2835.0], [213.0, 2831.0]]}], "img_idx": 0, "score": 0.9935173988342285}
{"type": "text", "bbox": [219, 281, 1161, 1023], "res": [{"text": "Similar to the \u201c\"prompt engineering\u201d discussion around GPT-", "confidence": 0.9790552258491516, "text_region": [[220.0, 274.0], [1167.0, 274.0], [1167.0, 320.0], [220.0, 320.0]]}, {"text": "3 (Brown et al., 2020; Ga0 et al., 2020), we have also", "confidence": 0.9898505210876465, "text_region": [[216.0, 323.0], [1164.0, 323.0], [1164.0, 370.0], [216.0, 370.0]]}, {"text": " observed that zero-shot performance can be significantly", "confidence": 0.9813179969787598, "text_region": [[210.0, 363.0], [1168.0, 366.0], [1167.0, 422.0], [209.0, 419.0]]}, {"text": "improved by customizing the prompt text to each task. A", "confidence": 0.999189555644989, "text_region": [[220.0, 422.0], [1164.0, 422.0], [1164.0, 465.0], [220.0, 465.0]]}, {"text": "few, non exhaustive, examples follow. We found on several", "confidence": 0.9920349717140198, "text_region": [[220.0, 465.0], [1161.0, 465.0], [1161.0, 511.0], [220.0, 511.0]]}, {"text": "fine-grained image classification datasets that it helped to", "confidence": 0.9925373792648315, "text_region": [[220.0, 511.0], [1157.0, 511.0], [1157.0, 558.0], [220.0, 558.0]]}, {"text": "specify the category. For example on Oxford-IIIT Pets, us-", "confidence": 0.983989953994751, "text_region": [[213.0, 558.0], [1171.0, 554.0], [1171.0, 610.0], [213.0, 614.0]]}, {"text": "ing\"A photo of a {label}, a type of pet.\"", "confidence": 0.9608376622200012, "text_region": [[220.0, 610.0], [1167.0, 610.0], [1167.0, 657.0], [220.0, 657.0]]}, {"text": "to help provide context worked well. Likewise, on Food101", "confidence": 0.9952600598335266, "text_region": [[220.0, 657.0], [1157.0, 657.0], [1157.0, 703.0], [220.0, 703.0]]}, {"text": "specifying a type of food and on FGVC Aircraft a type of", "confidence": 0.9955430030822754, "text_region": [[216.0, 706.0], [1167.0, 706.0], [1167.0, 752.0], [216.0, 752.0]]}, {"text": "aircraft helped too. For OCR datasets, we found that putting", "confidence": 0.999293327331543, "text_region": [[213.0, 746.0], [1168.0, 749.0], [1167.0, 805.0], [213.0, 802.0]]}, {"text": "quotes around the text or number to be recognized improved", "confidence": 0.982923686504364, "text_region": [[216.0, 802.0], [1161.0, 802.0], [1161.0, 848.0], [216.0, 848.0]]}, {"text": "performance. Finally, we found that on satellite image classi-", "confidence": 0.9989399313926697, "text_region": [[216.0, 848.0], [1167.0, 848.0], [1167.0, 894.0], [216.0, 894.0]]}, {"text": "fication datasets it helped to specify that the images were of", "confidence": 0.9975808262825012, "text_region": [[216.0, 898.0], [1167.0, 898.0], [1167.0, 944.0], [216.0, 944.0]]}, {"text": "this form and we use variants of \u201ca satellite photo", "confidence": 0.9824555516242981, "text_region": [[216.0, 940.0], [1161.0, 947.0], [1161.0, 993.0], [216.0, 987.0]]}, {"text": "of a {label}.\".", "confidence": 0.9858054518699646, "text_region": [[216.0, 990.0], [552.0, 987.0], [552.0, 1033.0], [216.0, 1036.0]]}], "img_idx": 0, "score": 0.9907475709915161}
{"type": "text", "bbox": [1226, 1376, 2169, 1539], "res": [{"text": "Figure 5. Zero-shot CLIP is competitive with a fully super-", "confidence": 0.9923503398895264, "text_region": [[1224.0, 1369.0], [2172.0, 1373.0], [2172.0, 1419.0], [1224.0, 1416.0]]}, {"text": "vised baseline. Across a 27 dataset eval suite, a zero-shot CLIP", "confidence": 0.9932664632797241, "text_region": [[1221.0, 1412.0], [2172.0, 1409.0], [2172.0, 1455.0], [1221.0, 1459.0]]}, {"text": "classifier outperforms a fully supervised linear classifier fitted on", "confidence": 0.9802374243736267, "text_region": [[1224.0, 1459.0], [2172.0, 1459.0], [2172.0, 1505.0], [1224.0, 1505.0]]}, {"text": " ResNet-50 features on 16 datasets, including ImageNet.", "confidence": 0.9960403442382812, "text_region": [[1221.0, 1498.0], [2036.0, 1502.0], [2035.0, 1548.0], [1221.0, 1544.0]]}], "img_idx": 0, "score": 0.9791371822357178}
{"type": "title", "bbox": [224, 2121, 995, 2153], "res": [{"text": "3.1.5. ANALYSIS OF ZERO-SHOT CLIP PERFORMANCE", "confidence": 0.9979372620582581, "text_region": [[220.0, 2115.0], [1157.0, 2115.0], [1157.0, 2158.0], [220.0, 2158.0]]}], "img_idx": 0, "score": 0.7020035982131958}
{"type": "figure", "bbox": [1225, 259, 2180, 1276], "res": [{"text": "StanfordCars", "confidence": 0.9987314343452454, "text_region": [[1464.0, 260.0], [1697.0, 268.0], [1695.0, 314.0], [1463.0, 306.0]]}, {"text": "1+28.9", "confidence": 0.980130672454834, "text_region": [[1978.0, 268.0], [2090.0, 260.0], [2093.0, 306.0], [1982.0, 315.0]]}, {"text": "Country211", "confidence": 0.9900199174880981, "text_region": [[1487.0, 307.0], [1686.0, 307.0], [1686.0, 343.0], [1487.0, 343.0]]}, {"text": "23.2", "confidence": 0.9996833205223083, "text_region": [[1959.0, 310.0], [2032.0, 310.0], [2032.0, 333.0], [1959.0, 333.0]]}, {"text": "Food101", "confidence": 0.9494524002075195, "text_region": [[1529.0, 337.0], [1691.0, 329.0], [1694.0, 376.0], [1532.0, 384.0]]}, {"text": "+22.5", "confidence": 0.9932023882865906, "text_region": [[1936.0, 337.0], [2029.0, 337.0], [2029.0, 373.0], [1936.0, 373.0]]}, {"text": "Kinetics700", "confidence": 0.9997492432594299, "text_region": [[1493.0, 373.0], [1690.0, 373.0], [1690.0, 409.0], [1493.0, 409.0]]}, {"text": "+14.5", "confidence": 0.997668445110321, "text_region": [[1845.0, 374.0], [1937.0, 366.0], [1940.0, 399.0], [1848.0, 407.0]]}, {"text": "SST2", "confidence": 0.9967539310455322, "text_region": [[1595.0, 407.0], [1684.0, 398.0], [1688.0, 438.0], [1599.0, 447.0]]}, {"text": "+12.4", "confidence": 0.8750240206718445, "text_region": [[1833.0, 403.0], [1929.0, 403.0], [1929.0, 439.0], [1833.0, 439.0]]}, {"text": "SUN397", "confidence": 0.9989673495292664, "text_region": [[1553.0, 439.0], [1690.0, 439.0], [1690.0, 475.0], [1553.0, 475.0]]}, {"text": "UCF101", "confidence": 0.9993445873260498, "text_region": [[1557.0, 472.0], [1690.0, 472.0], [1690.0, 508.0], [1557.0, 508.0]]}, {"text": "HatefulMemes", "confidence": 0.9916839599609375, "text_region": [[1447.0, 505.0], [1690.0, 505.0], [1690.0, 541.0], [1447.0, 541.0]]}, {"text": "-6.7", "confidence": 0.8476485013961792, "text_region": [[1779.0, 508.0], [1846.0, 508.0], [1846.0, 535.0], [1779.0, 535.0]]}, {"text": "CIFAR10", "confidence": 0.9967864155769348, "text_region": [[1540.0, 541.0], [1690.0, 541.0], [1690.0, 574.0], [1540.0, 574.0]]}, {"text": "CIFAR100", "confidence": 0.9965469837188721, "text_region": [[1527.0, 568.0], [1690.0, 568.0], [1690.0, 604.0], [1527.0, 604.0]]}, {"text": "STL10", "confidence": 0.993151068687439, "text_region": [[1580.0, 607.0], [1690.0, 607.0], [1690.0, 640.0], [1580.0, 640.0]]}, {"text": "3.0", "confidence": 0.9859029650688171, "text_region": [[1763.0, 614.0], [1806.0, 614.0], [1806.0, 637.0], [1763.0, 637.0]]}, {"text": "FER2013", "confidence": 0.998874306678772, "text_region": [[1537.0, 640.0], [1686.0, 640.0], [1686.0, 673.0], [1537.0, 673.0]]}, {"text": "-2.8", "confidence": 0.8818612694740295, "text_region": [[1743.0, 644.0], [1806.0, 644.0], [1806.0, 670.0], [1743.0, 670.0]]}, {"text": "Caltech101", "confidence": 0.9933517575263977, "text_region": [[1497.0, 676.0], [1686.0, 676.0], [1686.0, 713.0], [1497.0, 713.0]]}, {"text": "2.0", "confidence": 0.9995622038841248, "text_region": [[1736.0, 680.0], [1799.0, 680.0], [1799.0, 703.0], [1736.0, 703.0]]}, {"text": " ImageNet", "confidence": 0.9784285426139832, "text_region": [[1510.0, 710.0], [1690.0, 710.0], [1690.0, 746.0], [1510.0, 746.0]]}, {"text": "+1.9", "confidence": 0.9969931244850159, "text_region": [[1725.0, 711.0], [1800.0, 702.0], [1804.0, 738.0], [1729.0, 747.0]]}, {"text": "OxfordPets", "confidence": 0.9987580180168152, "text_region": [[1500.0, 742.0], [1690.0, 742.0], [1690.0, 776.0], [1500.0, 776.0]]}, {"text": "+1.1", "confidence": 0.9009652137756348, "text_region": [[1723.0, 746.0], [1789.0, 746.0], [1789.0, 772.0], [1723.0, 772.0]]}, {"text": "PascalVOC2007", "confidence": 0.9429882764816284, "text_region": [[1417.0, 772.0], [1700.0, 772.0], [1700.0, 815.0], [1417.0, 815.0]]}, {"text": "+0.5", "confidence": 0.9991899728775024, "text_region": [[1710.0, 772.0], [1789.0, 772.0], [1789.0, 809.0], [1710.0, 809.0]]}, {"text": "Birdsnap", "confidence": 0.9973622560501099, "text_region": [[1701.0, 798.0], [1860.0, 806.0], [1858.0, 852.0], [1699.0, 844.0]]}, {"text": "3.2", "confidence": 0.9996328353881836, "text_region": [[1603.0, 812.0], [1670.0, 812.0], [1670.0, 838.0], [1603.0, 838.0]]}, {"text": "-10.0", "confidence": 0.9570552706718445, "text_region": [[1510.0, 838.0], [1610.0, 838.0], [1610.0, 874.0], [1510.0, 874.0]]}, {"text": "MNIST", "confidence": 0.9968814849853516, "text_region": [[1706.0, 842.0], [1819.0, 842.0], [1819.0, 878.0], [1706.0, 878.0]]}, {"text": "11.3", "confidence": 0.999641478061676, "text_region": [[1507.0, 878.0], [1580.0, 878.0], [1580.0, 904.0], [1507.0, 904.0]]}, {"text": "FGVCAircraft", "confidence": 0.9993133544921875, "text_region": [[1706.0, 874.0], [1929.0, 874.0], [1929.0, 911.0], [1706.0, 911.0]]}, {"text": "11.9", "confidence": 0.9990831017494202, "text_region": [[1500.0, 911.0], [1583.0, 911.0], [1583.0, 940.0], [1500.0, 940.0]]}, {"text": "RESISC45", "confidence": 0.9993003606796265, "text_region": [[1706.0, 908.0], [1876.0, 908.0], [1876.0, 944.0], [1706.0, 944.0]]}, {"text": "12.5", "confidence": 0.9997468590736389, "text_region": [[1490.0, 944.0], [1573.0, 944.0], [1573.0, 973.0], [1490.0, 973.0]]}, {"text": "Flowers102", "confidence": 0.9701674580574036, "text_region": [[1706.0, 944.0], [1903.0, 944.0], [1903.0, 980.0], [1706.0, 980.0]]}, {"text": "16.6", "confidence": 0.9989544153213501, "text_region": [[1463.0, 980.0], [1530.0, 980.0], [1530.0, 1003.0], [1463.0, 1003.0]]}, {"text": "DTD", "confidence": 0.9986546039581299, "text_region": [[1710.0, 980.0], [1783.0, 980.0], [1783.0, 1007.0], [1710.0, 1007.0]]}, {"text": "CLEVRCounts", "confidence": 0.9993693828582764, "text_region": [[1710.0, 1010.0], [1939.0, 1010.0], [1939.0, 1046.0], [1710.0, 1046.0]]}, {"text": "GTSRB", "confidence": 0.999602198600769, "text_region": [[1710.0, 1043.0], [1826.0, 1043.0], [1826.0, 1079.0], [1710.0, 1079.0]]}, {"text": "PatchCamelyon", "confidence": 0.996337890625, "text_region": [[1704.0, 1069.0], [1976.0, 1076.0], [1975.0, 1119.0], [1703.0, 1112.0]]}, {"text": "-34.0", "confidence": 0.9766028523445129, "text_region": [[1269.0, 1113.0], [1368.0, 1105.0], [1371.0, 1141.0], [1273.0, 1149.0]]}, {"text": "KITTI Distance", "confidence": 0.9641467332839966, "text_region": [[1706.0, 1112.0], [1956.0, 1112.0], [1956.0, 1148.0], [1706.0, 1148.0]]}, {"text": "-37.1", "confidence": 0.9970270991325378, "text_region": [[1237.0, 1145.0], [1334.0, 1145.0], [1334.0, 1178.0], [1237.0, 1178.0]]}, {"text": "EuroSAT", "confidence": 0.9934949278831482, "text_region": [[1703.0, 1142.0], [1859.0, 1142.0], [1859.0, 1188.0], [1703.0, 1188.0]]}, {"text": "-40", "confidence": 0.9909310936927795, "text_region": [[1277.0, 1195.0], [1344.0, 1195.0], [1344.0, 1231.0], [1277.0, 1231.0]]}, {"text": "-30", "confidence": 0.9899383187294006, "text_region": [[1360.0, 1198.0], [1444.0, 1198.0], [1444.0, 1234.0], [1360.0, 1234.0]]}, {"text": "-20", "confidence": 0.9906967282295227, "text_region": [[1460.0, 1198.0], [1543.0, 1198.0], [1543.0, 1234.0], [1460.0, 1234.0]]}, {"text": "-10", "confidence": 0.9957860112190247, "text_region": [[1560.0, 1198.0], [1643.0, 1198.0], [1643.0, 1234.0], [1560.0, 1234.0]]}, {"text": "10", "confidence": 0.9994885325431824, "text_region": [[1773.0, 1195.0], [1829.0, 1195.0], [1829.0, 1234.0], [1773.0, 1234.0]]}, {"text": "20", "confidence": 0.9992847442626953, "text_region": [[1869.0, 1198.0], [1926.0, 1198.0], [1926.0, 1234.0], [1869.0, 1234.0]]}, {"text": "30", "confidence": 0.9997937679290771, "text_region": [[1969.0, 1195.0], [2026.0, 1195.0], [2026.0, 1234.0], [1969.0, 1234.0]]}, {"text": "40", "confidence": 0.9998369216918945, "text_region": [[2069.0, 1195.0], [2119.0, 1195.0], [2119.0, 1234.0], [2069.0, 1234.0]]}, {"text": "\u25b3 Score (%)", "confidence": 0.9125316143035889, "text_region": [[1593.0, 1244.0], [1806.0, 1244.0], [1806.0, 1280.0], [1593.0, 1280.0]]}], "img_idx": 0, "score": 0.9368925094604492}
{"type": "figure_caption", "bbox": [1323, 1284, 2071, 1314], "res": [{"text": "Zero-Shot CLIP vs. Linear Probe on ResNet50", "confidence": 0.9839087128639221, "text_region": [[1317.0, 1277.0], [2079.0, 1277.0], [2079.0, 1323.0], [1317.0, 1323.0]]}], "img_idx": 0, "score": 0.8880148530006409}
{"type": "header", "bbox": [228, 193, 1124, 225], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9977254271507263, "text_region": [[220.0, 188.0], [1370.0, 188.0], [1370.0, 231.0], [220.0, 231.0]]}], "img_idx": 0, "score": 0.7127969861030579}
{"type": "header", "bbox": [2144, 193, 2162, 221], "res": [], "img_idx": 0, "score": 0.6323356628417969}
