{"type": "text", "bbox": [1226, 1020, 2170, 2208], "res": [{"text": "Due to the large size of our pre-training dataset, over-fitting", "confidence": 0.9925627708435059, "text_region": [[1227.0, 1020.0], [2169.0, 1020.0], [2169.0, 1063.0], [1227.0, 1063.0]]}, {"text": "is not a major concern and the details of training CLIP are", "confidence": 0.993208646774292, "text_region": [[1224.0, 1066.0], [2172.0, 1066.0], [2172.0, 1112.0], [1224.0, 1112.0]]}, {"text": "simplified compared to the implementation of Zhang et al.", "confidence": 0.9893366694450378, "text_region": [[1224.0, 1115.0], [2175.0, 1115.0], [2175.0, 1158.0], [1224.0, 1158.0]]}, {"text": "(2020). We train CLIP from scratch without initializing the", "confidence": 0.9906468391418457, "text_region": [[1224.0, 1158.0], [2172.0, 1162.0], [2172.0, 1208.0], [1224.0, 1204.0]]}, {"text": "image encoder with ImageNet weights or the text encoder", "confidence": 0.9885790944099426, "text_region": [[1224.0, 1211.0], [2169.0, 1211.0], [2169.0, 1257.0], [1224.0, 1257.0]]}, {"text": "with pre-trained weights. We do not use the non-linear", "confidence": 0.9828721284866333, "text_region": [[1221.0, 1251.0], [2175.0, 1247.0], [2175.0, 1303.0], [1221.0, 1307.0]]}, {"text": "projection between the representation and the contrastive", "confidence": 0.996871292591095, "text_region": [[1224.0, 1307.0], [2172.0, 1303.0], [2172.0, 1350.0], [1224.0, 1353.0]]}, {"text": "embedding space, a change which was introduced by Bach-", "confidence": 0.9931331872940063, "text_region": [[1224.0, 1353.0], [2172.0, 1353.0], [2172.0, 1399.0], [1224.0, 1399.0]]}, {"text": "man et al. (2019) and popularized by Chen et al. (2020b).", "confidence": 0.9888975024223328, "text_region": [[1221.0, 1396.0], [2178.0, 1393.0], [2179.0, 1449.0], [1221.0, 1452.0]]}, {"text": "We instead use only a linear projection to map from each en-", "confidence": 0.9986832737922668, "text_region": [[1224.0, 1449.0], [2172.0, 1449.0], [2172.0, 1495.0], [1224.0, 1495.0]]}, {"text": " coder's representation to the multi-modal embedding space.", "confidence": 0.9788426756858826, "text_region": [[1221.0, 1492.0], [2172.0, 1492.0], [2172.0, 1548.0], [1221.0, 1548.0]]}, {"text": "We did not notice a difference in training effciency between", "confidence": 0.9686451554298401, "text_region": [[1217.0, 1534.0], [2175.0, 1538.0], [2175.0, 1594.0], [1217.0, 1591.0]]}, {"text": "the two versions and speculate that non-linear projections", "confidence": 0.9986833333969116, "text_region": [[1224.0, 1594.0], [2172.0, 1594.0], [2172.0, 1640.0], [1224.0, 1640.0]]}, {"text": " may be co-adapted with details of current image only in", "confidence": 0.9908162355422974, "text_region": [[1221.0, 1640.0], [2169.0, 1640.0], [2169.0, 1686.0], [1221.0, 1686.0]]}, {"text": "self-supervised representation learning methods. We also", "confidence": 0.9918256998062134, "text_region": [[1227.0, 1690.0], [2172.0, 1690.0], [2172.0, 1736.0], [1227.0, 1736.0]]}, {"text": "remove the text transformation function tu from Zhang et al.", "confidence": 0.9986693263053894, "text_region": [[1227.0, 1736.0], [2172.0, 1736.0], [2172.0, 1782.0], [1227.0, 1782.0]]}, {"text": "(2020) which samples a single sentence at uniform from", "confidence": 0.9917387366294861, "text_region": [[1224.0, 1779.0], [2172.0, 1782.0], [2172.0, 1828.0], [1224.0, 1825.0]]}, {"text": "the text since many of the (image, text) pairs in CLIP's pre-", "confidence": 0.9940124750137329, "text_region": [[1227.0, 1832.0], [2172.0, 1832.0], [2172.0, 1878.0], [1227.0, 1878.0]]}, {"text": "training dataset are only a single sentence. We also simplify", "confidence": 0.9848392605781555, "text_region": [[1227.0, 1878.0], [2169.0, 1878.0], [2169.0, 1924.0], [1227.0, 1924.0]]}, {"text": "the image transformation function t. A random square", "confidence": 0.9989255666732788, "text_region": [[1224.0, 1924.0], [2165.0, 1924.0], [2165.0, 1970.0], [1224.0, 1970.0]]}, {"text": "crop from resized images is the only data augmentation", "confidence": 0.9983193874359131, "text_region": [[1224.0, 1977.0], [2172.0, 1973.0], [2172.0, 2020.0], [1224.0, 2023.0]]}, {"text": "used during training. Finally, the temperature parameter", "confidence": 0.9862940907478333, "text_region": [[1227.0, 2023.0], [2169.0, 2023.0], [2169.0, 2069.0], [1227.0, 2069.0]]}, {"text": "which controls the range of the logits in the softmax, 7, is", "confidence": 0.9901971817016602, "text_region": [[1227.0, 2069.0], [2169.0, 2069.0], [2169.0, 2115.0], [1227.0, 2115.0]]}, {"text": "directly optimized during training as a log-parameterized", "confidence": 0.987517237663269, "text_region": [[1227.0, 2119.0], [2172.0, 2119.0], [2172.0, 2165.0], [1227.0, 2165.0]]}, {"text": "multiplicative scalar to avoid turning as a hyper-parameter.", "confidence": 0.9823582768440247, "text_region": [[1224.0, 2168.0], [2169.0, 2168.0], [2169.0, 2214.0], [1224.0, 2214.0]]}], "img_idx": 0, "score": 0.9964550137519836}
{"type": "text", "bbox": [218, 1604, 1159, 2499], "res": [{"text": " Both these approaches share a key similarity. They try to pre-", "confidence": 0.9861682653427124, "text_region": [[213.0, 1590.0], [1171.0, 1597.0], [1171.0, 1653.0], [213.0, 1647.0]]}, {"text": "dict the exact words of the text accompanying each image.", "confidence": 0.9916185140609741, "text_region": [[216.0, 1643.0], [1164.0, 1650.0], [1164.0, 1696.0], [216.0, 1690.0]]}, {"text": "This is a difficult task due to the wide variety of descriptions,", "confidence": 0.9909771084785461, "text_region": [[213.0, 1689.0], [1164.0, 1696.0], [1164.0, 1743.0], [213.0, 1736.0]]}, {"text": "comments, and related text that co-occur with images. Re-", "confidence": 0.9854586124420166, "text_region": [[216.0, 1746.0], [1164.0, 1746.0], [1164.0, 1789.0], [216.0, 1789.0]]}, {"text": "cent work in contrastive representation learning for images", "confidence": 0.9926072955131531, "text_region": [[216.0, 1785.0], [1161.0, 1789.0], [1161.0, 1845.0], [216.0, 1841.0]]}, {"text": "has found that contrastive objectives can learn better repre.", "confidence": 0.9600885510444641, "text_region": [[213.0, 1828.0], [1161.0, 1835.0], [1161.0, 1891.0], [213.0, 1884.0]]}, {"text": "sentations than their equivalent predictive objective (Tian", "confidence": 0.9945845007896423, "text_region": [[220.0, 1888.0], [1161.0, 1888.0], [1161.0, 1930.0], [220.0, 1930.0]]}, {"text": "et al., 2019). Other work has found that although generative", "confidence": 0.9849086403846741, "text_region": [[220.0, 1934.0], [1161.0, 1934.0], [1161.0, 1980.0], [220.0, 1980.0]]}, {"text": "models of images can learn high quality image representa-", "confidence": 0.9926778078079224, "text_region": [[220.0, 1983.0], [1164.0, 1983.0], [1164.0, 2030.0], [220.0, 2030.0]]}, {"text": "tions, they require over an order of magnitude more compute", "confidence": 0.9826949238777161, "text_region": [[220.0, 2030.0], [1164.0, 2030.0], [1164.0, 2076.0], [220.0, 2076.0]]}, {"text": "than contrastive models with the same performance (Chen", "confidence": 0.9995837211608887, "text_region": [[216.0, 2079.0], [1161.0, 2079.0], [1161.0, 2125.0], [216.0, 2125.0]]}, {"text": "et al., 2020a). Noting these findings, we explored training", "confidence": 0.9829703569412231, "text_region": [[220.0, 2125.0], [1164.0, 2125.0], [1164.0, 2171.0], [220.0, 2171.0]]}, {"text": " a system to solve the potentially easier proxy task of pre-", "confidence": 0.9888811111450195, "text_region": [[213.0, 2175.0], [1167.0, 2175.0], [1167.0, 2221.0], [213.0, 2221.0]]}, {"text": "dicting only which text as a whole is paired with which", "confidence": 0.9837659001350403, "text_region": [[216.0, 2218.0], [1161.0, 2218.0], [1161.0, 2264.0], [216.0, 2264.0]]}, {"text": "image and not the exact words of that text. Starting with", "confidence": 0.9917077422142029, "text_region": [[220.0, 2267.0], [1161.0, 2267.0], [1161.0, 2313.0], [220.0, 2313.0]]}, {"text": "the same bag-of-words encoding baseline, we swapped the", "confidence": 0.9742799997329712, "text_region": [[216.0, 2313.0], [1164.0, 2313.0], [1164.0, 2369.0], [216.0, 2369.0]]}, {"text": "predictive objective for a contrastive objective in Figure 2", "confidence": 0.9863053560256958, "text_region": [[220.0, 2366.0], [1164.0, 2366.0], [1164.0, 2412.0], [220.0, 2412.0]]}, {"text": " and observed a further 4x efficiency improvement in the rate", "confidence": 0.9919984340667725, "text_region": [[213.0, 2406.0], [1168.0, 2409.0], [1167.0, 2465.0], [213.0, 2462.0]]}, {"text": "of zero-shot transfer to ImageNet.", "confidence": 0.9981277585029602, "text_region": [[216.0, 2462.0], [765.0, 2462.0], [765.0, 2508.0], [216.0, 2508.0]]}], "img_idx": 0, "score": 0.9938592910766602}
{"type": "text", "bbox": [1225, 2343, 2168, 2858], "res": [{"text": "We consider two different architectures for the image en-", "confidence": 0.9826740026473999, "text_region": [[1224.0, 2336.0], [2172.0, 2340.0], [2172.0, 2386.0], [1224.0, 2383.0]]}, {"text": "coder. For the first, we use ResNet-50 (He et al., 2016a)", "confidence": 0.9965611696243286, "text_region": [[1227.0, 2386.0], [2172.0, 2386.0], [2172.0, 2432.0], [1227.0, 2432.0]]}, {"text": "as the base architecture for the image encoder due to its", "confidence": 0.9903915524482727, "text_region": [[1224.0, 2439.0], [2172.0, 2439.0], [2172.0, 2485.0], [1224.0, 2485.0]]}, {"text": "widespread adoption and proven performance. We make sev-", "confidence": 0.9919184446334839, "text_region": [[1227.0, 2485.0], [2179.0, 2485.0], [2179.0, 2531.0], [1227.0, 2531.0]]}, {"text": "eral modifications to the original version using the ResNet-", "confidence": 0.9907358884811401, "text_region": [[1224.0, 2534.0], [2172.0, 2534.0], [2172.0, 2577.0], [1224.0, 2577.0]]}, {"text": "D improvements from He et al. (2019) and the antialiased", "confidence": 0.9906342625617981, "text_region": [[1224.0, 2581.0], [2172.0, 2581.0], [2172.0, 2627.0], [1224.0, 2627.0]]}, {"text": "rect-2 blur pooling from Zhang (2019). We also replace", "confidence": 0.9919259548187256, "text_region": [[1224.0, 2627.0], [2169.0, 2627.0], [2169.0, 2673.0], [1224.0, 2673.0]]}, {"text": "the global average pooling layer with an attention pooling", "confidence": 0.9825139045715332, "text_region": [[1221.0, 2673.0], [2175.0, 2673.0], [2175.0, 2729.0], [1221.0, 2729.0]]}, {"text": "mechanism. The attention pooling is implemented as a sin-", "confidence": 0.9965255856513977, "text_region": [[1224.0, 2726.0], [2172.0, 2726.0], [2172.0, 2772.0], [1224.0, 2772.0]]}, {"text": "gle layer of \u201ctransformer-style\u201d multi-head QKV attention", "confidence": 0.9798200726509094, "text_region": [[1227.0, 2772.0], [2172.0, 2772.0], [2172.0, 2818.0], [1227.0, 2818.0]]}, {"text": "where the query is conditioned on the global average-pooled", "confidence": 0.9949285387992859, "text_region": [[1227.0, 2822.0], [2169.0, 2822.0], [2169.0, 2868.0], [1227.0, 2868.0]]}], "img_idx": 0, "score": 0.9935218691825867}
{"type": "text", "bbox": [1227, 281, 2166, 981], "res": [{"text": " multi-modal embedding space by jointly training an image", "confidence": 0.9950066804885864, "text_region": [[1221.0, 274.0], [2165.0, 274.0], [2165.0, 320.0], [1221.0, 320.0]]}, {"text": "encoder and text encoder to maximize the cosine similar-", "confidence": 0.9877599477767944, "text_region": [[1224.0, 327.0], [2172.0, 327.0], [2172.0, 370.0], [1224.0, 370.0]]}, {"text": "ity of the image and text embeddings of the N real pairs", "confidence": 0.9919485449790955, "text_region": [[1224.0, 373.0], [2169.0, 373.0], [2169.0, 419.0], [1224.0, 419.0]]}, {"text": "in the batch while minimizing the cosine similarity of the", "confidence": 0.9979012608528137, "text_region": [[1227.0, 422.0], [2172.0, 422.0], [2172.0, 465.0], [1227.0, 465.0]]}, {"text": "embeddings of the N2 \u2014 N incorrect pairings. We opti-", "confidence": 0.9726476073265076, "text_region": [[1224.0, 462.0], [2179.0, 462.0], [2179.0, 518.0], [1224.0, 518.0]]}, {"text": "mize a symmetric cross entropy loss over these similarity", "confidence": 0.9813140034675598, "text_region": [[1227.0, 515.0], [2169.0, 515.0], [2169.0, 561.0], [1227.0, 561.0]]}, {"text": "scores. In Figure 3 we include pseudocode of the core of an", "confidence": 0.9833184480667114, "text_region": [[1227.0, 564.0], [2169.0, 564.0], [2169.0, 610.0], [1227.0, 610.0]]}, {"text": "implementation of CLIP. To our knowledge this batch con-", "confidence": 0.9922100305557251, "text_region": [[1224.0, 610.0], [2175.0, 610.0], [2175.0, 657.0], [1224.0, 657.0]]}, {"text": "struction technique and objective was first introduced in the", "confidence": 0.9904153943061829, "text_region": [[1224.0, 660.0], [2172.0, 660.0], [2172.0, 706.0], [1224.0, 706.0]]}, {"text": " area of deep metric learning as the multi-class N-pair loss", "confidence": 0.9890152215957642, "text_region": [[1217.0, 703.0], [2172.0, 700.0], [2172.0, 756.0], [1217.0, 759.0]]}, {"text": " Sohn (2016), was popularized for contrastive representation", "confidence": 0.9843378067016602, "text_region": [[1221.0, 746.0], [2172.0, 749.0], [2172.0, 805.0], [1221.0, 802.0]]}, {"text": "learning by Oord et al. (2018) as the InfoNCE loss, and was", "confidence": 0.9902405738830566, "text_region": [[1227.0, 802.0], [2172.0, 802.0], [2172.0, 848.0], [1227.0, 848.0]]}, {"text": "recently adapted for contrastive (text, image) representation", "confidence": 0.9945623278617859, "text_region": [[1224.0, 851.0], [2172.0, 851.0], [2172.0, 898.0], [1224.0, 898.0]]}, {"text": "learning in the domain of medical imaging by Zhang et al.", "confidence": 0.9947000741958618, "text_region": [[1224.0, 894.0], [2175.0, 898.0], [2175.0, 944.0], [1224.0, 940.0]]}, {"text": "(2020).", "confidence": 0.9906883239746094, "text_region": [[1223.0, 936.0], [1345.0, 945.0], [1341.0, 994.0], [1220.0, 986.0]]}], "img_idx": 0, "score": 0.9924226999282837}
{"type": "text", "bbox": [221, 1146, 1158, 1566], "res": [{"text": "Our initial approach, similar to VirTex, jointly trained an", "confidence": 0.9911773800849915, "text_region": [[220.0, 1142.0], [1161.0, 1142.0], [1161.0, 1188.0], [220.0, 1188.0]]}, {"text": "image CNN and text transformer from scratch to predict the", "confidence": 0.9896962642669678, "text_region": [[220.0, 1195.0], [1164.0, 1195.0], [1164.0, 1238.0], [220.0, 1238.0]]}, {"text": "caption of an image. However, we encountered difficulties", "confidence": 0.9957228302955627, "text_region": [[220.0, 1241.0], [1164.0, 1241.0], [1164.0, 1287.0], [220.0, 1287.0]]}, {"text": "efficiently scaling this method. In Figure 2 we show that a", "confidence": 0.9989073276519775, "text_region": [[220.0, 1290.0], [1161.0, 1290.0], [1161.0, 1333.0], [220.0, 1333.0]]}, {"text": "63 million parameter transformer language model, which", "confidence": 0.9706374406814575, "text_region": [[216.0, 1333.0], [1164.0, 1333.0], [1164.0, 1389.0], [216.0, 1389.0]]}, {"text": "already uses twice the compute of its ResNet-50 image", "confidence": 0.9923125505447388, "text_region": [[216.0, 1383.0], [1164.0, 1386.0], [1164.0, 1432.0], [216.0, 1429.0]]}, {"text": "encoder, learns to recognize ImageNet classes three times", "confidence": 0.9954649806022644, "text_region": [[220.0, 1432.0], [1161.0, 1432.0], [1161.0, 1478.0], [220.0, 1478.0]]}, {"text": "slower than a much simpler baseline that predicts a bag-of-", "confidence": 0.9921376705169678, "text_region": [[216.0, 1482.0], [1167.0, 1482.0], [1167.0, 1528.0], [216.0, 1528.0]]}, {"text": "words encoding of the same text.", "confidence": 0.9839849472045898, "text_region": [[216.0, 1524.0], [749.0, 1528.0], [748.0, 1574.0], [216.0, 1571.0]]}], "img_idx": 0, "score": 0.9904099702835083}
{"type": "text", "bbox": [219, 601, 1160, 1112], "res": [{"text": "State-of-the-art computer vision systems use very large", "confidence": 0.9937248826026917, "text_region": [[216.0, 591.0], [1161.0, 594.0], [1161.0, 640.0], [216.0, 637.0]]}, {"text": "amounts of compute. Mahajan et al. (2018) required 19", "confidence": 0.9960218667984009, "text_region": [[216.0, 644.0], [1161.0, 644.0], [1161.0, 690.0], [216.0, 690.0]]}, {"text": "GPU years to train their ResNeXt101-32x48d and Xie et al.", "confidence": 0.9935820698738098, "text_region": [[216.0, 690.0], [1167.0, 690.0], [1167.0, 736.0], [216.0, 736.0]]}, {"text": "(2020) required 33 TPUv3 core-years to train their Noisy", "confidence": 0.9947296380996704, "text_region": [[220.0, 739.0], [1164.0, 739.0], [1164.0, 785.0], [220.0, 785.0]]}, {"text": "Student EffcientNet-L2. When considering that both these", "confidence": 0.9872421622276306, "text_region": [[220.0, 785.0], [1161.0, 785.0], [1161.0, 832.0], [220.0, 832.0]]}, {"text": "systems were trained to predict only 1000 ImageNet classes,", "confidence": 0.9965009093284607, "text_region": [[220.0, 835.0], [1161.0, 835.0], [1161.0, 881.0], [220.0, 881.0]]}, {"text": "the task of learning an open set of visual concepts from", "confidence": 0.9961055517196655, "text_region": [[220.0, 884.0], [1161.0, 884.0], [1161.0, 931.0], [220.0, 931.0]]}, {"text": "natural language seems daunting. In the course of our ef-", "confidence": 0.9916772842407227, "text_region": [[220.0, 927.0], [1167.0, 927.0], [1167.0, 973.0], [220.0, 973.0]]}, {"text": "forts, we found training efficiency was key to successfully", "confidence": 0.9825425744056702, "text_region": [[216.0, 973.0], [1164.0, 977.0], [1164.0, 1023.0], [216.0, 1020.0]]}, {"text": "scaling natural language supervision and we selected our", "confidence": 0.9827771782875061, "text_region": [[216.0, 1023.0], [1167.0, 1023.0], [1167.0, 1079.0], [216.0, 1079.0]]}, {"text": "final pre-training method based on this metric.", "confidence": 0.999578595161438, "text_region": [[220.0, 1076.0], [961.0, 1076.0], [961.0, 1119.0], [220.0, 1119.0]]}], "img_idx": 0, "score": 0.9888043999671936}
{"type": "text", "bbox": [220, 2533, 1159, 2817], "res": [{"text": "Given a batch of N (image, text) pairs, CLIP is trained to", "confidence": 0.9893287420272827, "text_region": [[213.0, 2524.0], [1164.0, 2528.0], [1164.0, 2584.0], [213.0, 2581.0]]}, {"text": "predict which of the N \u00d7 N possible (image, text) pairings", "confidence": 0.9687488079071045, "text_region": [[213.0, 2574.0], [1164.0, 2577.0], [1164.0, 2633.0], [213.0, 2630.0]]}, {"text": "across a batch actually occurred. To do this, CLIP learns a", "confidence": 0.9989647269248962, "text_region": [[220.0, 2630.0], [1164.0, 2630.0], [1164.0, 2673.0], [220.0, 2673.0]]}, {"text": "with high pointwise mutual information as well as the names of", "confidence": 0.9880595803260803, "text_region": [[220.0, 2699.0], [1164.0, 2699.0], [1164.0, 2746.0], [220.0, 2746.0]]}, {"text": "all Wikipedia articles above a certain search volume. Finally all", "confidence": 0.9950578212738037, "text_region": [[216.0, 2736.0], [1164.0, 2739.0], [1164.0, 2785.0], [216.0, 2782.0]]}, {"text": "WordNet synsets not already in the query list are added.", "confidence": 0.9783056378364563, "text_region": [[210.0, 2772.0], [1025.0, 2775.0], [1024.0, 2831.0], [209.0, 2828.0]]}], "img_idx": 0, "score": 0.9860897660255432}
{"type": "text", "bbox": [221, 280, 1158, 457], "res": [{"text": "balance the results by including up to 20,000 (image, text)", "confidence": 0.9908943772315979, "text_region": [[220.0, 277.0], [1167.0, 277.0], [1167.0, 323.0], [220.0, 323.0]]}, {"text": "pairs per query. The resulting dataset has a similar total", "confidence": 0.9918830394744873, "text_region": [[213.0, 320.0], [1167.0, 317.0], [1168.0, 373.0], [213.0, 376.0]]}, {"text": "word count as the WebText dataset used to train GPT-2. We", "confidence": 0.9988400936126709, "text_region": [[220.0, 373.0], [1164.0, 373.0], [1164.0, 416.0], [220.0, 416.0]]}, {"text": "refer to this dataset as WIT for WebImageText.", "confidence": 0.9832211136817932, "text_region": [[220.0, 422.0], [971.0, 422.0], [971.0, 465.0], [220.0, 465.0]]}], "img_idx": 0, "score": 0.985670268535614}
{"type": "title", "bbox": [222, 523, 1021, 558], "res": [{"text": "2.3. Selecting an Efficient Pre-Training Method", "confidence": 0.9959858655929565, "text_region": [[220.0, 521.0], [1024.0, 521.0], [1024.0, 568.0], [220.0, 568.0]]}], "img_idx": 0, "score": 0.9471483826637268}
{"type": "title", "bbox": [1273, 2269, 1722, 2304], "res": [{"text": " 2.4. Choosing and Scaling a Model", "confidence": 0.9772557616233826, "text_region": [[1221.0, 2261.0], [1822.0, 2257.0], [1823.0, 2313.0], [1221.0, 2317.0]]}], "img_idx": 0, "score": 0.7480485439300537}
{"type": "header", "bbox": [2144, 193, 2165, 221], "res": [], "img_idx": 0, "score": 0.7118436098098755}
{"type": "header", "bbox": [292, 193, 1189, 225], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9835689663887024, "text_region": [[220.0, 188.0], [1367.0, 188.0], [1367.0, 234.0], [220.0, 234.0]]}], "img_idx": 0, "score": 0.7071537971496582}
