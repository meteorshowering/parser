{"type": "text", "bbox": [1227, 836, 2172, 2632], "res": [{"text": "We train a series of 5 ResNets and 3 Vision Transformers.", "confidence": 0.9807071089744568, "text_region": [[1224.0, 832.0], [2169.0, 832.0], [2169.0, 878.0], [1224.0, 878.0]]}, {"text": "For the ResNets we train a ResNet-50, a ResNet-101, and", "confidence": 0.9960203170776367, "text_region": [[1224.0, 881.0], [2172.0, 881.0], [2172.0, 927.0], [1224.0, 927.0]]}, {"text": " then 3 more which follow EfficientNet-style model scaling", "confidence": 0.9815869331359863, "text_region": [[1214.0, 921.0], [2175.0, 924.0], [2175.0, 980.0], [1214.0, 977.0]]}, {"text": "and use approximately 4x, 16x, and 64x the compute of a", "confidence": 0.9926349520683289, "text_region": [[1224.0, 977.0], [2172.0, 977.0], [2172.0, 1023.0], [1224.0, 1023.0]]}, {"text": " ResNet-50. They are denoted as RN50x4, RN50x16, and", "confidence": 0.9881532192230225, "text_region": [[1221.0, 1020.0], [2169.0, 1020.0], [2169.0, 1066.0], [1221.0, 1066.0]]}, {"text": "RN50x64 respectively. For the Vision Transformers we", "confidence": 0.9904342889785767, "text_region": [[1227.0, 1072.0], [2169.0, 1072.0], [2169.0, 1119.0], [1227.0, 1119.0]]}, {"text": "train a ViT-B/32, a ViT-B/16, and a ViT-L/14. We train all", "confidence": 0.9954068660736084, "text_region": [[1227.0, 1119.0], [2172.0, 1119.0], [2172.0, 1165.0], [1227.0, 1165.0]]}, {"text": "models for 32 epochs. We use the Adam optimizer (Kingma", "confidence": 0.9943898320198059, "text_region": [[1227.0, 1168.0], [2172.0, 1168.0], [2172.0, 1214.0], [1227.0, 1214.0]]}, {"text": " & Ba, 2014) with decoupled weight decay regularization", "confidence": 0.9839303493499756, "text_region": [[1217.0, 1208.0], [2175.0, 1211.0], [2175.0, 1267.0], [1217.0, 1264.0]]}, {"text": "(Loshchilov & Hutter, 2017) applied to all weights that are", "confidence": 0.9955528378486633, "text_region": [[1227.0, 1264.0], [2172.0, 1264.0], [2172.0, 1310.0], [1227.0, 1310.0]]}, {"text": "not gains or biases, and decay the learning rate using a", "confidence": 0.9912238717079163, "text_region": [[1227.0, 1313.0], [2172.0, 1313.0], [2172.0, 1360.0], [1227.0, 1360.0]]}, {"text": "cosine schedule (Loshchilov & Hutter, 2016). Initial hyper-", "confidence": 0.9954164028167725, "text_region": [[1224.0, 1356.0], [2172.0, 1360.0], [2172.0, 1406.0], [1224.0, 1402.0]]}, {"text": " parameters were set using a combination of grid searches,", "confidence": 0.9945905804634094, "text_region": [[1217.0, 1403.0], [2175.0, 1399.0], [2175.0, 1455.0], [1217.0, 1459.0]]}, {"text": "random search, and manual tuning on the baseline ResNet-", "confidence": 0.9957934617996216, "text_region": [[1224.0, 1455.0], [2175.0, 1455.0], [2175.0, 1502.0], [1224.0, 1502.0]]}, {"text": " 50 model when trained for 1 epoch. Hyper-parameters were", "confidence": 0.9949777722358704, "text_region": [[1221.0, 1498.0], [2172.0, 1502.0], [2172.0, 1548.0], [1221.0, 1544.0]]}, {"text": "then adapted heuristically for larger models due to compu-", "confidence": 0.9910846948623657, "text_region": [[1224.0, 1551.0], [2172.0, 1551.0], [2172.0, 1597.0], [1224.0, 1597.0]]}, {"text": "tational constraints. The learnable temperature parameter", "confidence": 0.9911224842071533, "text_region": [[1221.0, 1587.0], [2175.0, 1597.0], [2175.0, 1650.0], [1220.0, 1640.0]]}, {"text": "T was initialized to the equivalent of O.07 from (Wu et al.,", "confidence": 0.96830153465271, "text_region": [[1227.0, 1647.0], [2172.0, 1647.0], [2172.0, 1693.0], [1227.0, 1693.0]]}, {"text": " 2018) and clipped to prevent scaling the logits by more", "confidence": 0.9882504343986511, "text_region": [[1221.0, 1686.0], [2172.0, 1690.0], [2172.0, 1746.0], [1221.0, 1742.0]]}, {"text": "than 100 which we found necessary to prevent training in-", "confidence": 0.9969200491905212, "text_region": [[1224.0, 1742.0], [2172.0, 1742.0], [2172.0, 1789.0], [1224.0, 1789.0]]}, {"text": "stability.", "confidence": 0.9976514577865601, "text_region": [[1225.0, 1781.0], [1408.0, 1789.0], [1406.0, 1835.0], [1223.0, 1828.0]]}, {"text": "We use a very large minibatch size of 32,768.", "confidence": 0.9992950558662415, "text_region": [[1384.0, 1785.0], [2175.0, 1785.0], [2175.0, 1841.0], [1384.0, 1841.0]]}, {"text": "Mixed-precision (Micikevicius et al., 2017) was used to ac-", "confidence": 0.9983286261558533, "text_region": [[1227.0, 1838.0], [2172.0, 1838.0], [2172.0, 1881.0], [1227.0, 1881.0]]}, {"text": "celerate training and save memory. To save additional mem-", "confidence": 0.9989898204803467, "text_region": [[1227.0, 1884.0], [2179.0, 1884.0], [2179.0, 1930.0], [1227.0, 1930.0]]}, {"text": " ory, gradient checkpointing (Griewank & Walther, 2000;", "confidence": 0.9858864545822144, "text_region": [[1221.0, 1931.0], [2178.0, 1924.0], [2179.0, 1980.0], [1221.0, 1987.0]]}, {"text": "Chen et al., 2016), half-precision Adam statistics (Dhariwal", "confidence": 0.9964139461517334, "text_region": [[1227.0, 1980.0], [2172.0, 1980.0], [2172.0, 2026.0], [1227.0, 2026.0]]}, {"text": "et al., 2020), and half-precision stochastically rounded text", "confidence": 0.9827020764350891, "text_region": [[1224.0, 2030.0], [2172.0, 2030.0], [2172.0, 2076.0], [1224.0, 2076.0]]}, {"text": "encoder weights were used. The calculation of embedding", "confidence": 0.9982706308364868, "text_region": [[1224.0, 2076.0], [2169.0, 2076.0], [2169.0, 2122.0], [1224.0, 2122.0]]}, {"text": " similarities was also sharded with individual GPUs comput-", "confidence": 0.9853934645652771, "text_region": [[1221.0, 2122.0], [2175.0, 2125.0], [2175.0, 2171.0], [1221.0, 2168.0]]}, {"text": " ing only the subset of the pairwise similarities necessary for", "confidence": 0.9898374676704407, "text_region": [[1217.0, 2168.0], [2175.0, 2165.0], [2175.0, 2221.0], [1217.0, 2224.0]]}, {"text": "their local batch of embeddings. The largest ResNet model,", "confidence": 0.9933345913887024, "text_region": [[1221.0, 2218.0], [2172.0, 2218.0], [2172.0, 2264.0], [1221.0, 2264.0]]}, {"text": "RN50x64, took 18 days to train on 592 V100 GPUs while", "confidence": 0.9876337051391602, "text_region": [[1224.0, 2264.0], [2169.0, 2264.0], [2169.0, 2310.0], [1224.0, 2310.0]]}, {"text": "the largest Vision Transformer took 12 days on 256 V100", "confidence": 0.9925764203071594, "text_region": [[1224.0, 2317.0], [2172.0, 2317.0], [2172.0, 2363.0], [1224.0, 2363.0]]}, {"text": "GPUs. For the ViT-L/14 we also pre-train at a higher 336", "confidence": 0.9786133766174316, "text_region": [[1227.0, 2363.0], [2172.0, 2363.0], [2172.0, 2409.0], [1227.0, 2409.0]]}, {"text": "pixel resolution for one additional epoch to boost perfor-", "confidence": 0.9992232322692871, "text_region": [[1227.0, 2412.0], [2172.0, 2412.0], [2172.0, 2458.0], [1227.0, 2458.0]]}, {"text": "mance similar to FixRes (Touvron et al., 2019). We denote", "confidence": 0.994229793548584, "text_region": [[1224.0, 2458.0], [2169.0, 2458.0], [2169.0, 2505.0], [1224.0, 2505.0]]}, {"text": "this model as ViT-L/14@336px. Unless otherwise specified,", "confidence": 0.997839093208313, "text_region": [[1224.0, 2508.0], [2175.0, 2508.0], [2175.0, 2554.0], [1224.0, 2554.0]]}, {"text": "all results reported in this paper as \u201cCLIP\" use this model", "confidence": 0.9733741879463196, "text_region": [[1221.0, 2551.0], [2175.0, 2551.0], [2175.0, 2607.0], [1221.0, 2607.0]]}, {"text": "which we found to perform best.", "confidence": 0.9828833341598511, "text_region": [[1221.0, 2600.0], [1753.0, 2604.0], [1753.0, 2650.0], [1221.0, 2647.0]]}], "img_idx": 0, "score": 0.995974600315094}
{"type": "text", "bbox": [1227, 280, 2169, 699], "res": [{"text": " one dimension of the model. While Tan & Le (2019) tune", "confidence": 0.9855768084526062, "text_region": [[1221.0, 271.0], [2172.0, 274.0], [2172.0, 320.0], [1221.0, 317.0]]}, {"text": "the ratio of compute allocated to each dimension for their", "confidence": 0.9946697354316711, "text_region": [[1224.0, 323.0], [2169.0, 323.0], [2169.0, 370.0], [1224.0, 370.0]]}, {"text": "EfficientNet architecture, we use a simple baseline of allo-", "confidence": 0.9877030849456787, "text_region": [[1227.0, 373.0], [2175.0, 373.0], [2175.0, 419.0], [1227.0, 419.0]]}, {"text": "cating additional compute equally to increasing the width,", "confidence": 0.9867444038391113, "text_region": [[1224.0, 419.0], [2169.0, 419.0], [2169.0, 462.0], [1224.0, 462.0]]}, {"text": "depth, and resolution of the model. For the text encoder, we", "confidence": 0.9959415197372437, "text_region": [[1227.0, 465.0], [2169.0, 465.0], [2169.0, 511.0], [1227.0, 511.0]]}, {"text": "only scale the width of the model to be proportional to the", "confidence": 0.9869080781936646, "text_region": [[1227.0, 515.0], [2172.0, 515.0], [2172.0, 561.0], [1227.0, 561.0]]}, {"text": "calculated increase in width of the ResNet and do not scale", "confidence": 0.9930511116981506, "text_region": [[1227.0, 561.0], [2169.0, 561.0], [2169.0, 607.0], [1227.0, 607.0]]}, {"text": "the depth at all, as we found CLIP's performance to be less", "confidence": 0.9866251349449158, "text_region": [[1224.0, 610.0], [2172.0, 610.0], [2172.0, 657.0], [1224.0, 657.0]]}, {"text": "sensitive to the capacity of the text encoder.", "confidence": 0.9913617968559265, "text_region": [[1224.0, 660.0], [1929.0, 660.0], [1929.0, 706.0], [1224.0, 706.0]]}], "img_idx": 0, "score": 0.9944283366203308}
{"type": "text", "bbox": [220, 1397, 1159, 1717], "res": [{"text": "representation of the image. For the second architecture, we", "confidence": 0.9762593507766724, "text_region": [[213.0, 1389.0], [1164.0, 1386.0], [1164.0, 1442.0], [213.0, 1445.0]]}, {"text": " experiment with the recently introduced Vision Transformer", "confidence": 0.9954007863998413, "text_region": [[209.0, 1436.0], [1167.0, 1432.0], [1168.0, 1488.0], [210.0, 1492.0]]}, {"text": "(ViT) (Dosovitskiy et al., 2020). We closely follow their", "confidence": 0.989190936088562, "text_region": [[216.0, 1488.0], [1164.0, 1488.0], [1164.0, 1534.0], [216.0, 1534.0]]}, {"text": "implementation with only the minor modification of adding", "confidence": 0.9950679540634155, "text_region": [[213.0, 1534.0], [1164.0, 1538.0], [1164.0, 1584.0], [213.0, 1581.0]]}, {"text": "an additional layer normalization to the combined patch", "confidence": 0.9848036170005798, "text_region": [[216.0, 1584.0], [1164.0, 1584.0], [1164.0, 1630.0], [216.0, 1630.0]]}, {"text": "and position embeddings before the transformer and use a", "confidence": 0.994580864906311, "text_region": [[220.0, 1634.0], [1164.0, 1634.0], [1164.0, 1680.0], [220.0, 1680.0]]}, {"text": "slightly different initialization scheme.", "confidence": 0.982585072517395, "text_region": [[216.0, 1680.0], [838.0, 1680.0], [838.0, 1726.0], [216.0, 1726.0]]}], "img_idx": 0, "score": 0.9916602373123169}
{"type": "text", "bbox": [219, 1759, 1159, 2550], "res": [{"text": "The text encoder is a Transformer (Vaswani et al., 2017)", "confidence": 0.9908318519592285, "text_region": [[216.0, 1752.0], [1164.0, 1752.0], [1164.0, 1795.0], [216.0, 1795.0]]}, {"text": "with the architecture modifications described in Radford", "confidence": 0.9946680665016174, "text_region": [[213.0, 1799.0], [1164.0, 1795.0], [1164.0, 1841.0], [213.0, 1845.0]]}, {"text": "et al. (2019). As a base size we use a 63M-parameter 12-", "confidence": 0.9982795715332031, "text_region": [[220.0, 1848.0], [1161.0, 1848.0], [1161.0, 1894.0], [220.0, 1894.0]]}, {"text": "layer 512-wide model with 8 attention heads. The trans-", "confidence": 0.9884609580039978, "text_region": [[220.0, 1894.0], [1164.0, 1894.0], [1164.0, 1940.0], [220.0, 1940.0]]}, {"text": "former operates on a lower-cased byte pair encoding (BPE)", "confidence": 0.9927573800086975, "text_region": [[216.0, 1940.0], [1161.0, 1940.0], [1161.0, 1987.0], [216.0, 1987.0]]}, {"text": "representation of the text with a 49,152 vocab size (Sen-", "confidence": 0.9957365989685059, "text_region": [[216.0, 1990.0], [1167.0, 1987.0], [1168.0, 2033.0], [216.0, 2036.0]]}, {"text": " nrich et al., 2015). For computational efficiency, the max", "confidence": 0.9799432754516602, "text_region": [[210.0, 2029.0], [1168.0, 2033.0], [1167.0, 2089.0], [209.0, 2086.0]]}, {"text": "sequence length was capped at 76. The text sequence is", "confidence": 0.9779875874519348, "text_region": [[213.0, 2082.0], [1167.0, 2082.0], [1167.0, 2138.0], [213.0, 2138.0]]}, {"text": "bracketed with [SOS] and [EOS] tokens and the activa-", "confidence": 0.9417087435722351, "text_region": [[220.0, 2135.0], [1161.0, 2135.0], [1161.0, 2181.0], [220.0, 2181.0]]}, {"text": "tions of the highest layer of the transformer at the [EOS \uff3d", "confidence": 0.9890435934066772, "text_region": [[216.0, 2178.0], [1154.0, 2178.0], [1154.0, 2224.0], [216.0, 2224.0]]}, {"text": "token are treated as the feature representation of the text", "confidence": 0.99483323097229, "text_region": [[220.0, 2231.0], [1164.0, 2231.0], [1164.0, 2277.0], [220.0, 2277.0]]}, {"text": "which is layer normalized and then linearly projected into", "confidence": 0.994565486907959, "text_region": [[213.0, 2267.0], [1168.0, 2274.0], [1167.0, 2330.0], [213.0, 2323.0]]}, {"text": "the multi-modal embedding space. Masked self-attention", "confidence": 0.9945064187049866, "text_region": [[216.0, 2326.0], [1164.0, 2326.0], [1164.0, 2373.0], [216.0, 2373.0]]}, {"text": "was used in the text encoder to preserve the ability to ini-", "confidence": 0.9881193041801453, "text_region": [[220.0, 2376.0], [1164.0, 2376.0], [1164.0, 2422.0], [220.0, 2422.0]]}, {"text": "tialize with a pre-trained language model or add language", "confidence": 0.9937083125114441, "text_region": [[220.0, 2422.0], [1157.0, 2422.0], [1157.0, 2468.0], [220.0, 2468.0]]}, {"text": "modeling as an auxiliary objective, though exploration of", "confidence": 0.9892996549606323, "text_region": [[220.0, 2468.0], [1164.0, 2468.0], [1164.0, 2515.0], [220.0, 2515.0]]}, {"text": "this is left as future work.", "confidence": 0.9981258511543274, "text_region": [[220.0, 2515.0], [629.0, 2515.0], [629.0, 2561.0], [220.0, 2561.0]]}], "img_idx": 0, "score": 0.9877033829689026}
{"type": "text", "bbox": [219, 2592, 1161, 2868], "res": [{"text": " While previous computer vision research has often scaled", "confidence": 0.9633431434631348, "text_region": [[210.0, 2581.0], [1168.0, 2584.0], [1167.0, 2640.0], [209.0, 2637.0]]}, {"text": "models by increasing the width (Mahajan et al., 2018) or", "confidence": 0.9914433360099792, "text_region": [[220.0, 2637.0], [1164.0, 2637.0], [1164.0, 2683.0], [220.0, 2683.0]]}, {"text": " depth (He et al., 2016a) in isolation, for the ResNet image", "confidence": 0.9817546010017395, "text_region": [[213.0, 2676.0], [1164.0, 2680.0], [1164.0, 2736.0], [213.0, 2732.0]]}, {"text": " encoders we adapt the approach of Tan & Le (2019) which", "confidence": 0.9830552339553833, "text_region": [[213.0, 2736.0], [1161.0, 2729.0], [1161.0, 2772.0], [213.0, 2779.0]]}, {"text": "found that allocating additional compute across all of width,", "confidence": 0.9915891885757446, "text_region": [[213.0, 2769.0], [1168.0, 2775.0], [1167.0, 2832.0], [213.0, 2825.0]]}, {"text": "depth, and resolution outperforms only allocating it to only", "confidence": 0.9986733198165894, "text_region": [[216.0, 2828.0], [1161.0, 2828.0], [1161.0, 2874.0], [216.0, 2874.0]]}], "img_idx": 0, "score": 0.9849621057510376}
{"type": "text", "bbox": [220, 1192, 1163, 1265], "res": [{"text": "Figure 3. Numpy-like pseudocode for the core of an implementa-", "confidence": 0.9920401573181152, "text_region": [[216.0, 1188.0], [1164.0, 1188.0], [1164.0, 1231.0], [216.0, 1231.0]]}, {"text": "tion of CLIP.", "confidence": 0.9973616003990173, "text_region": [[216.0, 1231.0], [406.0, 1231.0], [406.0, 1267.0], [216.0, 1267.0]]}], "img_idx": 0, "score": 0.9632349014282227}
{"type": "text", "bbox": [222, 959, 1159, 1135], "res": [{"text": "# symmetric loss function", "confidence": 0.9983764886856079, "text_region": [[226.0, 957.0], [688.0, 957.0], [688.0, 990.0], [226.0, 990.0]]}, {"text": "labels = np.arange(n)", "confidence": 0.9990794062614441, "text_region": [[223.0, 990.0], [612.0, 990.0], [612.0, 1033.0], [223.0, 1033.0]]}, {"text": "loss_i = cross_entropy_loss(logits, labels, axis=0)", "confidence": 0.9873304963111877, "text_region": [[223.0, 1023.0], [1164.0, 1023.0], [1164.0, 1069.0], [223.0, 1069.0]]}, {"text": "loss_t = cross_entropy_loss(logits, labels, axis=1)", "confidence": 0.9845318794250488, "text_region": [[220.0, 1063.0], [1167.0, 1063.0], [1167.0, 1106.0], [220.0, 1106.0]]}, {"text": "loss", "confidence": 0.996539294719696, "text_region": [[223.0, 1102.0], [306.0, 1102.0], [306.0, 1138.0], [223.0, 1138.0]]}, {"text": "=(loss_i + loss_t)/2", "confidence": 0.9418076276779175, "text_region": [[339.0, 1099.0], [745.0, 1099.0], [745.0, 1142.0], [339.0, 1142.0]]}], "img_idx": 0, "score": 0.9620075821876526}
{"type": "text", "bbox": [228, 714, 1034, 919], "res": [{"text": "# joint multimodal embedding [n, d-e]", "confidence": 0.9681604504585266, "text_region": [[220.0, 700.0], [908.0, 700.0], [908.0, 742.0], [220.0, 742.0]]}, {"text": "I-e = l2_normalize(np.dot(I-f, W_i), axis=1)", "confidence": 0.9550679922103882, "text_region": [[220.0, 736.0], [1034.0, 736.0], [1034.0, 779.0], [220.0, 779.0]]}, {"text": "T-e = l2_normalize(np.dot(T_f, W_t), axis=1)", "confidence": 0.9579410552978516, "text_region": [[216.0, 766.0], [1038.0, 769.0], [1038.0, 815.0], [216.0, 812.0]]}, {"text": "# scaled pairwise cosine similarities [n, n]", "confidence": 0.9768285155296326, "text_region": [[216.0, 842.0], [1038.0, 838.0], [1038.0, 884.0], [216.0, 888.0]]}, {"text": "logits = np.dot(I-e, T-e.T) * np.exp(t)", "confidence": 0.9675208926200867, "text_region": [[220.0, 881.0], [945.0, 881.0], [945.0, 924.0], [220.0, 924.0]]}], "img_idx": 0, "score": 0.7054675221443176}
{"type": "title", "bbox": [1228, 764, 1448, 795], "res": [{"text": "2.5. Training", "confidence": 0.996728777885437, "text_region": [[1222.0, 752.0], [1454.0, 759.0], [1453.0, 806.0], [1220.0, 798.0]]}], "img_idx": 0, "score": 0.9557676315307617}
{"type": "figure", "bbox": [212, 262, 1116, 529], "res": [{"text": "# image_encoder \uff0d F", "confidence": 0.9304526448249817, "text_region": [[223.0, 271.0], [565.0, 271.0], [565.0, 304.0], [223.0, 304.0]]}, {"text": "ResNet or Vision Transformer", "confidence": 0.9954022169113159, "text_region": [[549.0, 264.0], [1071.0, 264.0], [1071.0, 307.0], [549.0, 307.0]]}, {"text": "# text-encoder", "confidence": 0.9649999737739563, "text_region": [[220.0, 300.0], [489.0, 300.0], [489.0, 343.0], [220.0, 343.0]]}, {"text": "CBOW or Text Transformer", "confidence": 0.998812198638916, "text_region": [[549.0, 300.0], [1001.0, 300.0], [1001.0, 343.0], [549.0, 343.0]]}, {"text": "# I[n, h, w, c] ", "confidence": 0.8601595163345337, "text_region": [[217.0, 333.0], [509.0, 337.0], [509.0, 383.0], [216.0, 379.0]]}, {"text": "minibatch of aligned images", "confidence": 0.9875908493995667, "text_region": [[549.0, 337.0], [1058.0, 337.0], [1058.0, 380.0], [549.0, 380.0]]}, {"text": "# T[n, 1]", "confidence": 0.9031333923339844, "text_region": [[219.0, 374.0], [391.0, 366.0], [393.0, 412.0], [221.0, 420.0]]}, {"text": "minibatch of aligned texts", "confidence": 0.9988099336624146, "text_region": [[549.0, 373.0], [1044.0, 373.0], [1044.0, 419.0], [549.0, 419.0]]}, {"text": "# W-i[d-i, d-e]", "confidence": 0.9121376872062683, "text_region": [[220.0, 409.0], [506.0, 409.0], [506.0, 452.0], [220.0, 452.0]]}, {"text": "learned proj of image to embed", "confidence": 0.9970347881317139, "text_region": [[549.0, 409.0], [1114.0, 409.0], [1114.0, 455.0], [549.0, 455.0]]}, {"text": "learned proj of text to embed", "confidence": 0.9807748198509216, "text_region": [[545.0, 442.0], [1094.0, 442.0], [1094.0, 488.0], [545.0, 488.0]]}, {"text": "# W_t[d_t, d_e]", "confidence": 0.9437757134437561, "text_region": [[223.0, 452.0], [499.0, 452.0], [499.0, 485.0], [223.0, 485.0]]}, {"text": " learned temperature parameter", "confidence": 0.9882618188858032, "text_region": [[546.0, 478.0], [1088.0, 482.0], [1087.0, 528.0], [545.0, 525.0]]}, {"text": "#t", "confidence": 0.9343284368515015, "text_region": [[226.0, 488.0], [283.0, 488.0], [283.0, 515.0], [226.0, 515.0]]}], "img_idx": 0, "score": 0.9007798433303833}
{"type": "figure_caption", "bbox": [228, 559, 1125, 663], "res": [{"text": "# extract feature representations of each modality", "confidence": 0.9949997663497925, "text_region": [[220.0, 554.0], [1144.0, 554.0], [1144.0, 597.0], [220.0, 597.0]]}, {"text": "I-f = image_encoder(I) #[n, d-i]", "confidence": 0.9848881959915161, "text_region": [[220.0, 591.0], [812.0, 591.0], [812.0, 637.0], [220.0, 637.0]]}, {"text": "T_f = text_encoder(T) #[n, d-t]", "confidence": 0.9311265349388123, "text_region": [[220.0, 627.0], [808.0, 627.0], [808.0, 670.0], [220.0, 670.0]]}], "img_idx": 0, "score": 0.8660747408866882}
{"type": "header", "bbox": [228, 193, 1124, 225], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9977254271507263, "text_region": [[220.0, 188.0], [1370.0, 188.0], [1370.0, 231.0], [220.0, 231.0]]}], "img_idx": 0, "score": 0.6677978038787842}
{"type": "header", "bbox": [2145, 193, 2162, 221], "res": [], "img_idx": 0, "score": 0.6471182107925415}
