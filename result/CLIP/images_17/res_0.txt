{"type": "text", "bbox": [219, 2403, 1160, 2861], "res": [{"text": "A summary of this analysis is presented in Figure 17. Out", "confidence": 0.9928431510925293, "text_region": [[216.0, 2392.0], [1157.0, 2392.0], [1157.0, 2439.0], [216.0, 2439.0]]}, {"text": " of 35 datasets studied, 9 datasets have no detected overlap", "confidence": 0.994214653968811, "text_region": [[213.0, 2442.0], [1161.0, 2445.0], [1161.0, 2492.0], [213.0, 2488.0]]}, {"text": "at all. Most of these datasets are synthetic or specialized", "confidence": 0.989644467830658, "text_region": [[216.0, 2495.0], [1161.0, 2495.0], [1161.0, 2541.0], [216.0, 2541.0]]}, {"text": "making them unlikely to be posted as normal images on", "confidence": 0.9991018772125244, "text_region": [[216.0, 2541.0], [1161.0, 2541.0], [1161.0, 2587.0], [216.0, 2587.0]]}, {"text": "the internet (for instance MNIST, CLEVR, and GTSRB) or", "confidence": 0.9934610724449158, "text_region": [[216.0, 2587.0], [1161.0, 2587.0], [1161.0, 2633.0], [216.0, 2633.0]]}, {"text": "are guaranteed to have no overlap due to containing novel", "confidence": 0.9905235767364502, "text_region": [[220.0, 2640.0], [1161.0, 2640.0], [1161.0, 2686.0], [220.0, 2686.0]]}, {"text": "data from after the date our dataset was created (ObjectNet", "confidence": 0.990502655506134, "text_region": [[216.0, 2680.0], [1161.0, 2683.0], [1161.0, 2729.0], [216.0, 2726.0]]}, {"text": "and Hateful Memes). This demonstrates our detector has", "confidence": 0.9986246824264526, "text_region": [[216.0, 2732.0], [1161.0, 2732.0], [1161.0, 2779.0], [216.0, 2779.0]]}, {"text": "a low-false positive rate which is important as false posi-", "confidence": 0.98893141746521, "text_region": [[216.0, 2782.0], [1167.0, 2782.0], [1167.0, 2828.0], [216.0, 2828.0]]}, {"text": "tives would under-estimate the effect of contamination in", "confidence": 0.9869964718818665, "text_region": [[216.0, 2828.0], [1164.0, 2828.0], [1164.0, 2874.0], [216.0, 2874.0]]}], "img_idx": 0, "score": 0.993374764919281}
{"type": "text", "bbox": [1225, 2576, 2172, 2825], "res": [{"text": "There are still many limitations to CLIP. While several of", "confidence": 0.9792841672897339, "text_region": [[1224.0, 2571.0], [2169.0, 2567.0], [2169.0, 2614.0], [1224.0, 2617.0]]}, {"text": "these are discussed as part of analysis in various sections,", "confidence": 0.9807751774787903, "text_region": [[1224.0, 2620.0], [2175.0, 2620.0], [2175.0, 2666.0], [1224.0, 2666.0]]}, {"text": "we summarize and collect them here.", "confidence": 0.992108941078186, "text_region": [[1224.0, 2666.0], [1826.0, 2670.0], [1826.0, 2716.0], [1224.0, 2713.0]]}, {"text": "On datasets with training splits, the performance of zero-", "confidence": 0.9813183546066284, "text_region": [[1224.0, 2736.0], [2165.0, 2739.0], [2165.0, 2785.0], [1224.0, 2782.0]]}, {"text": "shot CLIP is on average competitive with the simple su-", "confidence": 0.992146909236908, "text_region": [[1227.0, 2788.0], [2175.0, 2788.0], [2175.0, 2831.0], [1227.0, 2831.0]]}], "img_idx": 0, "score": 0.9913500547409058}
{"type": "text", "bbox": [220, 2090, 1160, 2360], "res": [{"text": "3) The amount of overlap is often small so we also run a", "confidence": 0.9952300190925598, "text_region": [[216.0, 2082.0], [1161.0, 2082.0], [1161.0, 2128.0], [216.0, 2128.0]]}, {"text": "binomial significance test where we use the accuracy on", "confidence": 0.9942362904548645, "text_region": [[220.0, 2135.0], [1164.0, 2135.0], [1164.0, 2181.0], [220.0, 2181.0]]}, {"text": "Clean as the null hypothesis and compute the one-tailed", "confidence": 0.9992339611053467, "text_region": [[220.0, 2181.0], [1164.0, 2181.0], [1164.0, 2228.0], [220.0, 2228.0]]}, {"text": "(greater) p-value for the Ove rlap subset. We also calculate", "confidence": 0.9829513430595398, "text_region": [[220.0, 2231.0], [1164.0, 2231.0], [1164.0, 2274.0], [220.0, 2274.0]]}, {"text": "99.5% Clopper-Pearson confidence intervals on Di rt y as", "confidence": 0.9671638607978821, "text_region": [[216.0, 2274.0], [1167.0, 2274.0], [1167.0, 2330.0], [216.0, 2330.0]]}, {"text": "another check.", "confidence": 0.9988523125648499, "text_region": [[216.0, 2330.0], [452.0, 2330.0], [452.0, 2366.0], [216.0, 2366.0]]}], "img_idx": 0, "score": 0.9894958734512329}
{"type": "text", "bbox": [1225, 279, 2167, 1031], "res": [{"text": "our analysis. There is a median overlap of 2.2% and an av-", "confidence": 0.9956923723220825, "text_region": [[1227.0, 274.0], [2175.0, 274.0], [2175.0, 320.0], [1227.0, 320.0]]}, {"text": "erage overlap of 3.2%. Due to this small amount of overlap,", "confidence": 0.9886350631713867, "text_region": [[1221.0, 317.0], [2175.0, 320.0], [2175.0, 376.0], [1221.0, 373.0]]}, {"text": "overall accuracy is rarely shifted by more than 0.1% with", "confidence": 0.9873031377792358, "text_region": [[1227.0, 373.0], [2172.0, 373.0], [2172.0, 419.0], [1227.0, 419.0]]}, {"text": "only 7 datasets above this threshold. Of these, only 2 are", "confidence": 0.9910176396369934, "text_region": [[1227.0, 419.0], [2169.0, 419.0], [2169.0, 465.0], [1227.0, 465.0]]}, {"text": "statistically significant after Bonferroni correction. The max", "confidence": 0.9884502291679382, "text_region": [[1227.0, 465.0], [2169.0, 465.0], [2169.0, 511.0], [1227.0, 511.0]]}, {"text": "detected improvement is only 0.6% on Birdsnap which has", "confidence": 0.9829386472702026, "text_region": [[1227.0, 515.0], [2169.0, 515.0], [2169.0, 561.0], [1227.0, 561.0]]}, {"text": "the second largest overlap at 12.1%. The largest overlap is", "confidence": 0.9971290826797485, "text_region": [[1221.0, 554.0], [2172.0, 558.0], [2172.0, 614.0], [1221.0, 610.0]]}, {"text": "for Country211 at 21.5%. This is due to it being constructed", "confidence": 0.9827196598052979, "text_region": [[1221.0, 607.0], [2169.0, 607.0], [2169.0, 653.0], [1221.0, 653.0]]}, {"text": "out of YFCC100M, which our pre-training dataset contains", "confidence": 0.985845685005188, "text_region": [[1224.0, 660.0], [2172.0, 660.0], [2172.0, 703.0], [1224.0, 703.0]]}, {"text": "a filtered subset of. Despite this large overlap there is only", "confidence": 0.9984736442565918, "text_region": [[1224.0, 706.0], [2169.0, 706.0], [2169.0, 752.0], [1224.0, 752.0]]}, {"text": "a 0.2% increase in accuracy on Country211. This may be", "confidence": 0.9949980974197388, "text_region": [[1221.0, 749.0], [2175.0, 749.0], [2175.0, 805.0], [1221.0, 805.0]]}, {"text": "because the training text accompanying an example is often", "confidence": 0.9954535365104675, "text_region": [[1227.0, 802.0], [2172.0, 802.0], [2172.0, 848.0], [1227.0, 848.0]]}, {"text": "not related to the specific task a downstream eval measures.", "confidence": 0.9973362684249878, "text_region": [[1224.0, 851.0], [2172.0, 851.0], [2172.0, 898.0], [1224.0, 898.0]]}, {"text": "Country211 measures geo-localization ability, but inspect-", "confidence": 0.9848883152008057, "text_region": [[1227.0, 898.0], [2172.0, 898.0], [2172.0, 944.0], [1227.0, 944.0]]}, {"text": "ing the training text for these duplicates showed they often", "confidence": 0.9929258823394775, "text_region": [[1221.0, 941.0], [2172.0, 937.0], [2172.0, 993.0], [1221.0, 997.0]]}, {"text": "do not mention the location of the image.", "confidence": 0.991807222366333, "text_region": [[1224.0, 990.0], [1886.0, 993.0], [1886.0, 1040.0], [1224.0, 1036.0]]}], "img_idx": 0, "score": 0.988608717918396}
{"type": "text", "bbox": [220, 1781, 1160, 2053], "res": [{"text": "2) We then compute the zero-shot accuracy of CLIP", "confidence": 0.9854506850242615, "text_region": [[216.0, 1775.0], [1164.0, 1775.0], [1164.0, 1822.0], [216.0, 1822.0]]}, {"text": "RN50x64 on the three splits and report A11", "confidence": 0.9947358965873718, "text_region": [[220.0, 1825.0], [968.0, 1825.0], [968.0, 1871.0], [220.0, 1871.0]]}, {"text": "Clean", "confidence": 0.9935294985771179, "text_region": [[1034.0, 1828.0], [1161.0, 1828.0], [1161.0, 1865.0], [1034.0, 1865.0]]}, {"text": "as our main metric. This is the difference in accuracy due", "confidence": 0.9919795393943787, "text_region": [[216.0, 1871.0], [1164.0, 1871.0], [1164.0, 1917.0], [216.0, 1917.0]]}, {"text": "to contamination. When positive it is our estimate of how", "confidence": 0.9836920499801636, "text_region": [[220.0, 1921.0], [1164.0, 1921.0], [1164.0, 1964.0], [220.0, 1964.0]]}, {"text": "much the overall reported accuracy on the dataset was in-", "confidence": 0.9991341233253479, "text_region": [[220.0, 1967.0], [1161.0, 1967.0], [1161.0, 2013.0], [220.0, 2013.0]]}, {"text": "flated by over-fitting to overlapping data.", "confidence": 0.9830594658851624, "text_region": [[213.0, 2006.0], [878.0, 2010.0], [878.0, 2066.0], [213.0, 2062.0]]}], "img_idx": 0, "score": 0.9806455373764038}
{"type": "text", "bbox": [1227, 1071, 2169, 1971], "res": [{"text": "We are aware of two potential concerns with our analysis.", "confidence": 0.994462788105011, "text_region": [[1224.0, 1063.0], [2172.0, 1063.0], [2172.0, 1109.0], [1224.0, 1109.0]]}, {"text": "First our detector is not perfect. While it achieves near", "confidence": 0.9987419247627258, "text_region": [[1224.0, 1112.0], [2172.0, 1112.0], [2172.0, 1158.0], [1224.0, 1158.0]]}, {"text": " 100% accuracy on its proxy training task and manual in-", "confidence": 0.9909785389900208, "text_region": [[1217.0, 1152.0], [2179.0, 1155.0], [2178.0, 1211.0], [1217.0, 1208.0]]}, {"text": "spection + threshold tuning results in very high precision", "confidence": 0.990366518497467, "text_region": [[1224.0, 1205.0], [2172.0, 1201.0], [2172.0, 1257.0], [1224.0, 1261.0]]}, {"text": "with good recall among the found nearest-neighbors, we can", "confidence": 0.998832106590271, "text_region": [[1227.0, 1254.0], [2169.0, 1254.0], [2169.0, 1300.0], [1227.0, 1300.0]]}, {"text": "not tractably check its recall across 400 million examples.", "confidence": 0.9885781407356262, "text_region": [[1227.0, 1304.0], [2172.0, 1304.0], [2172.0, 1350.0], [1227.0, 1350.0]]}, {"text": "Another potential confounder of our analysis is that the un-", "confidence": 0.9965906739234924, "text_region": [[1227.0, 1353.0], [2175.0, 1353.0], [2175.0, 1399.0], [1227.0, 1399.0]]}, {"text": " derlying data distribution may shift between the Overlap", "confidence": 0.9757909774780273, "text_region": [[1221.0, 1393.0], [2172.0, 1396.0], [2172.0, 1452.0], [1221.0, 1449.0]]}, {"text": "and Clean subsets. For example, on Kinetics-700 many", "confidence": 0.9958496689796448, "text_region": [[1224.0, 1442.0], [2175.0, 1442.0], [2175.0, 1498.0], [1224.0, 1498.0]]}, {"text": "\"overlaps\u201d are in fact all black transition frames. This ex-", "confidence": 0.9712944626808167, "text_region": [[1227.0, 1495.0], [2175.0, 1495.0], [2175.0, 1541.0], [1227.0, 1541.0]]}, {"text": " plains why Kinetics-700 has an apparent 20% accuracy drop", "confidence": 0.9836561679840088, "text_region": [[1221.0, 1538.0], [2175.0, 1538.0], [2175.0, 1594.0], [1221.0, 1594.0]]}, {"text": "on Overlap. We suspect more subtle distribution shifts", "confidence": 0.9933696985244751, "text_region": [[1227.0, 1591.0], [2172.0, 1591.0], [2172.0, 1637.0], [1227.0, 1637.0]]}, {"text": "likely exist. One possibility we noticed on CIFAR-100 is", "confidence": 0.9867897033691406, "text_region": [[1227.0, 1640.0], [2172.0, 1640.0], [2172.0, 1686.0], [1227.0, 1686.0]]}, {"text": "that, due to the very low resolution of its images, many", "confidence": 0.9866387248039246, "text_region": [[1221.0, 1683.0], [2172.0, 1690.0], [2172.0, 1736.0], [1221.0, 1729.0]]}, {"text": "duplicates were false positives of small objects such as birds", "confidence": 0.9900376200675964, "text_region": [[1227.0, 1736.0], [2169.0, 1736.0], [2169.0, 1782.0], [1227.0, 1782.0]]}, {"text": "or planes. Changes in accuracy could instead be due to", "confidence": 0.9863322973251343, "text_region": [[1224.0, 1779.0], [2169.0, 1779.0], [2169.0, 1825.0], [1224.0, 1825.0]]}, {"text": "changes in the class distribution or difficulty of the dupli-", "confidence": 0.9845520257949829, "text_region": [[1224.0, 1825.0], [2179.0, 1825.0], [2179.0, 1881.0], [1224.0, 1881.0]]}, {"text": "cates. Unfortunately, these distribution and difficulty shifts ", "confidence": 0.9834104180335999, "text_region": [[1224.0, 1878.0], [2172.0, 1878.0], [2172.0, 1924.0], [1224.0, 1924.0]]}, {"text": " could also mask the effects of over-fitting.", "confidence": 0.9774421453475952, "text_region": [[1221.0, 1917.0], [1906.0, 1924.0], [1905.0, 1980.0], [1220.0, 1973.0]]}], "img_idx": 0, "score": 0.9773931503295898}
{"type": "text", "bbox": [219, 1517, 1157, 1736], "res": [{"text": "contains all examples that are below this threshold. We", "confidence": 0.9945607781410217, "text_region": [[220.0, 1511.0], [1164.0, 1511.0], [1164.0, 1558.0], [220.0, 1558.0]]}, {"text": "denote the unaltered full dataset Al1 for reference. From", "confidence": 0.9881967902183533, "text_region": [[216.0, 1561.0], [1161.0, 1561.0], [1161.0, 1607.0], [216.0, 1607.0]]}, {"text": "this we first record the degree of data contamination as the", "confidence": 0.9871751070022583, "text_region": [[216.0, 1604.0], [1164.0, 1607.0], [1164.0, 1653.0], [216.0, 1650.0]]}, {"text": "ratio of the number of examples in Overlap to the size of", "confidence": 0.9863092303276062, "text_region": [[216.0, 1657.0], [1164.0, 1657.0], [1164.0, 1703.0], [216.0, 1703.0]]}, {"text": "All.", "confidence": 0.9876617193222046, "text_region": [[216.0, 1706.0], [303.0, 1706.0], [303.0, 1742.0], [216.0, 1742.0]]}], "img_idx": 0, "score": 0.9710447192192078}
{"type": "text", "bbox": [1228, 2001, 2172, 2414], "res": [{"text": "However, these results closely follow the findings of simi-", "confidence": 0.994637131690979, "text_region": [[1224.0, 1996.0], [2175.0, 1996.0], [2175.0, 2043.0], [1224.0, 2043.0]]}, {"text": "lar duplicate analysis in previous work on large scale pre-", "confidence": 0.9944761991500854, "text_region": [[1227.0, 2046.0], [2175.0, 2046.0], [2175.0, 2092.0], [1227.0, 2092.0]]}, {"text": "training. Mahajan et al. (2018) and Kolesnikov et al. (2019)", "confidence": 0.9925378561019897, "text_region": [[1224.0, 2092.0], [2175.0, 2092.0], [2175.0, 2138.0], [1224.0, 2138.0]]}, {"text": "detected similar overlap rates and found minimal changes in", "confidence": 0.9996276497840881, "text_region": [[1227.0, 2142.0], [2172.0, 2142.0], [2172.0, 2188.0], [1227.0, 2188.0]]}, {"text": "overall performance. Importantly, Kolesnikov et al. (2019)", "confidence": 0.9993312358856201, "text_region": [[1227.0, 2188.0], [2175.0, 2188.0], [2175.0, 2234.0], [1227.0, 2234.0]]}, {"text": "also compared the alternative de-duplication strategy dis-", "confidence": 0.9990583062171936, "text_region": [[1224.0, 2237.0], [2172.0, 2237.0], [2172.0, 2284.0], [1224.0, 2284.0]]}, {"text": " cussed in the introduction to this section with the approach", "confidence": 0.9864223599433899, "text_region": [[1218.0, 2274.0], [2175.0, 2280.0], [2175.0, 2337.0], [1217.0, 2330.0]]}, {"text": "we settled on and observed little difference between the two", "confidence": 0.9874931573867798, "text_region": [[1224.0, 2330.0], [2172.0, 2330.0], [2172.0, 2376.0], [1224.0, 2376.0]]}, {"text": " approaches.", "confidence": 0.9644731879234314, "text_region": [[1220.0, 2383.0], [1422.0, 2375.0], [1424.0, 2422.0], [1222.0, 2429.0]]}], "img_idx": 0, "score": 0.9372266530990601}
{"type": "text", "bbox": [227, 1206, 1125, 1326], "res": [{"text": "Figure 16. The hardest problems for CLIP also tend to be the hard-", "confidence": 0.992161750793457, "text_region": [[220.0, 1201.0], [1164.0, 1201.0], [1164.0, 1244.0], [220.0, 1244.0]]}, {"text": "est problems for humans. Here we rank image categories by diffi-", "confidence": 0.9982985258102417, "text_region": [[216.0, 1244.0], [1164.0, 1244.0], [1164.0, 1290.0], [216.0, 1290.0]]}, {"text": "culty for CLIP as measured as probability of the correct label.", "confidence": 0.9875828623771667, "text_region": [[220.0, 1287.0], [1108.0, 1287.0], [1108.0, 1333.0], [220.0, 1333.0]]}], "img_idx": 0, "score": 0.8947192430496216}
{"type": "title", "bbox": [1229, 2493, 1516, 2528], "res": [{"text": "6. Limitations", "confidence": 0.9974356293678284, "text_region": [[1222.0, 2481.0], [1517.0, 2489.0], [1516.0, 2538.0], [1220.0, 2531.0]]}], "img_idx": 0, "score": 0.9511373043060303}
{"type": "figure", "bbox": [219, 276, 1153, 1141], "res": [{"text": "00", "confidence": 0.8649598956108093, "text_region": [[276.0, 340.0], [299.0, 340.0], [299.0, 356.0], [276.0, 356.0]]}, {"text": "40", "confidence": 0.9998043775558472, "text_region": [[279.0, 676.0], [309.0, 676.0], [309.0, 703.0], [279.0, 703.0]]}, {"text": "Zero-Shot CLIP", "confidence": 0.9949358105659485, "text_region": [[399.0, 746.0], [565.0, 746.0], [565.0, 779.0], [399.0, 779.0]]}, {"text": "One-Shot Human", "confidence": 0.9968252778053284, "text_region": [[396.0, 779.0], [579.0, 779.0], [579.0, 812.0], [396.0, 812.0]]}, {"text": "meml", "confidence": 0.5496731996536255, "text_region": [[343.0, 874.0], [416.0, 874.0], [416.0, 898.0], [343.0, 898.0]]}], "img_idx": 0, "score": 0.9530659914016724}
{"type": "header", "bbox": [2129, 193, 2164, 218], "res": [{"text": "18", "confidence": 0.9998288154602051, "text_region": [[2129.0, 191.0], [2169.0, 191.0], [2169.0, 221.0], [2129.0, 221.0]]}], "img_idx": 0, "score": 0.909691572189331}
{"type": "header", "bbox": [228, 194, 1125, 224], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9977254271507263, "text_region": [[220.0, 188.0], [1370.0, 188.0], [1370.0, 231.0], [220.0, 231.0]]}], "img_idx": 0, "score": 0.7435084581375122}
