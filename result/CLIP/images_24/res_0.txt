{"type": "text", "bbox": [1228, 2008, 2168, 2710], "res": [{"text": "Any model that leverages written, spoken, signed or any", "confidence": 0.9963424801826477, "text_region": [[1227.0, 2003.0], [2169.0, 2003.0], [2169.0, 2049.0], [1227.0, 2049.0]]}, {"text": " other form of human language as part of its training signal", "confidence": 0.9791669845581055, "text_region": [[1221.0, 2043.0], [2172.0, 2046.0], [2172.0, 2102.0], [1221.0, 2099.0]]}, {"text": "is arguably using natural language as a source of supervi-", "confidence": 0.9952774047851562, "text_region": [[1224.0, 2099.0], [2169.0, 2099.0], [2169.0, 2145.0], [1224.0, 2145.0]]}, {"text": "sion. This is an admittedly extremely broad area and covers", "confidence": 0.9988904595375061, "text_region": [[1224.0, 2145.0], [2169.0, 2145.0], [2169.0, 2191.0], [1224.0, 2191.0]]}, {"text": "most work in the field of distributional semantics including", "confidence": 0.9919230937957764, "text_region": [[1224.0, 2194.0], [2169.0, 2194.0], [2169.0, 2241.0], [1224.0, 2241.0]]}, {"text": "topic models (Blei et al., 2003), word, sentence, and para-", "confidence": 0.9978057742118835, "text_region": [[1227.0, 2241.0], [2175.0, 2241.0], [2175.0, 2287.0], [1227.0, 2287.0]]}, {"text": " graph vectors (Mikolov et al., 2013; Kiros et al., 2015; Le &", "confidence": 0.9831750392913818, "text_region": [[1221.0, 2287.0], [2175.0, 2280.0], [2175.0, 2336.0], [1221.0, 2343.0]]}, {"text": "Mikolov, 2014), and language models (Bengio et al., 2003).", "confidence": 0.9783185720443726, "text_region": [[1221.0, 2333.0], [2175.0, 2330.0], [2175.0, 2386.0], [1221.0, 2389.0]]}, {"text": "It also includes much of the broader field of NLP that deals", "confidence": 0.9906164407730103, "text_region": [[1224.0, 2386.0], [2169.0, 2386.0], [2169.0, 2432.0], [1224.0, 2432.0]]}, {"text": "with predicting or modeling sequences of natural language", "confidence": 0.9994624853134155, "text_region": [[1227.0, 2435.0], [2169.0, 2435.0], [2169.0, 2482.0], [1227.0, 2482.0]]}, {"text": "in some way. Work in NLP intentionally leveraging natural", "confidence": 0.9929720163345337, "text_region": [[1224.0, 2482.0], [2172.0, 2482.0], [2172.0, 2528.0], [1224.0, 2528.0]]}, {"text": "language supervision in the form of explanations, feedback,", "confidence": 0.9922334551811218, "text_region": [[1227.0, 2531.0], [2172.0, 2531.0], [2172.0, 2577.0], [1227.0, 2577.0]]}, {"text": "instructions, and advice for tasks such as classification (as", "confidence": 0.9937109351158142, "text_region": [[1227.0, 2577.0], [2169.0, 2577.0], [2169.0, 2624.0], [1227.0, 2624.0]]}, {"text": "opposed to the commonly used representation of supervision", "confidence": 0.9993148446083069, "text_region": [[1224.0, 2624.0], [2172.0, 2617.0], [2172.0, 2673.0], [1224.0, 2680.0]]}, {"text": "as a set of arbitrarily encoded discrete category labels) has", "confidence": 0.9778785109519958, "text_region": [[1227.0, 2673.0], [2169.0, 2673.0], [2169.0, 2719.0], [1227.0, 2719.0]]}], "img_idx": 0, "score": 0.994945228099823}
{"type": "text", "bbox": [1226, 281, 2166, 818], "res": [{"text": "We hope that this work motivates future research on the", "confidence": 0.9830323457717896, "text_region": [[1224.0, 274.0], [2169.0, 274.0], [2169.0, 320.0], [1224.0, 320.0]]}, {"text": "characterization of the capabilities, shortcomings, and biases", "confidence": 0.9772996306419373, "text_region": [[1224.0, 323.0], [2169.0, 323.0], [2169.0, 370.0], [1224.0, 370.0]]}, {"text": " of such models, and we are excited to engage with the", "confidence": 0.9901847839355469, "text_region": [[1214.0, 363.0], [2175.0, 366.0], [2175.0, 422.0], [1214.0, 419.0]]}, {"text": "research community on such questions.", "confidence": 0.9965811967849731, "text_region": [[1227.0, 422.0], [1859.0, 422.0], [1859.0, 465.0], [1227.0, 465.0]]}, {"text": "We believe one good step forward is community exploration", "confidence": 0.9937565326690674, "text_region": [[1221.0, 485.0], [2172.0, 485.0], [2172.0, 541.0], [1221.0, 541.0]]}, {"text": "to further characterize the capabilities of models like CLIP", "confidence": 0.9979918003082275, "text_region": [[1224.0, 538.0], [2165.0, 538.0], [2165.0, 584.0], [1224.0, 584.0]]}, {"text": "and - crucially - identify application areas where they have", "confidence": 0.9940961599349976, "text_region": [[1224.0, 587.0], [2169.0, 587.0], [2169.0, 634.0], [1224.0, 634.0]]}, {"text": "promising performance and areas where they may have", "confidence": 0.9871611595153809, "text_region": [[1221.0, 630.0], [2172.0, 627.0], [2172.0, 683.0], [1221.0, 686.0]]}, {"text": "reduced performance?. This process of characterization can", "confidence": 0.9952208399772644, "text_region": [[1227.0, 683.0], [2172.0, 683.0], [2172.0, 729.0], [1227.0, 729.0]]}, {"text": "help researchers increase the likelihood models are used", "confidence": 0.993956446647644, "text_region": [[1224.0, 729.0], [2172.0, 729.0], [2172.0, 776.0], [1224.0, 776.0]]}, {"text": "beneficially by:", "confidence": 0.9995298385620117, "text_region": [[1225.0, 772.0], [1481.0, 779.0], [1479.0, 825.0], [1224.0, 818.0]]}], "img_idx": 0, "score": 0.993296205997467}
{"type": "text", "bbox": [219, 2227, 1160, 2595], "res": [{"text": "However, CLIP does unlock a certain aspect of usability", "confidence": 0.9815084934234619, "text_region": [[216.0, 2218.0], [1164.0, 2218.0], [1164.0, 2274.0], [216.0, 2274.0]]}, {"text": "given how it removes the need for training data. Thus, CLIP", "confidence": 0.9923886656761169, "text_region": [[220.0, 2270.0], [1161.0, 2270.0], [1161.0, 2317.0], [220.0, 2317.0]]}, {"text": "and similar models could enable bespoke, niche surveillance", "confidence": 0.9962606430053711, "text_region": [[220.0, 2320.0], [1157.0, 2320.0], [1157.0, 2366.0], [220.0, 2366.0]]}, {"text": "use cases for which no well-tailored models or datasets exist,", "confidence": 0.9952709078788757, "text_region": [[220.0, 2366.0], [1161.0, 2366.0], [1161.0, 2412.0], [220.0, 2412.0]]}, {"text": "and could lower the skill requirements to build such appli-", "confidence": 0.979951024055481, "text_region": [[220.0, 2416.0], [1164.0, 2416.0], [1164.0, 2462.0], [220.0, 2462.0]]}, {"text": "cations. As our experiments show, ZS CLIP displays non-", "confidence": 0.9835128784179688, "text_region": [[220.0, 2462.0], [1161.0, 2462.0], [1161.0, 2508.0], [220.0, 2508.0]]}, {"text": "trivial, but not exceptional, performance on a few surveil-", "confidence": 0.9889699816703796, "text_region": [[220.0, 2508.0], [1164.0, 2508.0], [1164.0, 2554.0], [220.0, 2554.0]]}, {"text": "lance relevant tasks today.", "confidence": 0.9993933439254761, "text_region": [[220.0, 2558.0], [645.0, 2558.0], [645.0, 2604.0], [220.0, 2604.0]]}], "img_idx": 0, "score": 0.9902231097221375}
{"type": "text", "bbox": [220, 666, 1159, 997], "res": [{"text": "Table 8. CelebA Zero-Shot Top-1 Identity Recognition Accuracy", "confidence": 0.9809925556182861, "text_region": [[216.0, 653.0], [1154.0, 660.0], [1154.0, 713.0], [216.0, 706.0]]}, {"text": "mirrors recent developments in natural language processing,", "confidence": 0.9869701862335205, "text_region": [[213.0, 756.0], [1168.0, 759.0], [1167.0, 815.0], [213.0, 812.0]]}, {"text": "in which recent large language models trained on Internet", "confidence": 0.9825058579444885, "text_region": [[220.0, 812.0], [1164.0, 812.0], [1164.0, 858.0], [220.0, 858.0]]}, {"text": " data often exhibit a surprising ability to provide informa-", "confidence": 0.980492889881134, "text_region": [[210.0, 851.0], [1168.0, 855.0], [1167.0, 911.0], [209.0, 907.0]]}, {"text": "tion related to relatively minor public figures (Brown et al.,", "confidence": 0.9827650189399719, "text_region": [[216.0, 908.0], [1164.0, 908.0], [1164.0, 954.0], [216.0, 954.0]]}, {"text": "2020).", "confidence": 0.9994804859161377, "text_region": [[216.0, 946.0], [330.0, 955.0], [327.0, 1004.0], [212.0, 995.0]]}], "img_idx": 0, "score": 0.987946629524231}
{"type": "text", "bbox": [222, 2735, 1159, 2868], "res": [{"text": "This preliminary analysis is intended to illustrate some of", "confidence": 0.9861453175544739, "text_region": [[220.0, 2732.0], [1164.0, 2732.0], [1164.0, 2779.0], [220.0, 2779.0]]}, {"text": "the challenges that general purpose computer vision models", "confidence": 0.9864735007286072, "text_region": [[216.0, 2775.0], [1164.0, 2775.0], [1164.0, 2831.0], [216.0, 2831.0]]}, {"text": "pose and to give a glimpse into their biases and impacts.", "confidence": 0.9967766404151917, "text_region": [[220.0, 2828.0], [1164.0, 2828.0], [1164.0, 2874.0], [220.0, 2874.0]]}], "img_idx": 0, "score": 0.9809253811836243}
{"type": "text", "bbox": [1228, 1765, 2166, 1851], "res": [{"text": "We plan to contribute to this work, and hope this analysis", "confidence": 0.9931410551071167, "text_region": [[1224.0, 1766.0], [2165.0, 1766.0], [2165.0, 1808.0], [1224.0, 1808.0]]}, {"text": " provides some motivating examples for subsequent research.", "confidence": 0.9960426092147827, "text_region": [[1221.0, 1808.0], [2169.0, 1808.0], [2169.0, 1855.0], [1221.0, 1855.0]]}], "img_idx": 0, "score": 0.9742920994758606}
{"type": "text", "bbox": [1272, 893, 2166, 1025], "res": [{"text": "\u00b7 Identifying potentially beneficial downstream uses of", "confidence": 0.9967155456542969, "text_region": [[1271.0, 888.0], [2172.0, 888.0], [2172.0, 934.0], [1271.0, 934.0]]}, {"text": "models early in the research process, enabling other", "confidence": 0.9752533435821533, "text_region": [[1307.0, 937.0], [2169.0, 937.0], [2169.0, 983.0], [1307.0, 983.0]]}, {"text": "researchers to think about applications.", "confidence": 0.9998227953910828, "text_region": [[1307.0, 987.0], [1932.0, 987.0], [1932.0, 1030.0], [1307.0, 1030.0]]}], "img_idx": 0, "score": 0.969597578048706}
{"type": "text", "bbox": [219, 1665, 1161, 2185], "res": [{"text": "CLIP offers significant benefit for tasks that have relatively", "confidence": 0.99239581823349, "text_region": [[216.0, 1670.0], [1161.0, 1673.0], [1161.0, 1719.0], [216.0, 1716.0]]}, {"text": "little data given its zero-shot capabilities. However, large", "confidence": 0.9993568062782288, "text_region": [[220.0, 1723.0], [1161.0, 1723.0], [1161.0, 1769.0], [220.0, 1769.0]]}, {"text": "datasets and high performing supervised models exist for", "confidence": 0.9903336763381958, "text_region": [[220.0, 1769.0], [1164.0, 1769.0], [1164.0, 1815.0], [220.0, 1815.0]]}, {"text": "many in-demand surveillance tasks such as facial recogni-", "confidence": 0.9983808994293213, "text_region": [[223.0, 1815.0], [1164.0, 1815.0], [1164.0, 1861.0], [223.0, 1861.0]]}, {"text": "tion. As a result, CLIP's comparative appeal for such uses", "confidence": 0.9870398044586182, "text_region": [[220.0, 1865.0], [1157.0, 1865.0], [1157.0, 1911.0], [220.0, 1911.0]]}, {"text": "is low. Additionally, CLIP is not designed for common", "confidence": 0.9973793625831604, "text_region": [[220.0, 1911.0], [1157.0, 1911.0], [1157.0, 1957.0], [220.0, 1957.0]]}, {"text": "surveillance-relevant tasks like object detection and seman-", "confidence": 0.9936621785163879, "text_region": [[216.0, 1960.0], [1157.0, 1960.0], [1157.0, 2006.0], [216.0, 2006.0]]}, {"text": "tic segmentation. This means it has limited use for certain", "confidence": 0.9873459339141846, "text_region": [[220.0, 2010.0], [1161.0, 2010.0], [1161.0, 2056.0], [220.0, 2056.0]]}, {"text": "surveillance tasks when models that are designed with these", "confidence": 0.9912506937980652, "text_region": [[220.0, 2056.0], [1161.0, 2056.0], [1161.0, 2099.0], [220.0, 2099.0]]}, {"text": "uses in mind such as Detectron2 (Wu et al., 2019) are widely", "confidence": 0.9898011088371277, "text_region": [[220.0, 2102.0], [1161.0, 2102.0], [1161.0, 2148.0], [220.0, 2148.0]]}, {"text": "available.", "confidence": 0.9997572898864746, "text_region": [[220.0, 2155.0], [373.0, 2155.0], [373.0, 2191.0], [220.0, 2191.0]]}], "img_idx": 0, "score": 0.961880624294281}
{"type": "text", "bbox": [1270, 1611, 2158, 1689], "res": [{"text": "\u00b7 Identifying potential failure modes and areas for further", "confidence": 0.9935704469680786, "text_region": [[1267.0, 1600.0], [2172.0, 1600.0], [2172.0, 1657.0], [1267.0, 1657.0]]}, {"text": "work.", "confidence": 0.995452880859375, "text_region": [[1307.0, 1657.0], [1397.0, 1657.0], [1397.0, 1693.0], [1307.0, 1693.0]]}], "img_idx": 0, "score": 0.9544032216072083}
{"type": "text", "bbox": [1271, 1251, 2165, 1377], "res": [{"text": "\u00b7 Better characterizing biases in models, alerting other", "confidence": 0.9880747199058533, "text_region": [[1274.0, 1247.0], [2169.0, 1247.0], [2169.0, 1294.0], [1274.0, 1294.0]]}, {"text": "researchers to areas of concern and areas for interven--", "confidence": 0.9881071448326111, "text_region": [[1304.0, 1297.0], [2179.0, 1297.0], [2179.0, 1340.0], [1304.0, 1340.0]]}, {"text": "tions.", "confidence": 0.9999086856842041, "text_region": [[1307.0, 1346.0], [1400.0, 1346.0], [1400.0, 1389.0], [1307.0, 1389.0]]}], "img_idx": 0, "score": 0.9519681930541992}
{"type": "text", "bbox": [1272, 1067, 2169, 1204], "res": [{"text": "\u00b7 Surfacing tasks with significant sensitivity and a large", "confidence": 0.993230402469635, "text_region": [[1271.0, 1062.0], [2169.0, 1069.0], [2168.0, 1116.0], [1270.0, 1109.0]]}, {"text": "set of societal stakeholders, which may call for inter-", "confidence": 0.9887058734893799, "text_region": [[1304.0, 1112.0], [2172.0, 1112.0], [2172.0, 1158.0], [1304.0, 1158.0]]}, {"text": "vention by policymakers.", "confidence": 0.9688498377799988, "text_region": [[1307.0, 1165.0], [1713.0, 1165.0], [1713.0, 1211.0], [1307.0, 1211.0]]}], "img_idx": 0, "score": 0.9491956233978271}
{"type": "text", "bbox": [220, 1034, 1161, 1637], "res": [{"text": "We found that the model had 59.2% top-1 accuracy out", "confidence": 0.9775267839431763, "text_region": [[213.0, 1023.0], [1164.0, 1026.0], [1164.0, 1073.0], [213.0, 1069.0]]}, {"text": "of 100 possible classes for \u201cin the wild\u2019 8k celebrity im-", "confidence": 0.9826256036758423, "text_region": [[220.0, 1072.0], [1164.0, 1072.0], [1164.0, 1119.0], [220.0, 1119.0]]}, {"text": "ages. However, this performance dropped to 43.3% when", "confidence": 0.9955307841300964, "text_region": [[213.0, 1119.0], [1164.0, 1115.0], [1164.0, 1171.0], [213.0, 1175.0]]}, {"text": "we increased our class sizes to 1k celebrity names. This", "confidence": 0.9733178019523621, "text_region": [[220.0, 1172.0], [1161.0, 1172.0], [1161.0, 1218.0], [220.0, 1218.0]]}, {"text": "performance is not competitive when compared to produc-", "confidence": 0.9832008481025696, "text_region": [[220.0, 1221.0], [1161.0, 1221.0], [1161.0, 1264.0], [220.0, 1264.0]]}, {"text": "tion level models such as Google's Celebrity Recognition", "confidence": 0.9981033205986023, "text_region": [[216.0, 1261.0], [1158.0, 1264.0], [1157.0, 1313.0], [216.0, 1310.0]]}, {"text": "(Google). However, what makes these results noteworthy is", "confidence": 0.98451828956604, "text_region": [[220.0, 1317.0], [1157.0, 1317.0], [1157.0, 1360.0], [220.0, 1360.0]]}, {"text": "that this analysis was done using only zero-shot identifica-", "confidence": 0.9969177842140198, "text_region": [[220.0, 1363.0], [1161.0, 1363.0], [1161.0, 1409.0], [220.0, 1409.0]]}, {"text": "tion capabilities based on names inferred from pre-training", "confidence": 0.9985869526863098, "text_region": [[220.0, 1412.0], [1157.0, 1412.0], [1157.0, 1459.0], [220.0, 1459.0]]}, {"text": "data - we didn't use any additional task-specific dataset, and", "confidence": 0.9865801930427551, "text_region": [[216.0, 1459.0], [1161.0, 1459.0], [1161.0, 1505.0], [216.0, 1505.0]]}, {"text": "so the (relatively) strong results further indicate that before", "confidence": 0.9916096329689026, "text_region": [[216.0, 1508.0], [1161.0, 1508.0], [1161.0, 1554.0], [216.0, 1554.0]]}, {"text": "deploying multimodal models, people will need to carefully", "confidence": 0.9942856431007385, "text_region": [[220.0, 1554.0], [1161.0, 1554.0], [1161.0, 1600.0], [220.0, 1600.0]]}, {"text": "study them for behaviors in a given context and domain.", "confidence": 0.9981889128684998, "text_region": [[216.0, 1604.0], [1118.0, 1604.0], [1118.0, 1647.0], [216.0, 1647.0]]}], "img_idx": 0, "score": 0.8893657922744751}
{"type": "title", "bbox": [219, 2660, 512, 2691], "res": [{"text": "7.3. Future Work", "confidence": 0.9868903756141663, "text_region": [[220.0, 2656.0], [519.0, 2656.0], [519.0, 2699.0], [220.0, 2699.0]]}], "img_idx": 0, "score": 0.9515313506126404}
{"type": "title", "bbox": [1230, 1923, 1561, 1960], "res": [{"text": "8. Related Work", "confidence": 0.9794021844863892, "text_region": [[1224.0, 1921.0], [1563.0, 1921.0], [1563.0, 1967.0], [1224.0, 1967.0]]}], "img_idx": 0, "score": 0.9503653645515442}
{"type": "title", "bbox": [1268, 1428, 2170, 1561], "res": [{"text": "\u00b7 Creating suites of tests to evaluate systems like CLIP", "confidence": 0.9855809807777405, "text_region": [[1274.0, 1426.0], [2168.0, 1422.0], [2169.0, 1468.0], [1274.0, 1472.0]]}, {"text": "on, so we can better characterize model capabilities", "confidence": 0.989217221736908, "text_region": [[1307.0, 1475.0], [2169.0, 1475.0], [2169.0, 1521.0], [1307.0, 1521.0]]}, {"text": "earlier in the development cycle.", "confidence": 0.9795337915420532, "text_region": [[1307.0, 1525.0], [1826.0, 1525.0], [1826.0, 1568.0], [1307.0, 1568.0]]}], "img_idx": 0, "score": 0.8082790970802307}
{"type": "table", "bbox": [218, 311, 1188, 611], "res": {"cell_bbox": [[12.088157653808594, 4.107523441314697, 365.15252685546875, 4.308135509490967, 363.4623718261719, 70.44970703125, 11.778803825378418, 69.46212768554688], [377.21478271484375, 6.484866142272949, 758.927001953125, 6.534700393676758, 759.8128051757812, 73.88297271728516, 378.79620361328125, 74.20459747314453], [690.4888916015625, 7.8563971519470215, 959.3532104492188, 8.076611518859863, 959.3421020507812, 76.1776123046875, 691.8077392578125, 75.86846923828125], [8.820976257324219, 73.15727996826172, 511.0786437988281, 75.54258728027344, 505.7809753417969, 151.348388671875, 8.517362594604492, 148.08822631835938], [493.61553955078125, 78.62053680419922, 773.7177734375, 81.04945373535156, 770.56494140625, 148.2026824951172, 488.5291748046875, 145.38255310058594], [761.992431640625, 79.79283905029297, 957.8965454101562, 82.1082763671875, 957.6084594726562, 142.31727600097656, 759.9461059570312, 139.59364318847656], [10.313596725463867, 130.27783203125, 502.9360656738281, 132.25643920898438, 497.4325256347656, 191.5180206298828, 10.0512056350708, 189.9207000732422], [497.2342529296875, 132.6655731201172, 735.5841674804688, 134.6217803955078, 734.4381103515625, 189.3538818359375, 495.66986083984375, 187.89468383789062], [767.4440307617188, 133.46270751953125, 957.4891357421875, 135.3229217529297, 957.3348388671875, 186.64634704589844, 767.1535034179688, 185.16444396972656], [9.164327621459961, 178.22219848632812, 456.33062744140625, 179.7949981689453, 451.89007568359375, 234.70993041992188, 8.959919929504395, 233.55101013183594], [492.9107360839844, 179.42918395996094, 722.1890869140625, 181.02906799316406, 720.5182495117188, 234.19424438476562, 490.5570373535156, 233.2442169189453], [780.151123046875, 180.6166534423828, 955.63037109375, 181.89231872558594, 955.4755249023438, 230.59629821777344, 780.0694580078125, 229.7886962890625], [7.244493007659912, 225.37619018554688, 420.5843200683594, 226.4496612548828, 416.1145324707031, 278.90118408203125, 7.0287885665893555, 278.4048767089844], [479.6456604003906, 226.22250366210938, 701.6922607421875, 227.21315002441406, 700.0624389648438, 280.1102294921875, 477.14764404296875, 279.7821960449219], [777.9254150390625, 227.84262084960938, 955.1907348632812, 228.42324829101562, 955.0303955078125, 275.6767578125, 777.6461181640625, 275.45947265625]], "html": "<html><body><table><tbody><tr><td>Model</td><td>100 Classes 1k Classes</td><td>2k Classes</td></tr><tr><td>CLIP L/14 59.2</td><td>43.3</td><td>42.2</td></tr><tr><td>CLIP RN50x64 56.4</td><td>39.5</td><td>38.4</td></tr><tr><td>CLIP RN50x16 52.7</td><td>37.4</td><td>36.3</td></tr><tr><td>CLIP RN50x4 52.8</td><td>38.1</td><td>37.3</td></tr></tbody></table></body></html>"}, "img_idx": 0, "score": 0.9634502530097961}
{"type": "header", "bbox": [2128, 192, 2163, 216], "res": [{"text": "25", "confidence": 0.9997757077217102, "text_region": [[2129.0, 191.0], [2165.0, 191.0], [2165.0, 224.0], [2129.0, 224.0]]}], "img_idx": 0, "score": 0.9041632413864136}
{"type": "header", "bbox": [228, 193, 1124, 225], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.9937004446983337, "text_region": [[216.0, 181.0], [1370.0, 185.0], [1370.0, 231.0], [216.0, 228.0]]}], "img_idx": 0, "score": 0.7166079878807068}
{"type": "reference", "bbox": [1224, 2751, 2167, 2863], "res": [{"text": "\u00b0A model could be unfit for use due to inadequate performance", "confidence": 0.9751074314117432, "text_region": [[1271.0, 2739.0], [2169.0, 2746.0], [2168.0, 2802.0], [1270.0, 2795.0]]}, {"text": "or due to the inappropriateness of AI use in the application area", "confidence": 0.9924210906028748, "text_region": [[1224.0, 2792.0], [2165.0, 2792.0], [2165.0, 2838.0], [1224.0, 2838.0]]}, {"text": "itself.", "confidence": 0.999877393245697, "text_region": [[1227.0, 2835.0], [1314.0, 2835.0], [1314.0, 2871.0], [1227.0, 2871.0]]}], "img_idx": 0, "score": 0.970912516117096}
