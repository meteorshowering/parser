{"type": "text", "bbox": [1227, 2158, 2168, 2861], "res": [{"text": " Although CLIP can flexibly generate zero-shot classifiers", "confidence": 0.9808639883995056, "text_region": [[1221.0, 2145.0], [2172.0, 2148.0], [2172.0, 2204.0], [1221.0, 2201.0]]}, {"text": "for a wide variety of tasks and datasets, CLIP is still limited", "confidence": 0.9814687371253967, "text_region": [[1221.0, 2204.0], [2172.0, 2198.0], [2172.0, 2241.0], [1221.0, 2247.0]]}, {"text": "to choosing from only those concepts in a given zero-shot", "confidence": 0.9983375668525696, "text_region": [[1224.0, 2251.0], [2172.0, 2251.0], [2172.0, 2297.0], [1224.0, 2297.0]]}, {"text": "classifier. This is a significant restriction compared to a", "confidence": 0.9766642451286316, "text_region": [[1227.0, 2297.0], [2172.0, 2297.0], [2172.0, 2343.0], [1227.0, 2343.0]]}, {"text": "truly fexible approach like image captioning which could", "confidence": 0.9833325147628784, "text_region": [[1221.0, 2340.0], [2175.0, 2340.0], [2175.0, 2396.0], [1221.0, 2396.0]]}, {"text": "generate novel outputs. Unfortunately, as described in Sec-", "confidence": 0.9988541007041931, "text_region": [[1224.0, 2393.0], [2172.0, 2389.0], [2172.0, 2435.0], [1224.0, 2439.0]]}, {"text": " tion 2.3 we found the computational efficiency of the image", "confidence": 0.9865158200263977, "text_region": [[1218.0, 2429.0], [2172.0, 2436.0], [2172.0, 2492.0], [1217.0, 2485.0]]}, {"text": "caption baseline we tried to be much lower than CLIP. A", "confidence": 0.9813605546951294, "text_region": [[1227.0, 2488.0], [2172.0, 2488.0], [2172.0, 2534.0], [1227.0, 2534.0]]}, {"text": "simple idea worth trying is joint training of a contrastive", "confidence": 0.9933108687400818, "text_region": [[1221.0, 2528.0], [2175.0, 2531.0], [2175.0, 2587.0], [1221.0, 2584.0]]}, {"text": "and generative objective with the hope of combining the", "confidence": 0.9866365790367126, "text_region": [[1227.0, 2584.0], [2169.0, 2584.0], [2169.0, 2630.0], [1227.0, 2630.0]]}, {"text": "efficiency of CLIP with the fexibility of a caption model.", "confidence": 0.9866471290588379, "text_region": [[1224.0, 2630.0], [2169.0, 2630.0], [2169.0, 2676.0], [1224.0, 2676.0]]}, {"text": "As another alternative, search could be performed at infer-", "confidence": 0.9996231198310852, "text_region": [[1227.0, 2680.0], [2172.0, 2680.0], [2172.0, 2726.0], [1227.0, 2726.0]]}, {"text": "ence time over many natural language explanations of a", "confidence": 0.9907645583152771, "text_region": [[1227.0, 2729.0], [2169.0, 2729.0], [2169.0, 2775.0], [1227.0, 2775.0]]}, {"text": "given image, similar to approach proposed in Learning with", "confidence": 0.996282160282135, "text_region": [[1224.0, 2775.0], [2172.0, 2772.0], [2172.0, 2818.0], [1224.0, 2822.0]]}, {"text": "Latent Language Andreas et al. (2017).", "confidence": 0.9930697083473206, "text_region": [[1224.0, 2825.0], [1856.0, 2825.0], [1856.0, 2871.0], [1224.0, 2871.0]]}], "img_idx": 0, "score": 0.9935705065727234}
{"type": "text", "bbox": [1226, 1416, 2167, 2120], "res": [{"text": "CLIP learns a high quality semantic OCR representation that", "confidence": 0.9980999827384949, "text_region": [[1224.0, 1406.0], [2172.0, 1409.0], [2172.0, 1455.0], [1224.0, 1452.0]]}, {"text": " performs well on digitally rendered text, which is common", "confidence": 0.9754536747932434, "text_region": [[1217.0, 1455.0], [2172.0, 1452.0], [2172.0, 1508.0], [1217.0, 1511.0]]}, {"text": "in its pre-training dataset, as evidenced by performance on", "confidence": 0.9980660080909729, "text_region": [[1224.0, 1508.0], [2172.0, 1508.0], [2172.0, 1554.0], [1224.0, 1554.0]]}, {"text": " Rendered SST2. However, CLIP only achieves 88% accu-", "confidence": 0.9847241640090942, "text_region": [[1221.0, 1551.0], [2175.0, 1554.0], [2175.0, 1601.0], [1221.0, 1597.0]]}, {"text": "racy on the handwritten digits of MNIST. An embarrassingly", "confidence": 0.9907504916191101, "text_region": [[1224.0, 1604.0], [2169.0, 1604.0], [2169.0, 1650.0], [1224.0, 1650.0]]}, {"text": "simple baseline of logistic regression on raw pixels outper-", "confidence": 0.9857029318809509, "text_region": [[1224.0, 1650.0], [2172.0, 1650.0], [2172.0, 1696.0], [1224.0, 1696.0]]}, {"text": "forms zero-shot CLIP. Both semantic and near-duplicate", "confidence": 0.9983510375022888, "text_region": [[1224.0, 1699.0], [2169.0, 1699.0], [2169.0, 1742.0], [1224.0, 1742.0]]}, {"text": "nearest-neighbor retrieval verify that there are almost no im-", "confidence": 0.9826065897941589, "text_region": [[1224.0, 1749.0], [2175.0, 1749.0], [2175.0, 1792.0], [1224.0, 1792.0]]}, {"text": "ages that resemble MNIST digits in our pre-training dataset.", "confidence": 0.9865518808364868, "text_region": [[1224.0, 1795.0], [2175.0, 1795.0], [2175.0, 1841.0], [1224.0, 1841.0]]}, {"text": "This suggests CLIP does little to address the underlying", "confidence": 0.9856635928153992, "text_region": [[1221.0, 1838.0], [2172.0, 1838.0], [2172.0, 1894.0], [1221.0, 1894.0]]}, {"text": "problem of brittle generalization of deep learning models.", "confidence": 0.9968142509460449, "text_region": [[1227.0, 1891.0], [2169.0, 1891.0], [2169.0, 1937.0], [1227.0, 1937.0]]}, {"text": "Instead CLIP tries to circumvent the problem and hopes that", "confidence": 0.987708330154419, "text_region": [[1224.0, 1934.0], [2172.0, 1937.0], [2172.0, 1983.0], [1224.0, 1980.0]]}, {"text": "by training on such a large and varied dataset that all data", "confidence": 0.992087185382843, "text_region": [[1227.0, 1987.0], [2172.0, 1987.0], [2172.0, 2033.0], [1227.0, 2033.0]]}, {"text": "will be effectively in-distribution. This is a naive assumption", "confidence": 0.9899994134902954, "text_region": [[1227.0, 2033.0], [2172.0, 2033.0], [2172.0, 2079.0], [1227.0, 2079.0]]}, {"text": " that, as MNIST demonstrates, is easy to violate.", "confidence": 0.9814796447753906, "text_region": [[1217.0, 2072.0], [1999.0, 2076.0], [1999.0, 2132.0], [1217.0, 2128.0]]}], "img_idx": 0, "score": 0.9925742149353027}
{"type": "text", "bbox": [218, 1014, 2167, 1270], "res": [{"text": "Figure 17. Few statistically significant improvements in accuracy due to detected data overlap. (Left) While several datasets have", "confidence": 0.9935150146484375, "text_region": [[220.0, 1007.0], [2169.0, 1007.0], [2169.0, 1049.0], [220.0, 1049.0]]}, {"text": "up to \u00b120% apparent differences in zero-shot accuracy on detected overlapping vs clean examples only 5 datasets out of 35 total have", "confidence": 0.984619140625, "text_region": [[213.0, 1046.0], [2172.0, 1039.0], [2172.0, 1096.0], [213.0, 1102.0]]}, {"text": "99.5% Clopper-Pearson confidence intervals that exclude a 0% accuracy difference. 2 of these datasets do worse on overlapping data.", "confidence": 0.9900882840156555, "text_region": [[220.0, 1096.0], [2172.0, 1096.0], [2172.0, 1138.0], [220.0, 1138.0]]}, {"text": "(Right) Since the percentage of detected overlapping examples is almost always in the single digits, the overall test accuracy gain due to", "confidence": 0.9892011880874634, "text_region": [[216.0, 1138.0], [2169.0, 1138.0], [2169.0, 1185.0], [216.0, 1185.0]]}, {"text": " overlap is much smaller with the largest estimated increase being only 0.6% on Birdsnap. Similarly, for only 6 datasets are the accuracy", "confidence": 0.9804512858390808, "text_region": [[213.0, 1175.0], [2172.0, 1178.0], [2172.0, 1234.0], [213.0, 1231.0]]}, {"text": "improvements statistically significant when calculated using a one-sided binomial test.", "confidence": 0.9927622675895691, "text_region": [[220.0, 1228.0], [1463.0, 1228.0], [1463.0, 1270.0], [220.0, 1270.0]]}], "img_idx": 0, "score": 0.9795152544975281}
{"type": "text", "bbox": [220, 2611, 1163, 2835], "res": [{"text": "While zero-shot CLIP generalizes well to many natural im-", "confidence": 0.9875912070274353, "text_region": [[210.0, 2597.0], [1164.0, 2600.0], [1164.0, 2657.0], [209.0, 2653.0]]}, {"text": "age distributions as investigated in Section 3.3, we've ob-", "confidence": 0.9820235967636108, "text_region": [[213.0, 2650.0], [1171.0, 2647.0], [1171.0, 2703.0], [213.0, 2706.0]]}, {"text": "served that zero-shot CLIP still generalizes poorly to data", "confidence": 0.9910370111465454, "text_region": [[216.0, 2703.0], [1164.0, 2703.0], [1164.0, 2749.0], [216.0, 2749.0]]}, {"text": "that is truly out-of-distribution for it. An illustrative exam-", "confidence": 0.9917768836021423, "text_region": [[220.0, 2752.0], [1167.0, 2752.0], [1167.0, 2795.0], [220.0, 2795.0]]}, {"text": "ple occurs for the task of OCR as reported in Appendix E.", "confidence": 0.990781843662262, "text_region": [[220.0, 2802.0], [1164.0, 2802.0], [1164.0, 2848.0], [220.0, 2848.0]]}], "img_idx": 0, "score": 0.9786058664321899}
{"type": "text", "bbox": [220, 1806, 1158, 2581], "res": [{"text": "state-of-the-art performance. This is infeasible to train with", "confidence": 0.9974728226661682, "text_region": [[216.0, 1795.0], [1164.0, 1795.0], [1164.0, 1841.0], [216.0, 1841.0]]}, {"text": "current hardware. Further research into improving upon the", "confidence": 0.9915294647216797, "text_region": [[213.0, 1835.0], [1168.0, 1838.0], [1167.0, 1894.0], [213.0, 1891.0]]}, {"text": " computational and data efciency of CLIP will be necessary.", "confidence": 0.9682984948158264, "text_region": [[213.0, 1884.0], [1168.0, 1888.0], [1167.0, 1944.0], [213.0, 1940.0]]}, {"text": "Analysis in Section 3.1 found that CLIP's zero-shot perfor-", "confidence": 0.9853217005729675, "text_region": [[220.0, 1960.0], [1167.0, 1960.0], [1167.0, 2006.0], [220.0, 2006.0]]}, {"text": "mance is still quite weak on several kinds of tasks. When", "confidence": 0.9936171174049377, "text_region": [[216.0, 2010.0], [1161.0, 2010.0], [1161.0, 2056.0], [216.0, 2056.0]]}, {"text": "compared to task-specific models, the performance of CLIP", "confidence": 0.9846773743629456, "text_region": [[216.0, 2059.0], [1161.0, 2052.0], [1161.0, 2099.0], [216.0, 2106.0]]}, {"text": "is poor on several types of fine-grained classification such", "confidence": 0.9932231903076172, "text_region": [[216.0, 2105.0], [1161.0, 2105.0], [1161.0, 2152.0], [216.0, 2152.0]]}, {"text": "as differentiating models of cars, species of flowers, and", "confidence": 0.9876724481582642, "text_region": [[213.0, 2148.0], [1167.0, 2148.0], [1167.0, 2204.0], [213.0, 2204.0]]}, {"text": "variants of aircraft. CLIP also struggles with more abstract", "confidence": 0.9967485666275024, "text_region": [[220.0, 2201.0], [1164.0, 2201.0], [1164.0, 2247.0], [220.0, 2247.0]]}, {"text": "and systematic tasks such as counting the number of objects", "confidence": 0.9962198734283447, "text_region": [[216.0, 2251.0], [1164.0, 2251.0], [1164.0, 2294.0], [216.0, 2294.0]]}, {"text": "in an image. Finally for novel tasks which are unlikely to be", "confidence": 0.9992466568946838, "text_region": [[220.0, 2297.0], [1164.0, 2297.0], [1164.0, 2343.0], [220.0, 2343.0]]}, {"text": "included in CLIP's pre-training dataset, such as classifying", "confidence": 0.9799335598945618, "text_region": [[216.0, 2340.0], [1167.0, 2340.0], [1167.0, 2396.0], [216.0, 2396.0]]}, {"text": "the distance to the nearest car in a photo, CLIP's perfor-", "confidence": 0.9877665042877197, "text_region": [[220.0, 2392.0], [1164.0, 2392.0], [1164.0, 2439.0], [220.0, 2439.0]]}, {"text": "mance can be near random. We are confident that there are", "confidence": 0.9933500289916992, "text_region": [[220.0, 2439.0], [1164.0, 2439.0], [1164.0, 2485.0], [220.0, 2485.0]]}, {"text": " still many, many, tasks where CLIP's zero-shot performance", "confidence": 0.9883013367652893, "text_region": [[213.0, 2488.0], [1164.0, 2488.0], [1164.0, 2534.0], [213.0, 2534.0]]}, {"text": "is near chance level.", "confidence": 0.9862532019615173, "text_region": [[216.0, 2534.0], [545.0, 2534.0], [545.0, 2581.0], [216.0, 2581.0]]}], "img_idx": 0, "score": 0.966337263584137}
{"type": "text", "bbox": [222, 1417, 1160, 1931], "res": [{"text": "pervised baseline of a linear classifier on top of ResNet-50", "confidence": 0.9934890270233154, "text_region": [[220.0, 1412.0], [1161.0, 1412.0], [1161.0, 1455.0], [220.0, 1455.0]]}, {"text": "features. On most of these datasets, the performance of", "confidence": 0.9972526431083679, "text_region": [[223.0, 1459.0], [1164.0, 1459.0], [1164.0, 1505.0], [223.0, 1505.0]]}, {"text": "this baseline is now well below the overall state of the art.", "confidence": 0.9926415681838989, "text_region": [[216.0, 1505.0], [1168.0, 1508.0], [1167.0, 1554.0], [216.0, 1551.0]]}, {"text": "Significant work is still needed to improve the task learning", "confidence": 0.9803493618965149, "text_region": [[213.0, 1548.0], [1168.0, 1551.0], [1167.0, 1607.0], [213.0, 1604.0]]}, {"text": "and transfer capabilities of CLIP. While scaling has so far", "confidence": 0.9875580668449402, "text_region": [[220.0, 1600.0], [1164.0, 1600.0], [1164.0, 1647.0], [220.0, 1647.0]]}, {"text": "steadily improved performance and suggests a route for con-", "confidence": 0.9894899725914001, "text_region": [[216.0, 1650.0], [1164.0, 1650.0], [1164.0, 1696.0], [216.0, 1696.0]]}, {"text": "tinued improvement, we estimate around a 10o0x increase", "confidence": 0.972918689250946, "text_region": [[220.0, 1699.0], [1161.0, 1699.0], [1161.0, 1746.0], [220.0, 1746.0]]}, {"text": "in compute is required for zero-shot CLIP to reach overall", "confidence": 0.978488028049469, "text_region": [[220.0, 1746.0], [1164.0, 1746.0], [1164.0, 1792.0], [220.0, 1792.0]]}, {"text": "state-of-the-art performance. This is infeasible to train with", "confidence": 0.9974728226661682, "text_region": [[216.0, 1795.0], [1164.0, 1795.0], [1164.0, 1841.0], [216.0, 1841.0]]}, {"text": "current hardware. Further research into improving upon the", "confidence": 0.9915294647216797, "text_region": [[213.0, 1835.0], [1168.0, 1838.0], [1167.0, 1894.0], [213.0, 1891.0]]}, {"text": " computational and data efciency of CLIP will be necessary.", "confidence": 0.9682984948158264, "text_region": [[213.0, 1884.0], [1168.0, 1888.0], [1167.0, 1944.0], [213.0, 1940.0]]}], "img_idx": 0, "score": 0.6126251816749573}
{"type": "figure", "bbox": [204, 266, 2165, 958], "res": [{"text": "\uff08%)", "confidence": 0.8277602791786194, "text_region": [[223.0, 274.0], [243.0, 274.0], [243.0, 304.0], [223.0, 304.0]]}, {"text": "0.75", "confidence": 0.9991206526756287, "text_region": [[1241.0, 271.0], [1297.0, 271.0], [1297.0, 310.0], [1241.0, 310.0]]}, {"text": "\u25cfBirdsnap", "confidence": 0.9489556550979614, "text_region": [[1746.0, 300.0], [1866.0, 300.0], [1866.0, 337.0], [1746.0, 337.0]]}, {"text": "p<1e-3", "confidence": 0.908546507358551, "text_region": [[2012.0, 304.0], [2129.0, 304.0], [2129.0, 340.0], [2012.0, 340.0]]}, {"text": "OCIFAR-100", "confidence": 0.9643457531929016, "text_region": [[1467.0, 333.0], [1603.0, 333.0], [1603.0, 370.0], [1467.0, 370.0]]}, {"text": "p<0.05", "confidence": 0.9815528988838196, "text_region": [[2012.0, 333.0], [2125.0, 333.0], [2125.0, 370.0], [2012.0, 370.0]]}, {"text": "0.5", "confidence": 0.999849259853363, "text_region": [[1251.0, 376.0], [1294.0, 376.0], [1294.0, 406.0], [1251.0, 406.0]]}, {"text": "p>0.05", "confidence": 0.9946319460868835, "text_region": [[1992.0, 367.0], [2124.0, 359.0], [2126.0, 395.0], [1994.0, 403.0]]}, {"text": "FER2013", "confidence": 0.9994775056838989, "text_region": [[1414.0, 426.0], [1547.0, 426.0], [1547.0, 462.0], [1414.0, 462.0]]}, {"text": "SUN397\u00b0", "confidence": 0.9229002594947815, "text_region": [[1390.0, 462.0], [1510.0, 462.0], [1510.0, 495.0], [1390.0, 495.0]]}, {"text": "\u00b7SUN", "confidence": 0.8507852554321289, "text_region": [[1520.0, 455.0], [1587.0, 455.0], [1587.0, 482.0], [1520.0, 482.0]]}, {"text": "2 0.25", "confidence": 0.8439704775810242, "text_region": [[1197.0, 465.0], [1297.0, 465.0], [1297.0, 515.0], [1197.0, 515.0]]}, {"text": "OStanford Cars", "confidence": 0.9344464540481567, "text_region": [[1600.0, 462.0], [1776.0, 462.0], [1776.0, 498.0], [1600.0, 498.0]]}, {"text": "Country2110", "confidence": 0.9887351989746094, "text_region": [[1962.0, 488.0], [2112.0, 488.0], [2112.0, 525.0], [1962.0, 525.0]]}, {"text": "and", "confidence": 0.6702227592468262, "text_region": [[1201.0, 508.0], [1227.0, 508.0], [1227.0, 551.0], [1201.0, 551.0]]}, {"text": "NetSketch", "confidence": 0.9436467289924622, "text_region": [[506.0, 667.0], [612.0, 667.0], [612.0, 690.0], [506.0, 690.0]]}, {"text": "0.25", "confidence": 0.9998459219932556, "text_region": [[1237.0, 676.0], [1291.0, 676.0], [1291.0, 703.0], [1237.0, 703.0]]}, {"text": "0.5", "confidence": 0.9999085068702698, "text_region": [[1254.0, 772.0], [1291.0, 772.0], [1291.0, 802.0], [1254.0, 802.0]]}, {"text": "ics-700", "confidence": 0.9730706214904785, "text_region": [[412.0, 822.0], [486.0, 822.0], [486.0, 845.0], [412.0, 845.0]]}, {"text": "!", "confidence": 0.6674312353134155, "text_region": [[223.0, 874.0], [239.0, 874.0], [239.0, 898.0], [223.0, 898.0]]}, {"text": "0.75", "confidence": 0.9996622204780579, "text_region": [[1238.0, 880.0], [1281.0, 866.0], [1290.0, 891.0], [1247.0, 906.0]]}, {"text": "5.0", "confidence": 0.9998490214347839, "text_region": [[462.0, 891.0], [519.0, 891.0], [519.0, 927.0], [462.0, 927.0]]}, {"text": "10.012.515.017.520.022.5", "confidence": 0.997455894947052, "text_region": [[1640.0, 884.0], [2169.0, 884.0], [2169.0, 927.0], [1640.0, 927.0]]}, {"text": "2.5", "confidence": 0.9999501705169678, "text_region": [[379.0, 894.0], [419.0, 894.0], [419.0, 921.0], [379.0, 921.0]]}, {"text": "7.5", "confidence": 0.9999279975891113, "text_region": [[565.0, 894.0], [619.0, 894.0], [619.0, 921.0], [565.0, 921.0]]}, {"text": "2.5", "confidence": 0.9999501705169678, "text_region": [[1380.0, 894.0], [1420.0, 894.0], [1420.0, 921.0], [1380.0, 921.0]]}, {"text": "5.0", "confidence": 0.9993998408317566, "text_region": [[1477.0, 894.0], [1513.0, 894.0], [1513.0, 921.0], [1477.0, 921.0]]}, {"text": "Detected Data Overlap (%)", "confidence": 0.9963160753250122, "text_region": [[572.0, 927.0], [875.0, 927.0], [875.0, 960.0], [572.0, 960.0]]}, {"text": "Detected Data Overlap (%)", "confidence": 0.9633451104164124, "text_region": [[1577.0, 927.0], [1879.0, 927.0], [1879.0, 960.0], [1577.0, 960.0]]}], "img_idx": 0, "score": 0.9629932641983032}
{"type": "header", "bbox": [2129, 193, 2165, 218], "res": [{"text": "19", "confidence": 0.9998740553855896, "text_region": [[2129.0, 191.0], [2169.0, 191.0], [2169.0, 221.0], [2129.0, 221.0]]}], "img_idx": 0, "score": 0.9092490673065186}
{"type": "header", "bbox": [228, 193, 1125, 224], "res": [{"text": "Learning Transferable Visual Models From Natural Language Supervision", "confidence": 0.990749716758728, "text_region": [[220.0, 188.0], [1370.0, 188.0], [1370.0, 231.0], [220.0, 231.0]]}], "img_idx": 0, "score": 0.726256251335144}
