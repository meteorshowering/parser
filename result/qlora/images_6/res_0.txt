{"type": "text", "bbox": [429, 1111, 1460, 1448], "res": [{"text": "The first focuses on a comparison with full 16-bit finetuning", "confidence": 0.9929596185684204, "text_region": [[423.0, 1095.0], [1460.0, 1102.0], [1460.0, 1158.0], [422.0, 1152.0]]}, {"text": "of RoBERTA and T5 models sized 125M to 3B parameters on", "confidence": 0.9897705316543579, "text_region": [[429.0, 1148.0], [1460.0, 1148.0], [1460.0, 1195.0], [429.0, 1195.0]]}, {"text": "GLUE and the Super-Naturallnstructions dataset. Results are", "confidence": 0.9861224293708801, "text_region": [[423.0, 1185.0], [1460.0, 1188.0], [1460.0, 1244.0], [422.0, 1241.0]]}, {"text": "shown in Table 3. In both datasets, we observe that 16-bit, 8-bit,", "confidence": 0.9991543889045715, "text_region": [[429.0, 1238.0], [1463.0, 1238.0], [1463.0, 1280.0], [429.0, 1280.0]]}, {"text": "and 4-bit adapter methods replicate the performance of the fully", "confidence": 0.9952363967895508, "text_region": [[429.0, 1280.0], [1460.0, 1280.0], [1460.0, 1327.0], [429.0, 1327.0]]}, {"text": "finetuned 16-bit baseline. This suggests that the performance lost", "confidence": 0.9817589521408081, "text_region": [[422.0, 1320.0], [1463.0, 1320.0], [1463.0, 1376.0], [422.0, 1376.0]]}, {"text": "due to the imprecise quantization can be fully recovered through", "confidence": 0.9970132112503052, "text_region": [[429.0, 1370.0], [1457.0, 1370.0], [1457.0, 1416.0], [429.0, 1416.0]]}, {"text": "adapter finetuning after quantization.", "confidence": 0.9997502565383911, "text_region": [[426.0, 1409.0], [1021.0, 1412.0], [1021.0, 1459.0], [426.0, 1455.0]]}], "img_idx": 0, "score": 0.9734411835670471}
{"type": "text", "bbox": [431, 2764, 2015, 2888], "res": [{"text": " Having established that 4-bit QLoRA matches 16-bit performance across scales, tasks, and datasets", "confidence": 0.9862370491027832, "text_region": [[422.0, 2752.0], [2022.0, 2756.0], [2022.0, 2812.0], [422.0, 2808.0]]}, {"text": "we conduct an in-depth study of instruction finetuning up to the largest open-source language models", "confidence": 0.9986863732337952, "text_region": [[429.0, 2805.0], [2019.0, 2805.0], [2019.0, 2851.0], [429.0, 2851.0]]}, {"text": "available for research. To assess the performance of instruction finetuning these models, we evaluate", "confidence": 0.9837881922721863, "text_region": [[426.0, 2845.0], [2019.0, 2848.0], [2019.0, 2894.0], [426.0, 2891.0]]}], "img_idx": 0, "score": 0.9730803966522217}
{"type": "text", "bbox": [429, 1495, 1458, 1696], "res": [{"text": "For our second setup, since full finetuning models at and beyond", "confidence": 0.9819387793540955, "text_region": [[426.0, 1485.0], [1460.0, 1485.0], [1460.0, 1531.0], [426.0, 1531.0]]}, {"text": "11B parameters requires more than one server of high memory", "confidence": 0.9929379224777222, "text_region": [[426.0, 1524.0], [1457.0, 1531.0], [1457.0, 1578.0], [426.0, 1571.0]]}, {"text": "GPUs, we continue to test whether 4-bit QLoRA can match", "confidence": 0.9964179992675781, "text_region": [[429.0, 1571.0], [1460.0, 1571.0], [1460.0, 1617.0], [429.0, 1617.0]]}, {"text": " 16-bit LoRA at the 7B to 65B parameter scales. To this end, we", "confidence": 0.9883281588554382, "text_region": [[419.0, 1607.0], [1460.0, 1610.0], [1460.0, 1667.0], [419.0, 1663.0]]}, {"text": " finetune LLaMA 7B through 65B on two instruction following", "confidence": 0.9888917803764343, "text_region": [[419.0, 1650.0], [1464.0, 1657.0], [1463.0, 1713.0], [419.0, 1706.0]]}], "img_idx": 0, "score": 0.9654687643051147}
{"type": "text", "bbox": [430, 2249, 2020, 2621], "res": [{"text": "In line with previous work on quantization [13], our MMLU and Elo results indicate that with a given", "confidence": 0.9899978637695312, "text_region": [[426.0, 2247.0], [2019.0, 2247.0], [2019.0, 2294.0], [426.0, 2294.0]]}, {"text": "finetuning and inference resource budget it is beneficial to increase the number of parameters in the", "confidence": 0.9948285818099976, "text_region": [[429.0, 2294.0], [2022.0, 2294.0], [2022.0, 2340.0], [429.0, 2340.0]]}, {"text": "base model while decreasing their precision. This highlights the importance of efficiency benefits", "confidence": 0.9909898638725281, "text_region": [[429.0, 2336.0], [2019.0, 2336.0], [2019.0, 2383.0], [429.0, 2383.0]]}, {"text": "from QLoRA. Since we did not observe performance degradation compared to full-finetuning in", "confidence": 0.9946955442428589, "text_region": [[426.0, 2379.0], [2019.0, 2379.0], [2019.0, 2426.0], [426.0, 2426.0]]}, {"text": "our experiments with 4-bit finetuning, this raises the question of where the performance-precision", "confidence": 0.9892417192459106, "text_region": [[426.0, 2419.0], [2026.0, 2419.0], [2026.0, 2475.0], [426.0, 2475.0]]}, {"text": "trade-off exactly lies for QLoRA tuning, which we leave to future work to explore.", "confidence": 0.9937861561775208, "text_region": [[429.0, 2468.0], [1750.0, 2468.0], [1750.0, 2515.0], [429.0, 2515.0]]}, {"text": "We proceed to investigate instruction tuning at scales that would be impossible to explore with full", "confidence": 0.9970362782478333, "text_region": [[429.0, 2541.0], [2022.0, 2541.0], [2022.0, 2587.0], [429.0, 2587.0]]}, {"text": "16-bit finetuning on academic research hardware.", "confidence": 0.9981171488761902, "text_region": [[423.0, 2581.0], [1217.0, 2584.0], [1217.0, 2630.0], [422.0, 2627.0]]}], "img_idx": 0, "score": 0.9144665598869324}
{"type": "text", "bbox": [426, 948, 2023, 1074], "res": [{"text": " possible, but leads to performance degradation rel-", "confidence": 0.9749282002449036, "text_region": [[422.0, 941.0], [1250.0, 934.0], [1251.0, 990.0], [423.0, 997.0]]}, {"text": "ative to 16-bit [13, 18]. This raises the crucial question of whether the lost performance can be", "confidence": 0.9956451654434204, "text_region": [[429.0, 990.0], [2019.0, 990.0], [2019.0, 1033.0], [429.0, 1033.0]]}, {"text": " recovered by conducting 4-bit adapter finetuning. We test this for two setups.", "confidence": 0.9925265908241272, "text_region": [[419.0, 1026.0], [1663.0, 1030.0], [1663.0, 1086.0], [419.0, 1082.0]]}], "img_idx": 0, "score": 0.7892643213272095}
{"type": "title", "bbox": [430, 2682, 1523, 2726], "res": [{"text": "5Pushing the Chatbot State-of-the-art with QLoRA", "confidence": 0.9875925183296204, "text_region": [[446.0, 2673.0], [1530.0, 2676.0], [1530.0, 2732.0], [446.0, 2729.0]]}, {"text": "5", "confidence": 0.9993205070495605, "text_region": [[432.0, 2686.0], [466.0, 2686.0], [466.0, 2716.0], [432.0, 2716.0]]}], "img_idx": 0, "score": 0.926641583442688}
{"type": "table", "bbox": [424, 423, 2030, 863], "res": {"cell_bbox": [[35.84571838378906, 17.889949798583984, 234.83856201171875, 18.545042037963867, 234.98944091796875, 97.88617706298828, 34.766075134277344, 96.7098388671875], [426.731689453125, 16.64370346069336, 734.35888671875, 16.945301055908203, 736.139892578125, 85.30484771728516, 427.4152526855469, 85.03562927246094], [806.2252807617188, 13.555635452270508, 1386.9638671875, 13.599479675292969, 1390.3289794921875, 65.10794830322266, 812.8534545898438, 65.80585479736328], [74.83575439453125, 78.85667419433594, 310.0671691894531, 78.77111053466797, 308.49176025390625, 131.02658081054688, 73.17675018310547, 131.23748779296875], [412.2846374511719, 73.04899597167969, 694.3599243164062, 72.63031005859375, 699.56201171875, 120.9145736694336, 415.53472900390625, 121.38996887207031], [763.387451171875, 73.73346710205078, 902.9736328125, 73.32476806640625, 911.2669677734375, 127.13642120361328, 770.7998657226562, 128.08840942382812], [942.2394409179688, 74.1885757446289, 1166.64892578125, 74.07921600341797, 1169.270263671875, 121.32930755615234, 946.2349243164062, 121.93222045898438], [1140.9964599609375, 75.2698974609375, 1290.803466796875, 75.33428192138672, 1291.68798828125, 120.52568817138672, 1142.69677734375, 120.61546325683594], [1321.66943359375, 75.6870346069336, 1404.9056396484375, 75.73556518554688, 1405.8282470703125, 118.45283508300781, 1323.5648193359375, 118.4787368774414], [1460.8765869140625, 75.86275482177734, 1569.9486083984375, 76.02495574951172, 1569.9957275390625, 121.50725555419922, 1461.8619384765625, 121.44955444335938], [36.19486618041992, 145.7745819091797, 322.4870910644531, 145.3217010498047, 317.4660339355469, 209.0075225830078, 34.53745651245117, 209.0205841064453], [571.5051879882812, 153.20262145996094, 657.4966430664062, 152.62081909179688, 653.7265625, 196.19224548339844, 565.7440795898438, 196.62892150878906], [760.7361450195312, 145.7182159423828, 841.2275390625, 144.39857482910156, 842.4410400390625, 192.1023406982422, 759.1763916015625, 193.68560791015625], [959.8972778320312, 143.58572387695312, 1038.580078125, 142.8057403564453, 1036.1387939453125, 189.1533660888672, 957.1198120117188, 190.21304321289062], [1174.09814453125, 142.54315185546875, 1245.4468994140625, 142.05210876464844, 1243.3994140625, 189.4176788330078, 1171.86279296875, 190.1267852783203], [1322.1890869140625, 143.74310302734375, 1392.630615234375, 143.18154907226562, 1392.1539306640625, 190.09800720214844, 1321.7811279296875, 190.8683319091797], [1473.252685546875, 143.17881774902344, 1574.445556640625, 142.46115112304688, 1574.0447998046875, 192.2732696533203, 1472.4111328125, 193.20388793945312], [52.43506622314453, 206.46871948242188, 304.6788635253906, 205.80972290039062, 302.4530029296875, 254.0177764892578, 51.20170593261719, 254.55320739746094], [580.2888793945312, 206.21694946289062, 659.5104370117188, 205.74562072753906, 659.2139282226562, 245.57830810546875, 578.1693725585938, 246.08314514160156], [763.0086059570312, 201.39251708984375, 843.00390625, 200.759033203125, 842.4815063476562, 241.65594482421875, 762.436767578125, 242.73619079589844], [951.142578125, 199.8869171142578, 1084.2601318359375, 199.39500427246094, 1081.9443359375, 240.0342254638672, 949.6075439453125, 240.93850708007812], [1183.2843017578125, 199.99916076660156, 1264.9339599609375, 199.62091064453125, 1262.984130859375, 241.651123046875, 1181.412109375, 242.26107788085938], [1331.291015625, 200.3815155029297, 1401.1097412109375, 199.84803771972656, 1400.7672119140625, 242.94175720214844, 1330.9072265625, 243.68392944335938], [1487.5927734375, 200.1804656982422, 1577.4437255859375, 199.68661499023438, 1577.1337890625, 245.5013427734375, 1486.9693603515625, 246.27186584472656], [40.665931701660156, 259.88909912109375, 277.421142578125, 260.0591125488281, 276.4220886230469, 298.0478210449219, 39.69113540649414, 298.0742492675781], [586.9744873046875, 259.06109619140625, 658.9639892578125, 259.4936218261719, 656.9522705078125, 295.6741027832031, 583.720947265625, 295.56622314453125], [767.43310546875, 257.36944580078125, 845.3584594726562, 257.6106262207031, 843.6152954101562, 293.2118835449219, 765.668701171875, 293.5025634765625], [962.9942626953125, 256.8467102050781, 1049.6475830078125, 257.0348205566406, 1047.6363525390625, 291.97418212890625, 961.6016845703125, 292.179931640625], [1183.0894775390625, 256.30987548828125, 1260.0716552734375, 256.47088623046875, 1258.3651123046875, 292.25311279296875, 1181.1177978515625, 292.4070129394531], [1332.5308837890625, 256.8791198730469, 1397.2193603515625, 256.9532165527344, 1396.85400390625, 294.386962890625, 1332.1884765625, 294.5633544921875], [1489.68212890625, 257.152587890625, 1574.9029541015625, 257.2372131347656, 1574.542236328125, 294.9920654296875, 1488.5543212890625, 295.30316162109375], [37.68207550048828, 303.34283447265625, 269.4967346191406, 303.19476318359375, 269.8486633300781, 340.75909423828125, 37.175926208496094, 340.9290466308594], [589.0560913085938, 302.6302795410156, 658.8118896484375, 302.6474609375, 656.6129150390625, 337.9665832519531, 586.0200805664062, 338.11767578125], [770.4318237304688, 301.1007385253906, 841.3870849609375, 301.1196594238281, 839.9379272460938, 337.8525390625, 769.229248046875, 338.21881103515625], [963.9181518554688, 300.874267578125, 1045.0140380859375, 300.87213134765625, 1042.748779296875, 336.8653259277344, 962.2100830078125, 337.1924133300781], [1178.268310546875, 301.2685241699219, 1255.6298828125, 301.0827331542969, 1253.8087158203125, 336.27386474609375, 1176.3172607421875, 336.6153564453125], [1328.177978515625, 302.373291015625, 1395.3424072265625, 302.09454345703125, 1395.0003662109375, 337.7243347167969, 1328.1407470703125, 338.0574951171875], [1486.6168212890625, 303.1143798828125, 1573.881103515625, 302.87493896484375, 1573.5850830078125, 337.6163635253906, 1485.9666748046875, 337.9631042480469], [32.860816955566406, 345.8076171875, 284.3134460449219, 345.8383483886719, 285.83148193359375, 382.95184326171875, 32.51344299316406, 382.9352111816406], [592.590087890625, 346.3039855957031, 659.33447265625, 346.30157470703125, 657.8292846679688, 381.1838073730469, 589.917724609375, 381.21417236328125], [771.1803588867188, 345.84210205078125, 844.2440185546875, 345.7306823730469, 843.5545654296875, 382.0864562988281, 770.1912231445312, 382.34210205078125], [966.0880126953125, 345.7777404785156, 1044.5015869140625, 345.6210632324219, 1042.71435546875, 382.2663879394531, 964.3782958984375, 382.51025390625], [1177.4461669921875, 345.93115234375, 1251.5589599609375, 345.687744140625, 1250.6209716796875, 382.0672607421875, 1176.4097900390625, 382.2832336425781], [1323.4840087890625, 346.52142333984375, 1392.9014892578125, 346.1764221191406, 1393.051513671875, 383.3094177246094, 1324.16748046875, 383.5643310546875], [1486.8795166015625, 347.1120300292969, 1574.1378173828125, 346.7950439453125, 1574.1595458984375, 382.9159851074219, 1487.4515380859375, 383.1741638183594], [28.974000930786133, 386.2538757324219, 362.5569763183594, 386.6289978027344, 366.51678466796875, 423.20758056640625, 28.664241790771484, 422.9042663574219], [593.6298828125, 385.13983154296875, 662.4411010742188, 385.5533752441406, 661.8761596679688, 419.7377624511719, 591.1136474609375, 419.4655456542969], [775.3369140625, 385.2699279785156, 852.4027709960938, 385.4352111816406, 853.3470458984375, 422.06866455078125, 775.3651123046875, 422.0052185058594], [965.3255615234375, 386.197998046875, 1047.1124267578125, 386.0863037109375, 1047.099365234375, 421.3188171386719, 964.9791259765625, 421.35308837890625], [1174.0921630859375, 385.5501708984375, 1261.1705322265625, 385.340087890625, 1260.6728515625, 420.0316162109375, 1173.196044921875, 420.1047058105469], [1320.70703125, 385.22222900390625, 1396.3765869140625, 384.75775146484375, 1396.5545654296875, 419.644775390625, 1321.18310546875, 419.81671142578125], [1494.2744140625, 384.0036926269531, 1569.535888671875, 383.32769775390625, 1569.337646484375, 418.839111328125, 1494.2596435546875, 419.10211181640625]], "html": "<html><body><table><thead><tr><td>Dataset Model</td><td>GLUE (Acc.)</td><td colspan=\"5\">Super-NaturalInstructions (RougeL)</td></tr><tr><td></td><td>RoBERTa-large</td><td>T5-80M</td><td>T5-250M</td><td>T5-780M</td><td>T5-3B</td><td>T5-11B</td></tr></thead><tbody><tr><td>BF16 BF16 replication</td><td>88.6 88.6</td><td>40.1 40.0</td><td>42.1</td><td>48.0</td><td>54.3 54.9</td><td>62.0</td></tr><tr><td></td><td></td><td></td><td>42.2</td><td>47.3</td><td></td><td></td></tr><tr><td>LoRA BF16</td><td>88.8</td><td>40.5</td><td>42.6</td><td>47.1</td><td>55.4</td><td>60.7</td></tr><tr><td>QLORA Int8</td><td>88.8</td><td>40.4</td><td>42.9</td><td>45.4</td><td>56.5</td><td>60.7</td></tr><tr><td>QL0RA FP4</td><td>88.6</td><td>40.3</td><td>42.4</td><td>47.5</td><td>55.6</td><td>60.9</td></tr><tr><td>QL0RA NF4 + DQ</td><td>-</td><td>40.4</td><td>42.7</td><td>47.7</td><td>55.3</td><td>60.9</td></tr></tbody></table></body></html>"}, "img_idx": 0, "score": 0.9745573401451111}
{"type": "table", "bbox": [1490, 1346, 2022, 1619], "res": {"cell_bbox": [[5.721976280212402, 2.339301347732544, 505.1118469238281, 2.539348602294922, 504.91522216796875, 73.19020080566406, 5.62657356262207, 70.84917449951172], [13.893816947937012, 77.07585906982422, 522.8057250976562, 79.2948226928711, 522.5324096679688, 216.19866943359375, 13.288390159606934, 214.60572814941406], [26.079742431640625, 142.75167846679688, 512.5538330078125, 141.35263061523438, 512.2265014648438, 244.5623016357422, 25.633554458618164, 244.87281799316406], [42.02141189575195, 184.18759155273438, 515.0524291992188, 183.42156982421875, 514.9415893554688, 254.27447509765625, 41.24848175048828, 254.1484832763672]], "html": "<html><body><table><tbody><tr><td>Data type Mean PPL</td></tr><tr><td>Int4 34.34 Float4 (E2M1) 31.07</td></tr><tr><td>Float4 (E3M0) 29.48</td></tr><tr><td>NFloat4 + DQ 27.41</td></tr></tbody></table></body></html>"}, "img_idx": 0, "score": 0.9167147278785706}
{"type": "table_caption", "bbox": [433, 319, 2022, 388], "res": [{"text": "Table 3: Experiments comparing 16-bit BrainFloat (BF16), 8-bit Integer (Int8), 4-bit Float (FP4), and 4-", "confidence": 0.9938970804214478, "text_region": [[426.0, 314.0], [2022.0, 314.0], [2022.0, 356.0], [426.0, 356.0]]}, {"text": "bit NormalFloat (NF4) on GLUE and Super-NaturalInstructions. QLoRA replicates 16-bit LoRA and full-", "confidence": 0.9855181574821472, "text_region": [[426.0, 353.0], [2022.0, 353.0], [2022.0, 396.0], [426.0, 396.0]]}, {"text": "finetuning.", "confidence": 0.9989349246025085, "text_region": [[424.0, 385.0], [589.0, 393.0], [587.0, 440.0], [422.0, 432.0]]}], "img_idx": 0, "score": 0.9093271493911743}
{"type": "table_caption", "bbox": [1490, 1157, 2017, 1316], "res": [{"text": "Table 2: Pile Common Crawl mean", "confidence": 0.9950058460235596, "text_region": [[1484.0, 1155.0], [2022.0, 1158.0], [2022.0, 1205.0], [1483.0, 1201.0]]}, {"text": "perplexity for different data types", "confidence": 0.9959729909896851, "text_region": [[1490.0, 1198.0], [2019.0, 1198.0], [2019.0, 1244.0], [1490.0, 1244.0]]}, {"text": "for 125M to 13B OPT, BLOOM,", "confidence": 0.983733057975769, "text_region": [[1483.0, 1238.0], [2026.0, 1238.0], [2026.0, 1280.0], [1483.0, 1280.0]]}, {"text": "LLaMA, and Pythia models.", "confidence": 0.9958229064941406, "text_region": [[1487.0, 1277.0], [1906.0, 1277.0], [1906.0, 1323.0], [1487.0, 1323.0]]}], "img_idx": 0, "score": 0.7794564962387085}
{"type": "footer", "bbox": [1214, 2973, 1240, 2998], "res": [{"text": "7", "confidence": 0.9985495209693909, "text_region": [[1207.0, 2970.0], [1237.0, 2970.0], [1237.0, 3003.0], [1207.0, 3003.0]]}], "img_idx": 0, "score": 0.605096697807312}
