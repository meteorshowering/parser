{"type": "text", "bbox": [427, 720, 1244, 1235], "res": [{"text": "While paged optimizers are critical to do 33B/65B", "confidence": 0.9921165704727173, "text_region": [[429.0, 716.0], [1244.0, 716.0], [1244.0, 762.0], [429.0, 762.0]]}, {"text": "QLoRA tuning on a single 24/48GB GPU, we do", "confidence": 0.9810327291488647, "text_region": [[432.0, 759.0], [1244.0, 759.0], [1244.0, 805.0], [432.0, 805.0]]}, {"text": "not provide hard measurements for Paged Optimiz-", "confidence": 0.9987927079200745, "text_region": [[426.0, 802.0], [1244.0, 802.0], [1244.0, 848.0], [426.0, 848.0]]}, {"text": "ers since the paging only occurs when processing", "confidence": 0.9793230891227722, "text_region": [[429.0, 848.0], [1244.0, 848.0], [1244.0, 894.0], [429.0, 894.0]]}, {"text": " mini-batches with long sequence lengths, which is", "confidence": 0.9866446852684021, "text_region": [[422.0, 884.0], [1251.0, 884.0], [1251.0, 940.0], [422.0, 940.0]]}, {"text": "rare. We do, however, perform an analysis of the", "confidence": 0.9969625473022461, "text_region": [[422.0, 931.0], [1241.0, 931.0], [1241.0, 977.0], [422.0, 977.0]]}, {"text": "runtime of paged optimizers for 65B models on", "confidence": 0.9769014120101929, "text_region": [[429.0, 977.0], [1247.0, 977.0], [1247.0, 1023.0], [429.0, 1023.0]]}, {"text": "48GB GPUs and find that with a batch size of 16,", "confidence": 0.9914534091949463, "text_region": [[429.0, 1020.0], [1244.0, 1020.0], [1244.0, 1063.0], [429.0, 1063.0]]}, {"text": " paged optimizers provide the same training speed", "confidence": 0.9841705560684204, "text_region": [[422.0, 1059.0], [1251.0, 1059.0], [1251.0, 1115.0], [422.0, 1115.0]]}, {"text": "as regular optimizers. Future work should measure", "confidence": 0.9904910326004028, "text_region": [[426.0, 1109.0], [1244.0, 1109.0], [1244.0, 1152.0], [426.0, 1152.0]]}, {"text": " and characterize under what circumstances slow-", "confidence": 0.9951680302619934, "text_region": [[422.0, 1152.0], [1247.0, 1148.0], [1247.0, 1195.0], [423.0, 1198.0]]}, {"text": " downs occur from the paging process.", "confidence": 0.9933652281761169, "text_region": [[419.0, 1188.0], [1041.0, 1195.0], [1041.0, 1248.0], [419.0, 1241.0]]}], "img_idx": 0, "score": 0.9896880984306335}
{"type": "text", "bbox": [427, 2119, 1241, 2725], "res": [{"text": "4-bit NormalFloat yields better performance", "confidence": 0.9919564127922058, "text_region": [[426.0, 2112.0], [1241.0, 2115.0], [1241.0, 2162.0], [426.0, 2158.0]]}, {"text": "than 4-bit Floating Point While the 4-bit", "confidence": 0.9791398644447327, "text_region": [[426.0, 2155.0], [1247.0, 2155.0], [1247.0, 2211.0], [426.0, 2211.0]]}, {"text": "NormalFloat (NF4) data type is information-", "confidence": 0.9669429063796997, "text_region": [[426.0, 2204.0], [1244.0, 2204.0], [1244.0, 2251.0], [426.0, 2251.0]]}, {"text": "theoretically optimal, it still needs to be determined ", "confidence": 0.9728038311004639, "text_region": [[426.0, 2247.0], [1247.0, 2247.0], [1247.0, 2294.0], [426.0, 2294.0]]}, {"text": "if this property translates to empirical advantages.", "confidence": 0.9830556511878967, "text_region": [[422.0, 2287.0], [1251.0, 2287.0], [1251.0, 2343.0], [422.0, 2343.0]]}, {"text": "We follow the setup from Dettmers and Zettlemoyer", "confidence": 0.9846305251121521, "text_region": [[426.0, 2333.0], [1244.0, 2333.0], [1244.0, 2379.0], [426.0, 2379.0]]}, {"text": "[13] where quantized LLMs (OPT [72], BLOOM", "confidence": 0.9968436360359192, "text_region": [[426.0, 2376.0], [1244.0, 2373.0], [1244.0, 2419.0], [426.0, 2422.0]]}, {"text": "[52], Pythia [7], LLaMA) of different sizes (125M ", "confidence": 0.9777830243110657, "text_region": [[429.0, 2422.0], [1251.0, 2422.0], [1251.0, 2465.0], [429.0, 2465.0]]}, {"text": "to 65B) with different data types are evaluated on", "confidence": 0.9855807423591614, "text_region": [[426.0, 2465.0], [1251.0, 2465.0], [1251.0, 2511.0], [426.0, 2511.0]]}, {"text": " language modeling and a set of zero-shot tasks. In", "confidence": 0.9880015254020691, "text_region": [[419.0, 2505.0], [1250.0, 2498.0], [1251.0, 2554.0], [419.0, 2561.0]]}, {"text": " Figure 3 and Table 2 we see that NF4 improves per-", "confidence": 0.9790080785751343, "text_region": [[419.0, 2544.0], [1251.0, 2548.0], [1250.0, 2604.0], [419.0, 2600.0]]}, {"text": "formance significantly over FP4 and Int4 and that", "confidence": 0.9989666938781738, "text_region": [[426.0, 2597.0], [1247.0, 2597.0], [1247.0, 2643.0], [426.0, 2643.0]]}, {"text": "double quantization reduces the memory footprint", "confidence": 0.9956426620483398, "text_region": [[426.0, 2640.0], [1247.0, 2640.0], [1247.0, 2686.0], [426.0, 2686.0]]}, {"text": "without degrading performance.", "confidence": 0.9992637634277344, "text_region": [[426.0, 2686.0], [941.0, 2686.0], [941.0, 2732.0], [426.0, 2732.0]]}], "img_idx": 0, "score": 0.9892401695251465}
{"type": "text", "bbox": [428, 1276, 1241, 1787], "res": [{"text": "Default LoRA hyperparameters do not match 16-", "confidence": 0.9958071112632751, "text_region": [[426.0, 1270.0], [1244.0, 1270.0], [1244.0, 1317.0], [426.0, 1317.0]]}, {"text": "bit performance", "confidence": 0.9997738003730774, "text_region": [[427.0, 1310.0], [726.0, 1321.0], [724.0, 1367.0], [425.0, 1356.0]]}, {"text": "When using the standard prac-", "confidence": 0.9923641681671143, "text_region": [[742.0, 1317.0], [1244.0, 1317.0], [1244.0, 1363.0], [742.0, 1363.0]]}, {"text": "tice of applying LoRA to query and value attention", "confidence": 0.9902942180633545, "text_region": [[422.0, 1360.0], [1241.0, 1360.0], [1241.0, 1406.0], [422.0, 1406.0]]}, {"text": "projection matrices [28], we are not able to replicate ", "confidence": 0.9879992604255676, "text_region": [[426.0, 1402.0], [1247.0, 1402.0], [1247.0, 1449.0], [426.0, 1449.0]]}, {"text": "full finetuning performance for large base models.", "confidence": 0.9990655779838562, "text_region": [[429.0, 1445.0], [1247.0, 1445.0], [1247.0, 1492.0], [429.0, 1492.0]]}, {"text": "As shown in Figure 2 for LLaMA 7B finetuning on", "confidence": 0.9990338087081909, "text_region": [[423.0, 1478.0], [1251.0, 1485.0], [1250.0, 1541.0], [422.0, 1534.0]]}, {"text": "Alpaca, we find that the most critical LoRA hyper-", "confidence": 0.9873136281967163, "text_region": [[426.0, 1534.0], [1244.0, 1534.0], [1244.0, 1581.0], [426.0, 1581.0]]}, {"text": "parameter is how many LoRA adapters are used in", "confidence": 0.9971061944961548, "text_region": [[429.0, 1577.0], [1247.0, 1577.0], [1247.0, 1624.0], [429.0, 1624.0]]}, {"text": "total and that LoRA on all linear transformer block", "confidence": 0.9961824417114258, "text_region": [[426.0, 1620.0], [1247.0, 1620.0], [1247.0, 1667.0], [426.0, 1667.0]]}, {"text": " layers are required to match full finetuning perfor-", "confidence": 0.9824410676956177, "text_region": [[419.0, 1657.0], [1251.0, 1660.0], [1250.0, 1716.0], [419.0, 1713.0]]}, {"text": " mance. Other LoRA hyperparameters, such as the", "confidence": 0.9782418012619019, "text_region": [[422.0, 1703.0], [1247.0, 1703.0], [1247.0, 1759.0], [422.0, 1759.0]]}, {"text": "projection dimension r, do not affect performance (see Appendix A).", "confidence": 0.9967759251594543, "text_region": [[429.0, 1752.0], [1533.0, 1752.0], [1533.0, 1798.0], [429.0, 1798.0]]}], "img_idx": 0, "score": 0.9878072142601013}
{"type": "text", "bbox": [1278, 2438, 2021, 2829], "res": [{"text": " Figure 3: Mean zero-shot accuracy over Wino-", "confidence": 0.9843983054161072, "text_region": [[1267.0, 2425.0], [2026.0, 2429.0], [2025.0, 2475.0], [1267.0, 2472.0]]}, {"text": "grande, HellaSwag, PiQA, Arc-Easy, and Arc-", "confidence": 0.96712726354599, "text_region": [[1271.0, 2472.0], [2026.0, 2472.0], [2026.0, 2518.0], [1271.0, 2518.0]]}, {"text": " Challenge using LLaMA models with different 4-bit ", "confidence": 0.9732735753059387, "text_region": [[1271.0, 2511.0], [2026.0, 2511.0], [2026.0, 2557.0], [1271.0, 2557.0]]}, {"text": "data types. The NormalFloat data type significantly", "confidence": 0.998339831829071, "text_region": [[1274.0, 2551.0], [2019.0, 2551.0], [2019.0, 2597.0], [1274.0, 2597.0]]}, {"text": " improves the bit-for-bit accuracy gains compared", "confidence": 0.9794469475746155, "text_region": [[1267.0, 2587.0], [2022.0, 2587.0], [2022.0, 2643.0], [1267.0, 2643.0]]}, {"text": "to regular 4-bit Floats. While Double Quantization", "confidence": 0.9985849261283875, "text_region": [[1274.0, 2630.0], [2022.0, 2630.0], [2022.0, 2676.0], [1274.0, 2676.0]]}, {"text": "(DQ) only leads to minor gains, it allows for a more", "confidence": 0.9962481260299683, "text_region": [[1271.0, 2670.0], [2022.0, 2670.0], [2022.0, 2716.0], [1271.0, 2716.0]]}, {"text": "fine-grained control over the memory footprint to fit", "confidence": 0.9848251938819885, "text_region": [[1274.0, 2713.0], [2022.0, 2713.0], [2022.0, 2756.0], [1274.0, 2756.0]]}, {"text": "models of certain size (33B/65B) into certain GPUs", "confidence": 0.9991393089294434, "text_region": [[1271.0, 2749.0], [2022.0, 2749.0], [2022.0, 2795.0], [1271.0, 2795.0]]}, {"text": "(24/48GB).", "confidence": 0.9997864961624146, "text_region": [[1272.0, 2781.0], [1448.0, 2789.0], [1445.0, 2835.0], [1270.0, 2827.0]]}], "img_idx": 0, "score": 0.9868179559707642}
{"type": "text", "bbox": [424, 300, 2021, 685], "res": [{"text": "Experimental setup.", "confidence": 0.9878104329109192, "text_region": [[423.0, 290.0], [779.0, 297.0], [778.0, 344.0], [422.0, 336.0]]}, {"text": ". We consider three architectures (encoder, encoder-decoder, and decoder only)", "confidence": 0.9906412959098816, "text_region": [[758.0, 294.0], [2022.0, 294.0], [2022.0, 340.0], [758.0, 340.0]]}, {"text": " and compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for models up to 3B. Our", "confidence": 0.9854968190193176, "text_region": [[422.0, 333.0], [2026.0, 333.0], [2026.0, 389.0], [422.0, 389.0]]}, {"text": "evaluations include GLUE [58] with RoBERTa-large [38], Super-NaturalInstructions (TKInstruct)", "confidence": 0.9865157604217529, "text_region": [[426.0, 383.0], [2022.0, 383.0], [2022.0, 429.0], [426.0, 429.0]]}, {"text": "[61] with T5 [49], and 5-shot MMLU [24] after finetuning LLaMA on Flan v2 [39] and Alpaca", "confidence": 0.9948683977127075, "text_region": [[429.0, 426.0], [2019.0, 426.0], [2019.0, 472.0], [429.0, 472.0]]}, {"text": "[55]. To additionally study the advantages of NF4 over other 4-bit data types, we use the setup of", "confidence": 0.9938750267028809, "text_region": [[426.0, 459.0], [2026.0, 465.0], [2025.0, 521.0], [426.0, 515.0]]}, {"text": " Dettmers and Zetlemoyer [13] and measure post-quantization zero-shot accuracy and perplexity", "confidence": 0.9843745231628418, "text_region": [[419.0, 505.0], [2022.0, 508.0], [2022.0, 564.0], [419.0, 561.0]]}, {"text": "across different models (OPT [72], LLaMA [57], BLOOM [52], Pythia [7]) for model sizes 125m -", "confidence": 0.9910064339637756, "text_region": [[426.0, 558.0], [2016.0, 558.0], [2016.0, 604.0], [426.0, 604.0]]}, {"text": " 13B. We provide more details in the results section for each particular setup to make the results more", "confidence": 0.989005446434021, "text_region": [[422.0, 597.0], [2016.0, 597.0], [2016.0, 644.0], [422.0, 644.0]]}, {"text": "readable. Full details in Appendix A.", "confidence": 0.9979045987129211, "text_region": [[422.0, 640.0], [1028.0, 640.0], [1028.0, 696.0], [422.0, 696.0]]}], "img_idx": 0, "score": 0.9858294725418091}
{"type": "text", "bbox": [428, 1828, 1237, 2079], "res": [{"text": "Similarly, we find that default hyperparameters for", "confidence": 0.975427508354187, "text_region": [[426.0, 1818.0], [1247.0, 1818.0], [1247.0, 1874.0], [426.0, 1874.0]]}, {"text": " fully finetuned baselines are undertuned. We do a", "confidence": 0.9883183836936951, "text_region": [[422.0, 1861.0], [1244.0, 1861.0], [1244.0, 1907.0], [422.0, 1907.0]]}, {"text": "hyperparameter search over learning rates 1e-6 to", "confidence": 0.9903062582015991, "text_region": [[429.0, 1911.0], [1244.0, 1911.0], [1244.0, 1957.0], [429.0, 1957.0]]}, {"text": "5e-5 and batch sizes 8 to 128 to find robust baselines.", "confidence": 0.9889993071556091, "text_region": [[429.0, 1954.0], [1247.0, 1954.0], [1247.0, 1996.0], [429.0, 1996.0]]}, {"text": "Results for 7B LLaMA finetuning on Alpaca are", "confidence": 0.9939448237419128, "text_region": [[426.0, 1993.0], [1244.0, 1997.0], [1244.0, 2043.0], [426.0, 2039.0]]}, {"text": "shown in Figure 2.", "confidence": 0.9995824098587036, "text_region": [[426.0, 2039.0], [732.0, 2039.0], [732.0, 2086.0], [426.0, 2086.0]]}], "img_idx": 0, "score": 0.9842321872711182}
{"type": "text", "bbox": [1276, 1407, 2018, 1682], "res": [{"text": "Figure 2: RougeL for LLaMA 7B models on the", "confidence": 0.995026707649231, "text_region": [[1271.0, 1402.0], [2022.0, 1402.0], [2022.0, 1449.0], [1271.0, 1449.0]]}, {"text": "Alpaca dataset. Each point represents a run with a", "confidence": 0.9838678240776062, "text_region": [[1274.0, 1442.0], [2022.0, 1442.0], [2022.0, 1488.0], [1274.0, 1488.0]]}, {"text": " different random seed. We improve on the Stanford", "confidence": 0.9947188496589661, "text_region": [[1271.0, 1482.0], [2022.0, 1482.0], [2022.0, 1528.0], [1271.0, 1528.0]]}, {"text": "Alpaca fully finetuned default hyperparameters to", "confidence": 0.9940882921218872, "text_region": [[1274.0, 1521.0], [2022.0, 1521.0], [2022.0, 1568.0], [1274.0, 1568.0]]}, {"text": "construct a strong 16-bit baseline for comparisons.", "confidence": 0.9989200830459595, "text_region": [[1274.0, 1561.0], [2022.0, 1561.0], [2022.0, 1607.0], [1274.0, 1607.0]]}, {"text": "Using LoRA on all transformer layers is critical to", "confidence": 0.999383270740509, "text_region": [[1274.0, 1604.0], [2026.0, 1604.0], [2026.0, 1647.0], [1274.0, 1647.0]]}, {"text": " match 16-bit performance.", "confidence": 0.9831594824790955, "text_region": [[1268.0, 1637.0], [1663.0, 1644.0], [1662.0, 1690.0], [1267.0, 1683.0]]}], "img_idx": 0, "score": 0.974881112575531}
{"type": "text", "bbox": [427, 2764, 1237, 2886], "res": [{"text": "k-bit QLoRA matches 16-bit full finetuning and", "confidence": 0.9896916747093201, "text_region": [[423.0, 2755.0], [1247.0, 2759.0], [1247.0, 2808.0], [422.0, 2805.0]]}, {"text": "16-bit LoRA performance", "confidence": 0.9990347027778625, "text_region": [[423.0, 2798.0], [882.0, 2805.0], [881.0, 2851.0], [422.0, 2844.0]]}, {"text": " Recent findings have", "confidence": 0.9553446769714355, "text_region": [[875.0, 2805.0], [1247.0, 2805.0], [1247.0, 2851.0], [875.0, 2851.0]]}, {"text": "established that 4-bit quantization for inference is", "confidence": 0.9945417046546936, "text_region": [[426.0, 2845.0], [1244.0, 2848.0], [1244.0, 2894.0], [426.0, 2891.0]]}], "img_idx": 0, "score": 0.970182478427887}
{"type": "figure", "bbox": [1289, 1865, 2029, 2406], "res": [{"text": "4-bit LLaMA", "confidence": 0.997136652469635, "text_region": [[1378.0, 1884.0], [1584.0, 1891.0], [1582.0, 1938.0], [1376.0, 1930.0]]}, {"text": "0.65", "confidence": 0.9994631409645081, "text_region": [[1314.0, 2026.0], [1374.0, 2026.0], [1374.0, 2049.0], [1314.0, 2049.0]]}, {"text": "0.63", "confidence": 0.9998202323913574, "text_region": [[1337.0, 2135.0], [1374.0, 2135.0], [1374.0, 2158.0], [1337.0, 2158.0]]}, {"text": "Data type", "confidence": 0.9414558410644531, "text_region": [[1845.0, 2213.0], [1937.0, 2222.0], [1933.0, 2258.0], [1841.0, 2250.0]]}, {"text": "0.60", "confidence": 0.9998183250427246, "text_region": [[1337.0, 2307.0], [1374.0, 2307.0], [1374.0, 2330.0], [1337.0, 2330.0]]}, {"text": "NFloat", "confidence": 0.9841737747192383, "text_region": [[1866.0, 2300.0], [1919.0, 2300.0], [1919.0, 2323.0], [1866.0, 2323.0]]}, {"text": "Total model bits", "confidence": 0.9748704433441162, "text_region": [[1597.0, 2366.0], [1779.0, 2366.0], [1779.0, 2399.0], [1597.0, 2399.0]]}], "img_idx": 0, "score": 0.9413542151451111}
{"type": "figure", "bbox": [1297, 721, 2010, 1374], "res": [], "img_idx": 0, "score": 0.9345135688781738}
{"type": "footer", "bbox": [1213, 2976, 1240, 3000], "res": [{"text": "6", "confidence": 0.9982911944389343, "text_region": [[1207.0, 2970.0], [1241.0, 2970.0], [1241.0, 3003.0], [1207.0, 3003.0]]}], "img_idx": 0, "score": 0.5824044346809387}
