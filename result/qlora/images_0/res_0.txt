{"type": "text", "bbox": [569, 1141, 1875, 2177], "res": [{"text": "We present QLoRA, an efficient finetuning approach that reduces memory us-", "confidence": 0.9954380393028259, "text_region": [[572.0, 1135.0], [1876.0, 1135.0], [1876.0, 1181.0], [572.0, 1181.0]]}, {"text": "age enough to finetune a 65B parameter model on a single 48GB GPU while", "confidence": 0.97401362657547, "text_region": [[572.0, 1178.0], [1876.0, 1178.0], [1876.0, 1224.0], [572.0, 1224.0]]}, {"text": "preserving full 16-bit finetuning task performance. QLoRA backpropagates gradi-", "confidence": 0.99676513671875, "text_region": [[569.0, 1218.0], [1883.0, 1218.0], [1883.0, 1274.0], [569.0, 1274.0]]}, {"text": "ents through a frozen, 4-bit quantized pretrained language model into Low Rank", "confidence": 0.9903308153152466, "text_region": [[572.0, 1267.0], [1876.0, 1267.0], [1876.0, 1313.0], [572.0, 1313.0]]}, {"text": "Adapters (LoRA). Our best model family, which we name Guanaco, outperforms", "confidence": 0.9879565834999084, "text_region": [[569.0, 1307.0], [1873.0, 1310.0], [1873.0, 1356.0], [569.0, 1353.0]]}, {"text": "all previous openly released models on the Vicuna benchmark, reaching 99.3%", "confidence": 0.9981710910797119, "text_region": [[569.0, 1350.0], [1879.0, 1350.0], [1879.0, 1406.0], [569.0, 1406.0]]}, {"text": "of the performance level of ChatGPT while only requiring 24 hours of finetuning", "confidence": 0.9915663599967957, "text_region": [[572.0, 1396.0], [1876.0, 1396.0], [1876.0, 1442.0], [572.0, 1442.0]]}, {"text": " on a single GPU. QLoRA introduces a number of innovations to save memory", "confidence": 0.9866814613342285, "text_region": [[566.0, 1432.0], [1879.0, 1436.0], [1879.0, 1492.0], [565.0, 1488.0]]}, {"text": "without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that", "confidence": 0.9863958358764648, "text_region": [[569.0, 1478.0], [1883.0, 1478.0], [1883.0, 1534.0], [569.0, 1534.0]]}, {"text": "is information theoretically optimal for normally distributed weights (b) Double", "confidence": 0.9912782907485962, "text_region": [[572.0, 1528.0], [1876.0, 1528.0], [1876.0, 1574.0], [572.0, 1574.0]]}, {"text": "Quantization to reduce the average memory footprint by quantizing the quantization", "confidence": 0.9922354221343994, "text_region": [[572.0, 1571.0], [1876.0, 1571.0], [1876.0, 1617.0], [572.0, 1617.0]]}, {"text": " constants, and (c) Paged Optimizers to manage memory spikes. We use QLoRA", "confidence": 0.9706547856330872, "text_region": [[565.0, 1610.0], [1879.0, 1607.0], [1879.0, 1663.0], [566.0, 1667.0]]}, {"text": "to finetune more than 1,0o0 models, providing a detailed analysis of instruction", "confidence": 0.9821993112564087, "text_region": [[572.0, 1657.0], [1876.0, 1657.0], [1876.0, 1703.0], [572.0, 1703.0]]}, {"text": "following and chatbot performance across 8 instruction datasets, multiple model", "confidence": 0.9963900446891785, "text_region": [[572.0, 1703.0], [1879.0, 1703.0], [1879.0, 1749.0], [572.0, 1749.0]]}, {"text": "types (LLaMA, T5), and model scales that would be infeasible to run with regular", "confidence": 0.9937564730644226, "text_region": [[572.0, 1746.0], [1879.0, 1746.0], [1879.0, 1792.0], [572.0, 1792.0]]}, {"text": "finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA", "confidence": 0.9919012188911438, "text_region": [[565.0, 1785.0], [1879.0, 1779.0], [1879.0, 1835.0], [566.0, 1842.0]]}, {"text": "finetuning on a small high-quality dataset leads to state-of-the-art results, even", "confidence": 0.9974187612533569, "text_region": [[572.0, 1835.0], [1879.0, 1835.0], [1879.0, 1881.0], [572.0, 1881.0]]}, {"text": "when using smaller models than the previous SoTA. We provide a detailed analysis", "confidence": 0.9949197769165039, "text_region": [[572.0, 1878.0], [1879.0, 1878.0], [1879.0, 1924.0], [572.0, 1924.0]]}, {"text": "of chatbot performance based on both human and GPT-4 evaluations showing that", "confidence": 0.9991616606712341, "text_region": [[569.0, 1917.0], [1876.0, 1917.0], [1876.0, 1964.0], [569.0, 1964.0]]}, {"text": "GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Fur-", "confidence": 0.9989941120147705, "text_region": [[572.0, 1964.0], [1879.0, 1964.0], [1879.0, 2010.0], [572.0, 2010.0]]}, {"text": "thermore, we find that current chatbot benchmarks are not trustworthy to accurately", "confidence": 0.9912453293800354, "text_region": [[572.0, 2006.0], [1876.0, 2006.0], [1876.0, 2053.0], [572.0, 2053.0]]}, {"text": "evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates", "confidence": 0.9963602423667908, "text_region": [[572.0, 2053.0], [1876.0, 2053.0], [1876.0, 2099.0], [572.0, 2099.0]]}, {"text": "where Guanaco fails compared to ChatGPT. We release all of our models and code,", "confidence": 0.9961086511611938, "text_region": [[572.0, 2096.0], [1879.0, 2096.0], [1879.0, 2142.0], [572.0, 2142.0]]}, {"text": "including CUDA kernels for 4-bit training.2", "confidence": 0.9988174438476562, "text_region": [[572.0, 2138.0], [1274.0, 2138.0], [1274.0, 2185.0], [572.0, 2185.0]]}], "img_idx": 0, "score": 0.9957284331321716}
{"type": "text", "bbox": [427, 2324, 2017, 2855], "res": [{"text": "Finetuning large language models (LLMs) is a highly effective way to improve their performance,", "confidence": 0.9966959357261658, "text_region": [[429.0, 2320.0], [2019.0, 2320.0], [2019.0, 2366.0], [429.0, 2366.0]]}, {"text": "[40, 62, 43, 61, 59, 37] and to add desirable or remove undesirable behaviors [43, 2, 4]. However,", "confidence": 0.991044819355011, "text_region": [[429.0, 2359.0], [2019.0, 2363.0], [2019.0, 2409.0], [429.0, 2406.0]]}, {"text": "finetuning very large models is prohibitively expensive; regular 16-bit finetuning of a LLaMA 65B", "confidence": 0.9831582903862, "text_region": [[426.0, 2402.0], [2022.0, 2402.0], [2022.0, 2458.0], [426.0, 2458.0]]}, {"text": " parameter model [57] requires more than 780 GB of GPU memory. While recent quantization", "confidence": 0.9631588459014893, "text_region": [[422.0, 2442.0], [2019.0, 2442.0], [2019.0, 2498.0], [422.0, 2498.0]]}, {"text": "methods can reduce the memory footprint of LLMs [14, 13, 18, 66], such techniques only work for", "confidence": 0.9928195476531982, "text_region": [[429.0, 2495.0], [2019.0, 2495.0], [2019.0, 2541.0], [429.0, 2541.0]]}, {"text": "inference and break down during training [65].", "confidence": 0.9935064911842346, "text_region": [[426.0, 2538.0], [1177.0, 2538.0], [1177.0, 2584.0], [426.0, 2584.0]]}, {"text": " We demonstrate for the first time that it is possible to finetune a quantized 4-bit model without any", "confidence": 0.9856075048446655, "text_region": [[419.0, 2604.0], [2022.0, 2607.0], [2022.0, 2663.0], [419.0, 2660.0]]}, {"text": "performance degradation. Our method, QLoRA, uses a novel high-precision technique to quantize", "confidence": 0.9963398575782776, "text_region": [[426.0, 2656.0], [2019.0, 2656.0], [2019.0, 2703.0], [426.0, 2703.0]]}, {"text": " a pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [28]", "confidence": 0.980224609375, "text_region": [[419.0, 2696.0], [2022.0, 2693.0], [2022.0, 2749.0], [419.0, 2752.0]]}, {"text": "*Equal contribution.", "confidence": 0.9994953274726868, "text_region": [[482.0, 2759.0], [775.0, 2759.0], [775.0, 2805.0], [482.0, 2805.0]]}, {"text": "https://github.com/artidoro/qlora and https://github.com/TimDettmers/bitsandbytes", "confidence": 0.9979228973388672, "text_region": [[486.0, 2802.0], [1996.0, 2802.0], [1996.0, 2848.0], [486.0, 2848.0]]}], "img_idx": 0, "score": 0.9427855610847473}
{"type": "text", "bbox": [1068, 647, 1375, 683], "res": [{"text": "Artidoro Pagnoni*", "confidence": 0.9987768530845642, "text_region": [[1064.0, 640.0], [1380.0, 640.0], [1380.0, 686.0], [1064.0, 686.0]]}], "img_idx": 0, "score": 0.9241341352462769}
{"type": "text", "bbox": [1070, 761, 1377, 794], "res": [{"text": "Luke Zettlemoyer", "confidence": 0.997334361076355, "text_region": [[1064.0, 752.0], [1384.0, 752.0], [1384.0, 799.0], [1064.0, 799.0]]}], "img_idx": 0, "score": 0.92210853099823}
{"type": "text", "bbox": [1596, 649, 1831, 679], "res": [{"text": "Ari Holtzman", "confidence": 0.9995291233062744, "text_region": [[1581.0, 630.0], [1840.0, 637.0], [1838.0, 694.0], [1579.0, 686.0]]}], "img_idx": 0, "score": 0.8952162265777588}
{"type": "text", "bbox": [618, 648, 870, 678], "res": [{"text": "Tim Dettmers*", "confidence": 0.9893750548362732, "text_region": [[612.0, 640.0], [871.0, 640.0], [871.0, 686.0], [612.0, 686.0]]}], "img_idx": 0, "score": 0.8552442789077759}
{"type": "text", "bbox": [749, 844, 1701, 926], "res": [{"text": "University of Washington", "confidence": 0.9898267388343811, "text_region": [[1015.0, 835.0], [1431.0, 842.0], [1430.0, 888.0], [1014.0, 881.0]]}, {"text": "{dettmers,artidoro,ahai,lsz}@cs.washington.edu", "confidence": 0.9975875616073608, "text_region": [[738.0, 881.0], [1710.0, 884.0], [1710.0, 931.0], [738.0, 927.0]]}], "img_idx": 0, "score": 0.8459122180938721}
{"type": "text", "bbox": [73, 827, 145, 1249], "res": [{"text": "3", "confidence": 0.9952888488769531, "text_region": [[67.0, 818.0], [136.0, 818.0], [136.0, 871.0], [67.0, 871.0]]}, {"text": "2", "confidence": 0.9976435303688049, "text_region": [[67.0, 858.0], [136.0, 858.0], [136.0, 911.0], [67.0, 911.0]]}, {"text": "20", "confidence": 0.7286654710769653, "text_region": [[60.0, 891.0], [146.0, 891.0], [146.0, 993.0], [60.0, 993.0]]}, {"text": "E", "confidence": 0.5761705636978149, "text_region": [[73.0, 988.0], [156.0, 995.0], [148.0, 1094.0], [65.0, 1087.0]]}, {"text": "M", "confidence": 0.9730509519577026, "text_region": [[67.0, 1066.0], [140.0, 1066.0], [140.0, 1155.0], [67.0, 1155.0]]}, {"text": "3", "confidence": 0.9873350858688354, "text_region": [[67.0, 1165.0], [136.0, 1165.0], [136.0, 1221.0], [67.0, 1221.0]]}, {"text": "2", "confidence": 0.9987710118293762, "text_region": [[67.0, 1201.0], [140.0, 1201.0], [140.0, 1254.0], [67.0, 1254.0]]}], "img_idx": 0, "score": 0.5948007106781006}
{"type": "text", "bbox": [71, 1598, 137, 2221], "res": [{"text": "1", "confidence": 0.9909776449203491, "text_region": [[73.0, 1581.0], [130.0, 1581.0], [130.0, 1620.0], [73.0, 1620.0]]}, {"text": "4", "confidence": 0.839817464351654, "text_region": [[70.0, 1650.0], [136.0, 1650.0], [136.0, 1706.0], [70.0, 1706.0]]}, {"text": "3", "confidence": 0.9965178966522217, "text_region": [[67.0, 1729.0], [136.0, 1729.0], [136.0, 1785.0], [67.0, 1785.0]]}, {"text": "4", "confidence": 0.9991281628608704, "text_region": [[67.0, 1769.0], [133.0, 1769.0], [133.0, 1825.0], [67.0, 1825.0]]}, {"text": "1", "confidence": 0.9947811961174011, "text_region": [[70.0, 1822.0], [130.0, 1822.0], [130.0, 1865.0], [70.0, 1865.0]]}, {"text": "5", "confidence": 0.9658668637275696, "text_region": [[70.0, 1878.0], [133.0, 1878.0], [133.0, 1921.0], [70.0, 1921.0]]}, {"text": "0", "confidence": 0.9032704830169678, "text_region": [[70.0, 1911.0], [136.0, 1911.0], [136.0, 1967.0], [70.0, 1967.0]]}, {"text": "3", "confidence": 0.9970093369483948, "text_region": [[70.0, 1957.0], [133.0, 1957.0], [133.0, 2000.0], [70.0, 2000.0]]}, {"text": "2.", "confidence": 0.7659705877304077, "text_region": [[67.0, 1993.0], [140.0, 1993.0], [140.0, 2046.0], [67.0, 2046.0]]}, {"text": "AIX", "confidence": 0.8868193030357361, "text_region": [[60.0, 2023.0], [146.0, 2023.0], [146.0, 2191.0], [60.0, 2191.0]]}], "img_idx": 0, "score": 0.5699684619903564}
{"type": "title", "bbox": [432, 2243, 763, 2278], "res": [{"text": "Introduction", "confidence": 0.9995548725128174, "text_region": [[496.0, 2241.0], [768.0, 2241.0], [768.0, 2287.0], [496.0, 2287.0]]}], "img_idx": 0, "score": 0.9589457511901855}
{"type": "title", "bbox": [481, 403, 1966, 467], "res": [{"text": "QLoRA: Efficient Finetuning of Quantized LLMs", "confidence": 0.9680353999137878, "text_region": [[489.0, 403.0], [1966.0, 403.0], [1966.0, 459.0], [489.0, 459.0]]}], "img_idx": 0, "score": 0.9370641708374023}
{"type": "title", "bbox": [1132, 1053, 1315, 1086], "res": [{"text": "Abstract", "confidence": 0.9995748996734619, "text_region": [[1131.0, 1046.0], [1317.0, 1046.0], [1317.0, 1092.0], [1131.0, 1092.0]]}], "img_idx": 0, "score": 0.7891521453857422}
{"type": "footer", "bbox": [430, 2934, 763, 2966], "res": [{"text": "Preprint. Under review.", "confidence": 0.9997679591178894, "text_region": [[429.0, 2930.0], [768.0, 2930.0], [768.0, 2973.0], [429.0, 2973.0]]}], "img_idx": 0, "score": 0.7580754160881042}
