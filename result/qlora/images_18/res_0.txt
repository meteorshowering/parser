{"type": "footer", "bbox": [1206, 2973, 1244, 3001], "res": [{"text": "19", "confidence": 0.9999141097068787, "text_region": [[1201.0, 2970.0], [1251.0, 2970.0], [1251.0, 3013.0], [1201.0, 3013.0]]}], "img_idx": 0, "score": 0.9053428173065186}
{"type": "reference", "bbox": [434, 299, 2022, 2892], "res": [{"text": "[37] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. Few-shot", "confidence": 0.9955865740776062, "text_region": [[429.0, 297.0], [2019.0, 297.0], [2019.0, 343.0], [429.0, 343.0]]}, {"text": "parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in", "confidence": 0.9880439043045044, "text_region": [[516.0, 343.0], [2019.0, 343.0], [2019.0, 389.0], [516.0, 389.0]]}, {"text": "Neural Information Processing Systems, 35:1950-1965, 2022.", "confidence": 0.9989069700241089, "text_region": [[512.0, 386.0], [1507.0, 383.0], [1507.0, 429.0], [512.0, 432.0]]}, {"text": "[38] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer,", "confidence": 0.9895128607749939, "text_region": [[429.0, 459.0], [2019.0, 465.0], [2019.0, 512.0], [429.0, 505.0]]}, {"text": " and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint", "confidence": 0.9804468750953674, "text_region": [[509.0, 502.0], [2022.0, 508.0], [2022.0, 561.0], [509.0, 554.0]]}, {"text": "arXiv:1907.11692, 2019.", "confidence": 0.999796986579895, "text_region": [[516.0, 548.0], [918.0, 551.0], [918.0, 597.0], [515.0, 594.0]]}, {"text": "[39] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei,", "confidence": 0.9886572360992432, "text_region": [[429.0, 630.0], [2019.0, 630.0], [2019.0, 673.0], [429.0, 673.0]]}, {"text": " et al. The fan collection: Designing data and methods for effective instruction tuning. arXiv", "confidence": 0.9897698760032654, "text_region": [[509.0, 670.0], [2019.0, 673.0], [2019.0, 719.0], [509.0, 716.0]]}, {"text": "preprint arXiv:2301.13688, 2023.", "confidence": 0.9995672106742859, "text_region": [[509.0, 720.0], [1057.0, 709.0], [1058.0, 756.0], [509.0, 766.0]]}, {"text": "[40] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. Metaicl: Learning to learn in context.", "confidence": 0.9936043620109558, "text_region": [[426.0, 792.0], [2019.0, 795.0], [2019.0, 842.0], [426.0, 838.0]]}, {"text": "arXiv preprint arXiv:2110.15943, 2021.", "confidence": 0.995875895023346, "text_region": [[516.0, 842.0], [1154.0, 842.0], [1154.0, 884.0], [516.0, 884.0]]}, {"text": "[41] A. Nematzadeh, K. Burns, E. Grant, A. Gopnik, and T. Griffiths. Evaluating theory of mind in", "confidence": 0.9884184002876282, "text_region": [[429.0, 917.0], [2016.0, 917.0], [2016.0, 960.0], [429.0, 960.0]]}, {"text": "question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural", "confidence": 0.9974139928817749, "text_region": [[516.0, 957.0], [2016.0, 957.0], [2016.0, 1003.0], [516.0, 1003.0]]}, {"text": "Language Processing, pages 2392-2400, 2018.", "confidence": 0.9922381639480591, "text_region": [[519.0, 1006.0], [1271.0, 1006.0], [1271.0, 1053.0], [519.0, 1053.0]]}, {"text": "[42]  OpenAI. Gpt-4 technical report. arXiv, 2023.", "confidence": 0.9618787169456482, "text_region": [[432.0, 1082.0], [1247.0, 1082.0], [1247.0, 1129.0], [432.0, 1129.0]]}, {"text": "[43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,", "confidence": 0.9889617562294006, "text_region": [[432.0, 1162.0], [2019.0, 1162.0], [2019.0, 1208.0], [432.0, 1208.0]]}, {"text": "K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.", "confidence": 0.9925637245178223, "text_region": [[512.0, 1201.0], [2019.0, 1205.0], [2019.0, 1251.0], [512.0, 1247.0]]}, {"text": "Advances in Neural Information Processing Systems, 35:27730-27744, 2022.", "confidence": 0.9982749223709106, "text_region": [[509.0, 1247.0], [1753.0, 1244.0], [1753.0, 1290.0], [509.0, 1294.0]]}, {"text": "[44] G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee, and D. Lee. nuqmm: Quantized matmul for", "confidence": 0.9881728887557983, "text_region": [[432.0, 1327.0], [2016.0, 1327.0], [2016.0, 1373.0], [432.0, 1373.0]]}, {"text": " efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557,", "confidence": 0.9898878931999207, "text_region": [[509.0, 1363.0], [2022.0, 1366.0], [2022.0, 1422.0], [509.0, 1419.0]]}, {"text": "2022.", "confidence": 0.9999343156814575, "text_region": [[516.0, 1409.0], [612.0, 1409.0], [612.0, 1459.0], [516.0, 1459.0]]}, {"text": "[45] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint", "confidence": 0.9904198050498962, "text_region": [[426.0, 1488.0], [2019.0, 1492.0], [2019.0, 1538.0], [426.0, 1534.0]]}, {"text": "arXiv:2304.03277, 2023.", "confidence": 0.9984822273254395, "text_region": [[516.0, 1534.0], [918.0, 1534.0], [918.0, 1577.0], [516.0, 1577.0]]}, {"text": "[46] A. Poliak, J. Naradowsky, A. Haldar, R. Rudinger, and B. Van Durme. Hypothesis only baselines", "confidence": 0.9934051632881165, "text_region": [[429.0, 1614.0], [2016.0, 1614.0], [2016.0, 1660.0], [429.0, 1660.0]]}, {"text": "in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and", "confidence": 0.9905325174331665, "text_region": [[519.0, 1660.0], [2019.0, 1660.0], [2019.0, 1706.0], [519.0, 1706.0]]}, {"text": "Computational Semantics, pages 180-191, 2018.", "confidence": 0.9928551316261292, "text_region": [[516.0, 1699.0], [1300.0, 1699.0], [1300.0, 1756.0], [516.0, 1756.0]]}, {"text": "[47] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao,", "confidence": 0.9955031871795654, "text_region": [[429.0, 1779.0], [2019.0, 1779.0], [2019.0, 1825.0], [429.0, 1825.0]]}, {"text": " S. Agrawal, and J. Dean.  Efficiently scaling transformer inference.  arXiv preprint", "confidence": 0.961779773235321, "text_region": [[506.0, 1815.0], [2022.0, 1818.0], [2022.0, 1874.0], [506.0, 1871.0]]}, {"text": "arXiv:2211.05102, 2022.", "confidence": 0.9810909628868103, "text_region": [[516.0, 1868.0], [918.0, 1868.0], [918.0, 1911.0], [516.0, 1911.0]]}, {"text": "[48] G. Qin and J. Eisner. Learning how to ask: Querying Ims with mixtures of soft prompts. arXiv", "confidence": 0.9794200658798218, "text_region": [[426.0, 1940.0], [2019.0, 1944.0], [2019.0, 1990.0], [426.0, 1987.0]]}, {"text": "preprint arXiv:2104.06599, 2021.", "confidence": 0.9908996820449829, "text_region": [[508.0, 1993.0], [1060.0, 1980.0], [1061.0, 2026.0], [510.0, 2040.0]]}, {"text": "[49] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu.", "confidence": 0.9851690530776978, "text_region": [[429.0, 2066.0], [2019.0, 2066.0], [2019.0, 2112.0], [429.0, 2112.0]]}, {"text": "Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn.", "confidence": 0.9960523247718811, "text_region": [[512.0, 2109.0], [2019.0, 2109.0], [2019.0, 2152.0], [512.0, 2152.0]]}, {"text": "Res., 21(1), jan 2020. ISSN 1532-4435.", "confidence": 0.9822030067443848, "text_region": [[516.0, 2155.0], [1151.0, 2155.0], [1151.0, 2198.0], [516.0, 2198.0]]}, {"text": "[50] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler,", "confidence": 0.9902239441871643, "text_region": [[429.0, 2231.0], [2019.0, 2231.0], [2019.0, 2277.0], [429.0, 2277.0]]}, {"text": "T. L. Scao, A. Raja, et al. Multitask prompted training enables zero-shot task generalization.", "confidence": 0.9900121688842773, "text_region": [[516.0, 2277.0], [2019.0, 2277.0], [2019.0, 2323.0], [516.0, 2323.0]]}, {"text": "arXiv preprint arXiv:2110.08207, 2021.", "confidence": 0.99969881772995, "text_region": [[515.0, 2320.0], [1154.0, 2317.0], [1154.0, 2363.0], [516.0, 2366.0]]}, {"text": "[51] M. Sap, R. LeBras, D. Fried, and Y. Choi. Neural theory-of-mind? on the limits of social", "confidence": 0.9820971488952637, "text_region": [[429.0, 2399.0], [2019.0, 2399.0], [2019.0, 2442.0], [429.0, 2442.0]]}, {"text": "intelligence in large Ims. arXiv preprint arXiv:2210.13312, 2022.", "confidence": 0.9873703122138977, "text_region": [[512.0, 2439.0], [1563.0, 2435.0], [1563.0, 2491.0], [512.0, 2495.0]]}, {"text": "[52] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Mic, D. Hesslow, R. Castagn\u00e9, A. S. Luccioni,", "confidence": 0.9825578331947327, "text_region": [[432.0, 2518.0], [2019.0, 2518.0], [2019.0, 2564.0], [432.0, 2564.0]]}, {"text": "F. Yvon, M. Gall\u00e9, et al. Bloom: A 176b-parameter open-access multilingual language model.", "confidence": 0.9941316246986389, "text_region": [[509.0, 2564.0], [2019.0, 2564.0], [2019.0, 2610.0], [509.0, 2610.0]]}, {"text": "arXiv preprint arXiv:2211.05100, 2022.", "confidence": 0.9992271065711975, "text_region": [[516.0, 2610.0], [1157.0, 2610.0], [1157.0, 2653.0], [516.0, 2653.0]]}, {"text": "[53] S. Shaphiro and M. Wilk. An analysis of variance test for normality. Biometrika, 52(3):591-611,", "confidence": 0.9852133989334106, "text_region": [[429.0, 2680.0], [2022.0, 2683.0], [2022.0, 2729.0], [429.0, 2726.0]]}, {"text": "1965.", "confidence": 0.9999450445175171, "text_region": [[512.0, 2726.0], [609.0, 2726.0], [609.0, 2775.0], [512.0, 2775.0]]}, {"text": "[54] Y.-L. Sung, V. Nair, and C. A. Raffel. Training neural networks with fixed sparse masks.", "confidence": 0.9838137030601501, "text_region": [[429.0, 2805.0], [2019.0, 2805.0], [2019.0, 2851.0], [429.0, 2851.0]]}, {"text": "Advances in Neural Information Processing Systems, 34:24193-24205, 2021.", "confidence": 0.9959127902984619, "text_region": [[516.0, 2854.0], [1753.0, 2854.0], [1753.0, 2897.0], [516.0, 2897.0]]}], "img_idx": 0, "score": 0.9819447994232178}
