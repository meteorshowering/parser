{"type": "text", "bbox": [425, 1892, 2020, 2522], "res": [{"text": "The problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input", "confidence": 0.9827567338943481, "text_region": [[426.0, 1884.0], [2022.0, 1884.0], [2022.0, 1940.0], [426.0, 1940.0]]}, {"text": "tensor, then the quantization bins\u2014certain bit combinations-", "confidence": 0.9834523797035217, "text_region": [[426.0, 1934.0], [1417.0, 1934.0], [1417.0, 1980.0], [426.0, 1980.0]]}, {"text": "-are not utilized well with few or no", "confidence": 0.987256646156311, "text_region": [[1427.0, 1934.0], [2022.0, 1934.0], [2022.0, 1980.0], [1427.0, 1980.0]]}, {"text": "numbers quantized in some bins. To prevent the outlier issue, a common approach is to chunk the", "confidence": 0.990726113319397, "text_region": [[422.0, 1973.0], [2026.0, 1970.0], [2026.0, 2026.0], [422.0, 2030.0]]}, {"text": "input tensor into blocks that are independently quantized, each with their own quantization constant c.", "confidence": 0.9854778051376343, "text_region": [[429.0, 2020.0], [2022.0, 2020.0], [2022.0, 2066.0], [429.0, 2066.0]]}, {"text": "This can be formalized as follows: We chunk the input tensor X E Rbxh into n contiguous blocks of", "confidence": 0.9897595643997192, "text_region": [[426.0, 2056.0], [2026.0, 2056.0], [2026.0, 2112.0], [426.0, 2112.0]]}, {"text": "size B by flattening the input tensor and slicing the linear segment into n = (b x h)/ B blocks. We", "confidence": 0.9777958989143372, "text_region": [[422.0, 2102.0], [2026.0, 2099.0], [2026.0, 2155.0], [422.0, 2158.0]]}, {"text": "quantize these blocks independently with Equation 1 to create a quantized tensor and n quantization", "confidence": 0.9912153482437134, "text_region": [[429.0, 2152.0], [2022.0, 2152.0], [2022.0, 2198.0], [429.0, 2198.0]]}, {"text": "constants Ci.", "confidence": 0.9977289438247681, "text_region": [[426.0, 2198.0], [635.0, 2198.0], [635.0, 2244.0], [426.0, 2244.0]]}, {"text": "Low-rank Adapters  Low-rank Adapter (LoRA) finetuning [28] is a method that reduces memory", "confidence": 0.9819128513336182, "text_region": [[422.0, 2257.0], [2019.0, 2261.0], [2019.0, 2317.0], [422.0, 2313.0]]}, {"text": "requirements by using a small set of trainable parameters, often termed adapters, while not updating", "confidence": 0.9963166117668152, "text_region": [[429.0, 2310.0], [2019.0, 2310.0], [2019.0, 2356.0], [429.0, 2356.0]]}, {"text": "the full model parameters which remain fixed. Gradients during stochastic gradient descent are", "confidence": 0.987025260925293, "text_region": [[426.0, 2353.0], [2022.0, 2353.0], [2022.0, 2399.0], [426.0, 2399.0]]}, {"text": "passed through the fixed pretrained model weights to the adapter, which is updated to optimize the", "confidence": 0.9899086952209473, "text_region": [[429.0, 2396.0], [2022.0, 2396.0], [2022.0, 2442.0], [429.0, 2442.0]]}, {"text": "loss function. LoRA augments a linear projection through an additional factorized projection. Given", "confidence": 0.9924548864364624, "text_region": [[429.0, 2442.0], [2022.0, 2442.0], [2022.0, 2488.0], [429.0, 2488.0]]}, {"text": "a projection XW = Y with X E Rbxh, W E Rhxo LoRA computes:", "confidence": 0.9635752439498901, "text_region": [[419.0, 2472.0], [1567.0, 2472.0], [1567.0, 2538.0], [419.0, 2538.0]]}], "img_idx": 0, "score": 0.9884547591209412}
{"type": "text", "bbox": [428, 1230, 2020, 1532], "res": [{"text": "Block-wise k-bit Quantization  Quantization is the process of discretizing an input from a rep-", "confidence": 0.9918095469474792, "text_region": [[429.0, 1228.0], [2019.0, 1228.0], [2019.0, 1274.0], [429.0, 1274.0]]}, {"text": "resentation that holds more information to a representation with less information. It often means", "confidence": 0.9833812117576599, "text_region": [[426.0, 1270.0], [2019.0, 1270.0], [2019.0, 1317.0], [426.0, 1317.0]]}, {"text": "taking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to", "confidence": 0.9845731258392334, "text_region": [[429.0, 1313.0], [2022.0, 1313.0], [2022.0, 1360.0], [429.0, 1360.0]]}, {"text": " 8-bit Integers. To ensure that the entire range of the low-bit data type is used, the input data type is", "confidence": 0.9916521310806274, "text_region": [[419.0, 1350.0], [2022.0, 1353.0], [2022.0, 1409.0], [419.0, 1406.0]]}, {"text": " commonly rescaled into the target data type range through normalization by the absolute maximum", "confidence": 0.9908722043037415, "text_region": [[422.0, 1402.0], [2016.0, 1402.0], [2016.0, 1449.0], [422.0, 1449.0]]}, {"text": "of the input elements, which are usually structured as a tensor. For example, quantizing a 32-bit", "confidence": 0.9960032105445862, "text_region": [[426.0, 1445.0], [2022.0, 1445.0], [2022.0, 1492.0], [426.0, 1492.0]]}, {"text": "Floating Point (FP32) tensor into a Int8 tensor with range [\u2014127, 127]:", "confidence": 0.9870423674583435, "text_region": [[426.0, 1488.0], [1563.0, 1488.0], [1563.0, 1534.0], [426.0, 1534.0]]}], "img_idx": 0, "score": 0.9862490892410278}
{"type": "text", "bbox": [430, 2720, 2017, 2889], "res": [{"text": "Memory Requirement of Parameter-Efficient Finetuning ", "confidence": 0.9898084998130798, "text_region": [[429.0, 2716.0], [1447.0, 2716.0], [1447.0, 2762.0], [429.0, 2762.0]]}, {"text": " One important point of discussion is", "confidence": 0.9888362288475037, "text_region": [[1427.0, 2716.0], [2019.0, 2716.0], [2019.0, 2762.0], [1427.0, 2762.0]]}, {"text": "the memory requirement of LoRA during training both in terms of the number and size of adapters", "confidence": 0.980976939201355, "text_region": [[426.0, 2756.0], [2022.0, 2756.0], [2022.0, 2812.0], [426.0, 2812.0]]}, {"text": "used. Since the memory footprint of LoRA is so minimal, we can use more adapters to improve", "confidence": 0.9872337579727173, "text_region": [[429.0, 2805.0], [2019.0, 2805.0], [2019.0, 2851.0], [429.0, 2851.0]]}, {"text": "performance without significantly increasing the total memory used. While LoRA was designed as a", "confidence": 0.9933463931083679, "text_region": [[429.0, 2848.0], [2022.0, 2848.0], [2022.0, 2894.0], [429.0, 2894.0]]}], "img_idx": 0, "score": 0.9726142287254333}
{"type": "text", "bbox": [1970, 2559, 2019, 2589], "res": [{"text": "(3)", "confidence": 0.9981193542480469, "text_region": [[1966.0, 2554.0], [2026.0, 2554.0], [2026.0, 2597.0], [1966.0, 2597.0]]}], "img_idx": 0, "score": 0.8957717418670654}
{"type": "text", "bbox": [1972, 1809, 2018, 1843], "res": [{"text": "(2)", "confidence": 0.9991209506988525, "text_region": [[1966.0, 1805.0], [2026.0, 1805.0], [2026.0, 1848.0], [1966.0, 1848.0]]}], "img_idx": 0, "score": 0.89309161901474}
{"type": "text", "bbox": [1970, 1591, 2019, 1625], "res": [{"text": "(1)", "confidence": 0.9973604679107666, "text_region": [[1966.0, 1587.0], [2026.0, 1587.0], [2026.0, 1630.0], [1966.0, 1630.0]]}], "img_idx": 0, "score": 0.8834701776504517}
{"type": "text", "bbox": [440, 2648, 1260, 2686], "res": [{"text": "where L1 E Rhxr and L2 E Rrx\u00b0, and s is a scalar.", "confidence": 0.9563455581665039, "text_region": [[426.0, 2643.0], [1261.0, 2643.0], [1261.0, 2690.0], [426.0, 2690.0]]}], "img_idx": 0, "score": 0.8475341796875}
{"type": "title", "bbox": [430, 1156, 754, 1197], "res": [{"text": "2Background", "confidence": 0.9998287558555603, "text_region": [[426.0, 1152.0], [758.0, 1152.0], [758.0, 1198.0], [426.0, 1198.0]]}], "img_idx": 0, "score": 0.9486337304115295}
{"type": "figure", "bbox": [404, 299, 2061, 956], "res": [{"text": " Full Finetuning", "confidence": 0.9635356068611145, "text_region": [[683.0, 303.0], [909.0, 311.0], [907.0, 357.0], [681.0, 349.0]]}, {"text": "LoRA", "confidence": 0.9905428290367126, "text_region": [[1207.0, 314.0], [1291.0, 314.0], [1291.0, 350.0], [1207.0, 350.0]]}, {"text": "QLoRA", "confidence": 0.9427260160446167, "text_region": [[1626.0, 314.0], [1736.0, 314.0], [1736.0, 350.0], [1626.0, 350.0]]}, {"text": "(No Adapters)", "confidence": 0.9997063279151917, "text_region": [[688.0, 350.0], [901.0, 350.0], [901.0, 396.0], [688.0, 396.0]]}, {"text": "Optimizer", "confidence": 0.9998860955238342, "text_region": [[452.0, 429.0], [609.0, 429.0], [609.0, 475.0], [452.0, 475.0]]}, {"text": "State", "confidence": 0.9996002912521362, "text_region": [[456.0, 472.0], [539.0, 472.0], [539.0, 511.0], [456.0, 511.0]]}, {"text": "(32 bit)", "confidence": 0.9502437114715576, "text_region": [[452.0, 508.0], [575.0, 508.0], [575.0, 554.0], [452.0, 554.0]]}, {"text": "11.1", "confidence": 0.5873837471008301, "text_region": [[1866.0, 535.0], [1942.0, 535.0], [1942.0, 561.0], [1866.0, 561.0]]}, {"text": "1111", "confidence": 0.5050636529922485, "text_region": [[1859.0, 587.0], [1962.0, 587.0], [1962.0, 610.0], [1859.0, 610.0]]}, {"text": "Adapters", "confidence": 0.9996851682662964, "text_region": [[447.0, 616.0], [596.0, 624.0], [594.0, 671.0], [445.0, 663.0]]}, {"text": "(16 bit)", "confidence": 0.9777532815933228, "text_region": [[446.0, 660.0], [569.0, 660.0], [569.0, 706.0], [446.0, 706.0]]}, {"text": "Base", "confidence": 0.9998587369918823, "text_region": [[456.0, 795.0], [539.0, 795.0], [539.0, 835.0], [456.0, 835.0]]}, {"text": "Parameter Updates", "confidence": 0.9924187660217285, "text_region": [[1756.0, 822.0], [1922.0, 822.0], [1922.0, 855.0], [1756.0, 855.0]]}, {"text": "Model", "confidence": 0.9990625381469727, "text_region": [[452.0, 832.0], [559.0, 832.0], [559.0, 878.0], [452.0, 878.0]]}, {"text": "Gradient Flow", "confidence": 0.9961865544319153, "text_region": [[1796.0, 861.0], [1922.0, 861.0], [1922.0, 894.0], [1796.0, 894.0]]}, {"text": "16-bit Transformer", "confidence": 0.9938730597496033, "text_region": [[690.0, 894.0], [902.0, 901.0], [900.0, 948.0], [688.0, 940.0]]}, {"text": "4-bit Transformer", "confidence": 0.9666511416435242, "text_region": [[1534.0, 891.0], [1737.0, 898.0], [1735.0, 944.0], [1533.0, 937.0]]}, {"text": "16-bit Transformer", "confidence": 0.9570931196212769, "text_region": [[1114.0, 904.0], [1314.0, 904.0], [1314.0, 940.0], [1114.0, 940.0]]}, {"text": "Paging Flow", "confidence": 0.9996212124824524, "text_region": [[1809.0, 901.0], [1929.0, 901.0], [1929.0, 937.0], [1809.0, 937.0]]}], "img_idx": 0, "score": 0.9465895295143127}
{"type": "figure_caption", "bbox": [743, 994, 1639, 1066], "res": [{"text": "Figure 1: Different finetuning methods and their memory requirements. QLoRA improves over LoRA by", "confidence": 0.9947190284729004, "text_region": [[426.0, 987.0], [2022.0, 987.0], [2022.0, 1033.0], [426.0, 1033.0]]}, {"text": "quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes.", "confidence": 0.9988312721252441, "text_region": [[426.0, 1030.0], [1946.0, 1030.0], [1946.0, 1076.0], [426.0, 1076.0]]}], "img_idx": 0, "score": 0.5122283697128296}
{"type": "footer", "bbox": [1213, 2973, 1240, 2999], "res": [{"text": "3", "confidence": 0.9998162388801575, "text_region": [[1211.0, 2970.0], [1241.0, 2970.0], [1241.0, 3003.0], [1211.0, 3003.0]]}], "img_idx": 0, "score": 0.6157962083816528}
{"type": "equation", "bbox": [707, 1563, 1740, 1656], "res": [{"text": "127", "confidence": 0.9999297261238098, "text_region": [[1064.0, 1561.0], [1131.0, 1561.0], [1131.0, 1600.0], [1064.0, 1600.0]]}, {"text": "XInt8 = round (", "confidence": 0.8903921246528625, "text_region": [[699.0, 1574.0], [955.0, 1581.0], [954.0, 1634.0], [698.0, 1626.0]]}, {"text": "round(CFP32 . xFP32),", "confidence": 0.9240795969963074, "text_region": [[1394.0, 1577.0], [1743.0, 1577.0], [1743.0, 1634.0], [1394.0, 1634.0]]}, {"text": "(absmax(XFP32)", "confidence": 0.9745622873306274, "text_region": [[944.0, 1614.0], [1219.0, 1603.0], [1221.0, 1656.0], [946.0, 1667.0]]}], "img_idx": 0, "score": 0.910419762134552}
{"type": "equation", "bbox": [1037, 2555, 1411, 2590], "res": [{"text": "Y = XW + sXLL2", "confidence": 0.9705302119255066, "text_region": [[1032.0, 2547.0], [1407.0, 2554.0], [1406.0, 2601.0], [1031.0, 2594.0]]}], "img_idx": 0, "score": 0.9058326482772827}
{"type": "equation", "bbox": [883, 1708, 1834, 1859], "res": [{"text": "where c is the quantization constant or quantization scale. Dequantization is the inverse:", "confidence": 0.9824367165565491, "text_region": [[429.0, 1706.0], [1846.0, 1706.0], [1846.0, 1749.0], [429.0, 1749.0]]}, {"text": "XInt8", "confidence": 0.9811476469039917, "text_region": [[1291.0, 1771.0], [1385.0, 1757.0], [1393.0, 1807.0], [1299.0, 1821.0]]}, {"text": "=XFP32", "confidence": 0.9144997596740723, "text_region": [[1408.0, 1804.0], [1538.0, 1783.0], [1546.0, 1830.0], [1415.0, 1850.0]]}, {"text": "CFP32", "confidence": 0.9884848594665527, "text_region": [[1295.0, 1826.0], [1385.0, 1813.0], [1392.0, 1862.0], [1302.0, 1875.0]]}], "img_idx": 0, "score": 0.7629454135894775}
