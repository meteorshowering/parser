{"type": "text", "bbox": [427, 762, 2018, 1875], "res": [{"text": " Our QLoRA finetuning method is the first method that enables the finetuning of 33B parameter", "confidence": 0.9819233417510986, "text_region": [[422.0, 756.0], [2022.0, 759.0], [2022.0, 815.0], [422.0, 812.0]]}, {"text": "models on a single consumer GPU and 65B parameter models on a single professional GPU, while", "confidence": 0.9908474087715149, "text_region": [[426.0, 809.0], [2016.0, 809.0], [2016.0, 855.0], [426.0, 855.0]]}, {"text": "not degrading performance relative to a full finetuning baseline. We have demonstrated that our", "confidence": 0.9864826202392578, "text_region": [[426.0, 851.0], [2019.0, 851.0], [2019.0, 898.0], [426.0, 898.0]]}, {"text": "best 33B model trained on the Open Assistant dataset can rival ChatGPT on the Vicuna benchmark.", "confidence": 0.9946572780609131, "text_region": [[426.0, 891.0], [2019.0, 894.0], [2019.0, 941.0], [426.0, 937.0]]}, {"text": " Since instruction finetuning is an essential tool to transform raw pretrained LLMs into ChatGPT-like", "confidence": 0.9859176278114319, "text_region": [[422.0, 931.0], [2022.0, 934.0], [2022.0, 990.0], [422.0, 987.0]]}, {"text": "chatbots, we believe that our method will make finetuning widespread and common in particular for", "confidence": 0.990140438079834, "text_region": [[429.0, 983.0], [2019.0, 983.0], [2019.0, 1030.0], [429.0, 1030.0]]}, {"text": "the researchers that have the least resources, a big win for the accessibility of state of the art NLP", "confidence": 0.9874514937400818, "text_region": [[426.0, 1026.0], [2019.0, 1026.0], [2019.0, 1072.0], [426.0, 1072.0]]}, {"text": "technology. QLoRA can be seen as an equalizing factor that helps to close the resource gap between", "confidence": 0.9938374757766724, "text_region": [[429.0, 1072.0], [2019.0, 1072.0], [2019.0, 1115.0], [429.0, 1115.0]]}, {"text": "large corporations and small teams with consumer GPUs.", "confidence": 0.9804885387420654, "text_region": [[422.0, 1112.0], [1344.0, 1112.0], [1344.0, 1158.0], [422.0, 1158.0]]}, {"text": "Another potential source of impact is deployment to mobile phones. We believe our QLoRA method", "confidence": 0.9972984790802002, "text_region": [[429.0, 1178.0], [2019.0, 1178.0], [2019.0, 1224.0], [429.0, 1224.0]]}, {"text": "might enable the critical milestone of enabling the finetuning of LLMs on phones and other low", "confidence": 0.9885042905807495, "text_region": [[429.0, 1224.0], [2019.0, 1224.0], [2019.0, 1267.0], [429.0, 1267.0]]}, {"text": "resource settings. While 7B models were shown to be able to be run on phones before, QLoRA is", "confidence": 0.9955838322639465, "text_region": [[429.0, 1267.0], [2019.0, 1267.0], [2019.0, 1310.0], [429.0, 1310.0]]}, {"text": "the first method that would enable the finetuning of such models. We estimate that with an iPhone 12", "confidence": 0.9880790710449219, "text_region": [[429.0, 1310.0], [2019.0, 1310.0], [2019.0, 1356.0], [429.0, 1356.0]]}, {"text": "Plus, QLoRA can finetune 3 million tokens per night while the phone is charging. While finetuned", "confidence": 0.9812373518943787, "text_region": [[426.0, 1353.0], [2016.0, 1353.0], [2016.0, 1399.0], [426.0, 1399.0]]}, {"text": "7B models do not reach the quality of ChatGPT, we believe that the quality is good enough to enable", "confidence": 0.9953000545501709, "text_region": [[432.0, 1399.0], [2019.0, 1399.0], [2019.0, 1442.0], [432.0, 1442.0]]}, {"text": "novel applications that have not been possible before due to privacy or LLM quality issues. QLoRA", "confidence": 0.9893774390220642, "text_region": [[429.0, 1442.0], [2019.0, 1442.0], [2019.0, 1488.0], [429.0, 1488.0]]}, {"text": "can help enable privacy-preserving usage of LLMs, where users can own and manage their own data", "confidence": 0.9991329908370972, "text_region": [[429.0, 1485.0], [2019.0, 1485.0], [2019.0, 1531.0], [429.0, 1531.0]]}, {"text": "and models, while simultaneously making LLMs easier to deploy.", "confidence": 0.9954286813735962, "text_region": [[429.0, 1528.0], [1483.0, 1528.0], [1483.0, 1574.0], [429.0, 1574.0]]}, {"text": "However, finetuning is a dual-use technology that can be abused to cause harm. Widespread use of", "confidence": 0.9948420524597168, "text_region": [[429.0, 1594.0], [2019.0, 1594.0], [2019.0, 1640.0], [429.0, 1640.0]]}, {"text": "LLMs has known dangers [8, 6], but we believe that equalizing access to a technology that is quickly", "confidence": 0.9912687540054321, "text_region": [[423.0, 1630.0], [2022.0, 1637.0], [2022.0, 1690.0], [422.0, 1683.0]]}, {"text": "becoming ubiquitous will allow for better more independent analysis than keeping the power of LLMs ", "confidence": 0.9878379702568054, "text_region": [[429.0, 1680.0], [2022.0, 1680.0], [2022.0, 1726.0], [429.0, 1726.0]]}, {"text": "in the hands of large corporations that do not release models or source code for auditing.", "confidence": 0.9945945739746094, "text_region": [[426.0, 1726.0], [1839.0, 1726.0], [1839.0, 1772.0], [426.0, 1772.0]]}, {"text": " All in all, we believe that QLoRA will have a broadly positive impact making the finetuning of high", "confidence": 0.9815681576728821, "text_region": [[422.0, 1782.0], [2022.0, 1785.0], [2022.0, 1841.0], [422.0, 1838.0]]}, {"text": "quality LLMs much more widely and easily accessible.", "confidence": 0.986812949180603, "text_region": [[432.0, 1835.0], [1314.0, 1835.0], [1314.0, 1881.0], [432.0, 1881.0]]}], "img_idx": 0, "score": 0.9912787079811096}
{"type": "text", "bbox": [423, 300, 2018, 641], "res": [{"text": "An additional limitation is that we did not evaluate different bit-precisions, such as using 3-bit base", "confidence": 0.9852355122566223, "text_region": [[429.0, 297.0], [2019.0, 297.0], [2019.0, 343.0], [429.0, 343.0]]}, {"text": "models, or different adapter methods. Besides LoRA, there is also a wide variety Parameter Efficient", "confidence": 0.9947939515113831, "text_region": [[429.0, 340.0], [2019.0, 340.0], [2019.0, 386.0], [429.0, 386.0]]}, {"text": "FineTuning (PEFT) methods that have been shown to work well. However, it is unclear if these", "confidence": 0.9969907999038696, "text_region": [[426.0, 383.0], [2016.0, 383.0], [2016.0, 429.0], [426.0, 429.0]]}, {"text": "methods scale to large models. We used LoRA as many results established its robustness but other", "confidence": 0.9927477836608887, "text_region": [[426.0, 429.0], [2019.0, 429.0], [2019.0, 475.0], [426.0, 475.0]]}, {"text": "adapters might yield better performance. Since finetuning after quantization seems to recover most of", "confidence": 0.998688280582428, "text_region": [[426.0, 469.0], [2019.0, 469.0], [2019.0, 515.0], [426.0, 515.0]]}, {"text": " the information that is lost during quantization this might enable much more aggressive quantization.", "confidence": 0.9829281568527222, "text_region": [[422.0, 511.0], [2019.0, 515.0], [2019.0, 561.0], [422.0, 558.0]]}, {"text": "For example, 3-bit GPTQ quantization of the basemodel with LoRA might also yield 16-bit full", "confidence": 0.9860265851020813, "text_region": [[429.0, 558.0], [2019.0, 558.0], [2019.0, 604.0], [429.0, 604.0]]}, {"text": "finetuning performance after finetuning.", "confidence": 0.9985309839248657, "text_region": [[426.0, 604.0], [1071.0, 604.0], [1071.0, 650.0], [426.0, 650.0]]}], "img_idx": 0, "score": 0.9889683127403259}
{"type": "text", "bbox": [426, 1996, 2016, 2293], "res": [{"text": "We thank Aditya Kusupati, Ofr Press, Ashish Sharma, Margaret Li, Raphael Olivier, Zihao Ye, and", "confidence": 0.9866316914558411, "text_region": [[422.0, 1987.0], [2022.0, 1990.0], [2022.0, 2046.0], [422.0, 2043.0]]}, {"text": "Evangelia Spiliopoulou for their valuable feedback. Our research was facilitated by the advanced", "confidence": 0.9896466732025146, "text_region": [[426.0, 2039.0], [2019.0, 2039.0], [2019.0, 2086.0], [426.0, 2086.0]]}, {"text": "computational, storage, and networking infrastructure of the Hyak supercomputer system at the", "confidence": 0.9965747594833374, "text_region": [[429.0, 2086.0], [2019.0, 2086.0], [2019.0, 2128.0], [429.0, 2128.0]]}, {"text": " University of Washington. We thank the Hyak team for ensuring a smooth operation. We thank", "confidence": 0.9861252903938293, "text_region": [[422.0, 2119.0], [2022.0, 2122.0], [2022.0, 2178.0], [422.0, 2175.0]]}, {"text": "the beta testers of the bitsandbytes library, in particular Alex Birch and Alyssa Vance. We thank", "confidence": 0.9924486875534058, "text_region": [[426.0, 2171.0], [2019.0, 2171.0], [2019.0, 2218.0], [426.0, 2218.0]]}, {"text": "Younes Belkada for help with the integration of our software into the Hugging Face transformers", "confidence": 0.9896743893623352, "text_region": [[426.0, 2211.0], [2019.0, 2214.0], [2019.0, 2261.0], [426.0, 2257.0]]}, {"text": "stack.", "confidence": 0.966266930103302, "text_region": [[426.0, 2257.0], [526.0, 2257.0], [526.0, 2303.0], [426.0, 2303.0]]}], "img_idx": 0, "score": 0.9858805537223816}
{"type": "title", "bbox": [432, 1924, 828, 1964], "res": [{"text": "Acknowledgements", "confidence": 0.9988266229629517, "text_region": [[426.0, 1914.0], [835.0, 1917.0], [835.0, 1974.0], [426.0, 1970.0]]}], "img_idx": 0, "score": 0.9632517099380493}
{"type": "title", "bbox": [430, 692, 849, 733], "res": [{"text": "9Broader Impacts", "confidence": 0.9705976247787476, "text_region": [[420.0, 683.0], [855.0, 690.0], [854.0, 743.0], [419.0, 736.0]]}], "img_idx": 0, "score": 0.9597247838973999}
{"type": "footer", "bbox": [1206, 2973, 1244, 3000], "res": [{"text": "16", "confidence": 0.9989711046218872, "text_region": [[1201.0, 2970.0], [1254.0, 2970.0], [1254.0, 3010.0], [1201.0, 3010.0]]}], "img_idx": 0, "score": 0.8854875564575195}
