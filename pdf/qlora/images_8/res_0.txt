{"type": "text", "bbox": [428, 2392, 2017, 2889], "res": [{"text": "The Vicuna benchmark [10] results relative to ChatGPT are shown in Table 6. We find that Guanaco", "confidence": 0.9943262934684753, "text_region": [[432.0, 2389.0], [2019.0, 2389.0], [2019.0, 2435.0], [432.0, 2435.0]]}, {"text": "65B is the best-performing model after GPT-4, achieving 99.3% performance relative to ChatGPT.", "confidence": 0.9937385320663452, "text_region": [[426.0, 2429.0], [2029.0, 2429.0], [2029.0, 2485.0], [426.0, 2485.0]]}, {"text": "Guanaco 33B has more parameters than the Vicuna 13B model, but uses only 4-bit precision for its", "confidence": 0.9896944165229797, "text_region": [[429.0, 2478.0], [2019.0, 2478.0], [2019.0, 2524.0], [429.0, 2524.0]]}, {"text": "weights and is thus much more memory efficient at 21 GB vs 26 GB, providing a three percentage", "confidence": 0.9900566935539246, "text_region": [[426.0, 2518.0], [2019.0, 2521.0], [2019.0, 2567.0], [426.0, 2564.0]]}, {"text": "points of improvement over Vicuna 13B. Furthermore, Guanaco 7B easily fits on modern phones at a", "confidence": 0.9944536685943604, "text_region": [[426.0, 2564.0], [2019.0, 2564.0], [2019.0, 2610.0], [426.0, 2610.0]]}, {"text": "5 GB footprint while still scoring nearly 20 percentage points higher than Alpaca 13B.", "confidence": 0.9892189502716064, "text_region": [[426.0, 2610.0], [1813.0, 2610.0], [1813.0, 2656.0], [426.0, 2656.0]]}, {"text": "However, Table 6 also has very wide confidence intervals, with many models overlapping in per-", "confidence": 0.9819134473800659, "text_region": [[422.0, 2666.0], [2022.0, 2670.0], [2022.0, 2726.0], [422.0, 2722.0]]}, {"text": "formance. We hypothesize that this uncertainty comes from the lack of clear specification of scale,", "confidence": 0.9824337363243103, "text_region": [[429.0, 2719.0], [2022.0, 2719.0], [2022.0, 2765.0], [429.0, 2765.0]]}, {"text": "e.g., it is unclear what 8 on a 10 point scale means across different scenarios. As such, we instead", "confidence": 0.9938899278640747, "text_region": [[422.0, 2759.0], [2022.0, 2752.0], [2022.0, 2808.0], [423.0, 2815.0]]}, {"text": "recommend using the Elo ranking method [16], based on pairwise judgments from human annotators", "confidence": 0.998112142086029, "text_region": [[429.0, 2805.0], [2019.0, 2805.0], [2019.0, 2851.0], [429.0, 2851.0]]}, {"text": "and GPT-4 to avoid the problem of grounding an absolute scale. Elo ratings of the most competitive", "confidence": 0.9897570610046387, "text_region": [[432.0, 2851.0], [2019.0, 2851.0], [2019.0, 2897.0], [432.0, 2897.0]]}], "img_idx": 0, "score": 0.9754309058189392}
{"type": "text", "bbox": [430, 2153, 2020, 2363], "res": [{"text": "Based on our automated and human evaluations, we find that the top QLoRA tuned model, Guanaco", "confidence": 0.9863071441650391, "text_region": [[429.0, 2148.0], [2019.0, 2148.0], [2019.0, 2194.0], [429.0, 2194.0]]}, {"text": "65B, which we finetune on a variant of OASST1, is the best-performing open-source chatbot model", "confidence": 0.9878854155540466, "text_region": [[429.0, 2194.0], [2019.0, 2194.0], [2019.0, 2241.0], [429.0, 2241.0]]}, {"text": " and offers performance competitive to ChatGPT. When compared to GPT-4, Guanaco 65B and 33B", "confidence": 0.9866353869438171, "text_region": [[422.0, 2231.0], [2022.0, 2227.0], [2022.0, 2284.0], [422.0, 2287.0]]}, {"text": "have an expected win probability of 30%, based on Elo rating from human annotators system-level", "confidence": 0.9958223104476929, "text_region": [[426.0, 2277.0], [2016.0, 2277.0], [2016.0, 2323.0], [426.0, 2323.0]]}, {"text": " pairwise comparisons - the highest reported to date.", "confidence": 0.9780023694038391, "text_region": [[422.0, 2327.0], [1254.0, 2323.0], [1254.0, 2369.0], [423.0, 2373.0]]}], "img_idx": 0, "score": 0.9566821455955505}
{"type": "text", "bbox": [424, 1453, 2021, 2055], "res": [{"text": "Elo Rating With both human and automated pairwise comparisons, we create a tournament-style", "confidence": 0.9955453276634216, "text_region": [[423.0, 1439.0], [2022.0, 1445.0], [2022.0, 1498.0], [422.0, 1492.0]]}, {"text": "competition where models compete against each other. The tournament is made up of matches where", "confidence": 0.9900789856910706, "text_region": [[426.0, 1492.0], [2016.0, 1492.0], [2016.0, 1538.0], [426.0, 1538.0]]}, {"text": " pairs of models compete to produce the best response for a given prompt. This is similar to how Bai", "confidence": 0.9921888709068298, "text_region": [[422.0, 1531.0], [2022.0, 1528.0], [2022.0, 1584.0], [422.0, 1587.0]]}, {"text": "et al. [4] and Chiang et al. [10] compare models, but we also employ GPT-4 ratings in addition to", "confidence": 0.9925148487091064, "text_region": [[426.0, 1574.0], [2022.0, 1574.0], [2022.0, 1630.0], [426.0, 1630.0]]}, {"text": "human ratings. We randomly sample from the set of labeled comparisons to compute Elo [16, 17].", "confidence": 0.993845522403717, "text_region": [[426.0, 1620.0], [2019.0, 1620.0], [2019.0, 1667.0], [426.0, 1667.0]]}, {"text": "Elo rating, which is widely used in chess and other games, is a measure of the expected win-rate", "confidence": 0.9897077083587646, "text_region": [[426.0, 1660.0], [2022.0, 1660.0], [2022.0, 1716.0], [426.0, 1716.0]]}, {"text": "relative to an opponent's win rate, for example, an Elo of 1100 vs 1000 means the Elo 1100 player", "confidence": 0.9840466976165771, "text_region": [[429.0, 1709.0], [2019.0, 1709.0], [2019.0, 1756.0], [429.0, 1756.0]]}, {"text": "has an expected win-rate of approximately 65% against the Elo 1000 opponent; a 1000 vs 1000 or", "confidence": 0.9953112602233887, "text_region": [[426.0, 1749.0], [2022.0, 1749.0], [2022.0, 1805.0], [426.0, 1805.0]]}, {"text": "1100 vs 1i00 match results in an expected win-rate of 50%. The Elo rating changes after each match", "confidence": 0.9867647886276245, "text_region": [[423.0, 1785.0], [2022.0, 1792.0], [2022.0, 1848.0], [422.0, 1841.0]]}, {"text": "proportionally to the expected outcome, that is, an unexpected upset leads to a large change in Elo", "confidence": 0.9953550100326538, "text_region": [[429.0, 1841.0], [2019.0, 1841.0], [2019.0, 1888.0], [429.0, 1888.0]]}, {"text": "rating while an expected outcome leads to a small change. Over time, Elo ratings approximately", "confidence": 0.9859870672225952, "text_region": [[429.0, 1884.0], [2019.0, 1884.0], [2019.0, 1930.0], [429.0, 1930.0]]}, {"text": "match the skill of each player at playing the game. We start with a score of 1,000 and use K = 32.", "confidence": 0.988579511642456, "text_region": [[426.0, 1924.0], [2026.0, 1924.0], [2026.0, 1980.0], [426.0, 1980.0]]}, {"text": " Similar to Chiang et al. [10], we repeat this procedure 10,000 times with different random seeds to", "confidence": 0.9741361737251282, "text_region": [[422.0, 1967.0], [2022.0, 1963.0], [2022.0, 2020.0], [422.0, 2023.0]]}, {"text": "control for ordering effects, e.g., the effect of which model pairs compete with each other first.", "confidence": 0.997222363948822, "text_region": [[429.0, 2016.0], [1903.0, 2016.0], [1903.0, 2062.0], [429.0, 2062.0]]}], "img_idx": 0, "score": 0.9553362131118774}
{"type": "text", "bbox": [424, 299, 2018, 546], "res": [{"text": " Benchmark Data We evaluate on two curated datasets of queries (questions): the Vicuna prompts", "confidence": 0.9860634207725525, "text_region": [[422.0, 287.0], [2022.0, 290.0], [2022.0, 347.0], [422.0, 343.0]]}, {"text": "[10] and the OASST1 validation dataset [31]. We use the Vicuna prompts, a set of 80 prompts from a", "confidence": 0.9837947487831116, "text_region": [[429.0, 340.0], [2019.0, 340.0], [2019.0, 386.0], [429.0, 386.0]]}, {"text": "diverse set of categories, without modifications. The OASST1 dataset is a multilingual collection of", "confidence": 0.9915524125099182, "text_region": [[429.0, 383.0], [2022.0, 383.0], [2022.0, 429.0], [429.0, 429.0]]}, {"text": "crowd-sourced multiturn dialogs between a user and an assistant. We select all user messages in the", "confidence": 0.9859915971755981, "text_region": [[432.0, 429.0], [2019.0, 429.0], [2019.0, 475.0], [432.0, 475.0]]}, {"text": "validation dataset as queries and include previous turns in the prompt. This procedure leads to 953", "confidence": 0.9946067929267883, "text_region": [[432.0, 472.0], [2019.0, 472.0], [2019.0, 518.0], [432.0, 518.0]]}, {"text": "unique user queries. We term these two datasets the Vicuna and OA benchmarks.", "confidence": 0.994465708732605, "text_region": [[429.0, 515.0], [1723.0, 511.0], [1723.0, 558.0], [429.0, 561.0]]}], "img_idx": 0, "score": 0.9226905703544617}
{"type": "text", "bbox": [425, 583, 2018, 1420], "res": [{"text": "Automated Evaluation  First, based on the evaluation protocol introduced by Chiang et al. [10]", "confidence": 0.9861100316047668, "text_region": [[429.0, 581.0], [2016.0, 581.0], [2016.0, 627.0], [429.0, 627.0]]}, {"text": "we use GPT-4 to rate the performance of different systems against ChatGPT (GPT-3.5 Turbo) on the", "confidence": 0.9964027404785156, "text_region": [[432.0, 630.0], [2019.0, 630.0], [2019.0, 676.0], [432.0, 676.0]]}, {"text": "Vicuna benchmark. Given a query along with ChatGPT's and a model's responses, GPT-4 is prompted", "confidence": 0.9952734112739563, "text_region": [[426.0, 663.0], [2022.0, 667.0], [2022.0, 723.0], [426.0, 719.0]]}, {"text": "to assign a score out of ten to both responses and provide an explanation. The overall performance of", "confidence": 0.9922999739646912, "text_region": [[429.0, 716.0], [2019.0, 716.0], [2019.0, 762.0], [429.0, 762.0]]}, {"text": "a model is calculated as a percentage of the score that ChatGPT achieved. Note this relative score", "confidence": 0.9817319512367249, "text_region": [[426.0, 759.0], [2022.0, 752.0], [2022.0, 805.0], [426.0, 812.0]]}, {"text": "can be higher than 1o0% if the model achieves a higher absolute score than ChatGPT. We find a", "confidence": 0.9818468689918518, "text_region": [[432.0, 805.0], [2019.0, 805.0], [2019.0, 848.0], [432.0, 848.0]]}, {"text": "significant ordering effect with GPT-4 increasing the score of the response occurring earlier in the", "confidence": 0.9908768534660339, "text_region": [[429.0, 845.0], [2022.0, 845.0], [2022.0, 901.0], [429.0, 901.0]]}, {"text": " prompt. To control for such effects, we recommend reporting the mean score over both orders.", "confidence": 0.9788171052932739, "text_region": [[422.0, 888.0], [1902.0, 884.0], [1903.0, 940.0], [422.0, 944.0]]}, {"text": "Next, we measure performance through direct comparisons between system outputs. We simplify", "confidence": 0.9957442283630371, "text_region": [[429.0, 957.0], [2016.0, 957.0], [2016.0, 1003.0], [429.0, 1003.0]]}, {"text": "the rating scheme to a three-class labeling problem that accounts for ties. We prompt GPT-4 to", "confidence": 0.9904746413230896, "text_region": [[429.0, 1003.0], [2019.0, 1003.0], [2019.0, 1049.0], [429.0, 1049.0]]}, {"text": "pick the best response or declare a tie and provide an explanation. We conduct these head-to-head", "confidence": 0.9937529563903809, "text_region": [[432.0, 1046.0], [2019.0, 1046.0], [2019.0, 1092.0], [432.0, 1092.0]]}, {"text": "comparisons on all permutations of pairs of systems on both the Vicuna and OA benchmarks.", "confidence": 0.9975130558013916, "text_region": [[429.0, 1086.0], [1899.0, 1086.0], [1899.0, 1132.0], [429.0, 1132.0]]}, {"text": "Human Evaluation While recent work indicates generative models can be effectively employed", "confidence": 0.985802948474884, "text_region": [[432.0, 1158.0], [2016.0, 1158.0], [2016.0, 1204.0], [432.0, 1204.0]]}, {"text": "for system evaluations [19], the reliability GPT-4 ratings to assess chatbot performance is, to our", "confidence": 0.9875444173812866, "text_region": [[429.0, 1201.0], [2016.0, 1201.0], [2016.0, 1244.0], [429.0, 1244.0]]}, {"text": "knowledge, yet to be proven to correlate with human judgments. Therefore, we run two parallel", "confidence": 0.9844421744346619, "text_region": [[429.0, 1247.0], [2016.0, 1247.0], [2016.0, 1294.0], [429.0, 1294.0]]}, {"text": " human evaluations on the Vicuna benchmark matching both automated evaluation protocols described", "confidence": 0.9895883798599243, "text_region": [[419.0, 1280.0], [2022.0, 1284.0], [2022.0, 1340.0], [419.0, 1336.0]]}, {"text": "above. We use Amazon Mechanical Turk (AMT) and get two human annotators for comparisons to", "confidence": 0.9886036515235901, "text_region": [[426.0, 1330.0], [2022.0, 1333.0], [2022.0, 1379.0], [426.0, 1376.0]]}, {"text": "ChatGPT and three annotators for pairwise comparisons.", "confidence": 0.998775064945221, "text_region": [[426.0, 1373.0], [1337.0, 1380.0], [1337.0, 1426.0], [426.0, 1419.0]]}], "img_idx": 0, "score": 0.8212073445320129}
{"type": "title", "bbox": [549, 2093, 1446, 2131], "res": [{"text": " 5.3  Guanaco: QLoRA trained on OASST1 is a State-of-the-art Chatbot", "confidence": 0.9750039577484131, "text_region": [[422.0, 2086.0], [1686.0, 2089.0], [1686.0, 2135.0], [422.0, 2132.0]]}], "img_idx": 0, "score": 0.6782145500183105}
{"type": "footer", "bbox": [1214, 2974, 1240, 2998], "res": [{"text": "9", "confidence": 0.9969291090965271, "text_region": [[1207.0, 2973.0], [1241.0, 2973.0], [1241.0, 3006.0], [1207.0, 3006.0]]}], "img_idx": 0, "score": 0.605563223361969}
