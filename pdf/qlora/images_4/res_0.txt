{"type": "text", "bbox": [430, 1955, 2016, 2471], "res": [{"text": "We use NF4 for W and FP8 for c2. We use a blocksize of 64 for W for higher quantization precision", "confidence": 0.9883686304092407, "text_region": [[429.0, 1957.0], [2019.0, 1957.0], [2019.0, 2003.0], [429.0, 2003.0]]}, {"text": "and a blocksize of 256 for c2 to conserve memory.", "confidence": 0.9917470216751099, "text_region": [[426.0, 1996.0], [1234.0, 2007.0], [1234.0, 2053.0], [426.0, 2043.0]]}, {"text": "For parameter updates only the gradient with respect to the eror for the adapters weights , are", "confidence": 0.9867401719093323, "text_region": [[422.0, 2062.0], [2026.0, 2062.0], [2026.0, 2128.0], [422.0, 2128.0]]}, {"text": "which proceeds via equation (5) with dequantization from storage WNF4 to computation data type", "confidence": 0.980933427810669, "text_region": [[419.0, 2168.0], [2026.0, 2171.0], [2025.0, 2241.0], [419.0, 2237.0]]}, {"text": " To summarize, QLoRA has one storage data type (usually 4-bit NormalFloat) and a computation", "confidence": 0.9879348874092102, "text_region": [[422.0, 2290.0], [2019.0, 2294.0], [2019.0, 2350.0], [422.0, 2346.0]]}, {"text": "data type (16-bit BrainFloat). We dequantize the storage data type to the computation data type", "confidence": 0.9991285800933838, "text_region": [[429.0, 2343.0], [2019.0, 2343.0], [2019.0, 2389.0], [429.0, 2389.0]]}, {"text": "to perform the forward and backward pass, but we only compute weight gradients for the LoRA", "confidence": 0.9912575483322144, "text_region": [[429.0, 2389.0], [2019.0, 2389.0], [2019.0, 2435.0], [429.0, 2435.0]]}, {"text": "parameters which use 16-bit BrainFloat.", "confidence": 0.9955192804336548, "text_region": [[429.0, 2432.0], [1074.0, 2432.0], [1074.0, 2478.0], [429.0, 2478.0]]}], "img_idx": 0, "score": 0.981036901473999}
{"type": "text", "bbox": [430, 2597, 2016, 2812], "res": [{"text": "We have discussed how QLoRA works and how it can significantly reduce the required memory for", "confidence": 0.9806748032569885, "text_region": [[426.0, 2590.0], [2019.0, 2594.0], [2019.0, 2640.0], [426.0, 2637.0]]}, {"text": "finetuning models. The main question now is whether QLoRA can perform as well as full-model", "confidence": 0.990623414516449, "text_region": [[429.0, 2640.0], [2016.0, 2640.0], [2016.0, 2686.0], [429.0, 2686.0]]}, {"text": "finetuning. Furthermore, we want to analyze the components of QLoRA including the impact of", "confidence": 0.9836437702178955, "text_region": [[422.0, 2676.0], [2022.0, 2680.0], [2022.0, 2736.0], [422.0, 2732.0]]}, {"text": " NormalFloat4 over standard Float4. The following sections will discuss the experiments that aimed", "confidence": 0.9858298897743225, "text_region": [[422.0, 2719.0], [2022.0, 2723.0], [2022.0, 2779.0], [422.0, 2775.0]]}, {"text": "at answering these questions.", "confidence": 0.9835716485977173, "text_region": [[429.0, 2775.0], [895.0, 2775.0], [895.0, 2818.0], [429.0, 2818.0]]}], "img_idx": 0, "score": 0.9806800484657288}
{"type": "text", "bbox": [426, 299, 2019, 1576], "res": [{"text": "ensure a discrete zeropoint of O and to use all 2k bits for a k-bit datatype, we create an asymmetric", "confidence": 0.9866349101066589, "text_region": [[426.0, 294.0], [2016.0, 294.0], [2016.0, 340.0], [426.0, 340.0]]}, {"text": "data type by estimating the quantiles qi of two ranges qi: 2k-1 for the negative part and 2k-1 + 1 for", "confidence": 0.9718722701072693, "text_region": [[426.0, 333.0], [2022.0, 333.0], [2022.0, 389.0], [426.0, 389.0]]}, {"text": "the positive part and then we unify these sets of qi and remove one of the two zeros that occurs in both", "confidence": 0.9908062815666199, "text_region": [[429.0, 386.0], [2019.0, 386.0], [2019.0, 429.0], [429.0, 429.0]]}, {"text": "sets. We term the resulting data type that has equal expected number of values in each quantization bin", "confidence": 0.9906759262084961, "text_region": [[432.0, 429.0], [2019.0, 429.0], [2019.0, 475.0], [432.0, 475.0]]}, {"text": "k-bit NormalFloat (NFk), since the data type is information-theoretically optimal for zero-centered", "confidence": 0.9906727075576782, "text_region": [[432.0, 472.0], [2019.0, 472.0], [2019.0, 518.0], [432.0, 518.0]]}, {"text": "normally distributed data. The exact values of this data type can be found in Appendix E.", "confidence": 0.9855779409408569, "text_region": [[429.0, 515.0], [1853.0, 515.0], [1853.0, 561.0], [429.0, 561.0]]}, {"text": "Double Quantization We introduce Double Quantization (DQ), the process of quantizing the", "confidence": 0.9806735515594482, "text_region": [[422.0, 577.0], [2022.0, 581.0], [2022.0, 637.0], [422.0, 634.0]]}, {"text": "quantization constants for additional memory savings. While a small blocksize is required for precise", "confidence": 0.9965255260467529, "text_region": [[429.0, 630.0], [2019.0, 630.0], [2019.0, 676.0], [429.0, 676.0]]}, {"text": "4-bit quantization [13], it also has a considerable memory overhead. For example, using 32-bit", "confidence": 0.9951717257499695, "text_region": [[422.0, 667.0], [2022.0, 670.0], [2022.0, 726.0], [422.0, 723.0]]}, {"text": "constants and a blocksize of 64 for W, quantization constants add 32/64 = 0.5 bits per parameter on", "confidence": 0.9887000322341919, "text_region": [[429.0, 719.0], [2019.0, 719.0], [2019.0, 766.0], [429.0, 766.0]]}, {"text": "average. Double Quantization helps reduce the memory footprint of quantization constants.", "confidence": 0.9826564788818359, "text_region": [[426.0, 766.0], [1883.0, 766.0], [1883.0, 809.0], [426.0, 809.0]]}, {"text": "More specifically, Double Quantization treats quantization constants cFP32 of the first quantization", "confidence": 0.9652191996574402, "text_region": [[422.0, 815.0], [2026.0, 815.0], [2026.0, 881.0], [422.0, 881.0]]}, {"text": "as inputs to a second quantization. This second step yields the quantized quantization constants", "confidence": 0.9888232350349426, "text_region": [[432.0, 874.0], [2019.0, 874.0], [2019.0, 921.0], [432.0, 921.0]]}, {"text": "cFP8 and the second level of quantization constants CFP32. We use 8-bit Floats with a blocksize of", "confidence": 0.9826262593269348, "text_region": [[429.0, 908.0], [2026.0, 908.0], [2026.0, 964.0], [429.0, 964.0]]}, {"text": "256 for the second quantization as no performance degradation is observed for 8-bit quantization,", "confidence": 0.9963772296905518, "text_region": [[426.0, 947.0], [2022.0, 954.0], [2022.0, 1010.0], [426.0, 1003.0]]}, {"text": "in line with results from Dettmers and Zettlemoyer [13]. Since the cEP32 are positive, we subtract", "confidence": 0.9772437810897827, "text_region": [[426.0, 997.0], [2019.0, 997.0], [2019.0, 1053.0], [426.0, 1053.0]]}, {"text": "the mean from c2 before quantization to center the values around zero and make use of symmetric", "confidence": 0.9965473413467407, "text_region": [[429.0, 1049.0], [2019.0, 1049.0], [2019.0, 1092.0], [429.0, 1092.0]]}, {"text": "quantization. On average, for a blocksize of 64, this quantization reduces the memory footprint per", "confidence": 0.9846518039703369, "text_region": [[432.0, 1092.0], [2016.0, 1092.0], [2016.0, 1138.0], [432.0, 1138.0]]}, {"text": "parameter from 32/64 = 0.5 bits, to 8/64 + 32/(64 \u00b7 256) = 0.127 bits, a reduction of 0.373 bits", "confidence": 0.9820974469184875, "text_region": [[426.0, 1135.0], [2019.0, 1132.0], [2019.0, 1178.0], [426.0, 1181.0]]}, {"text": " per parameter.", "confidence": 0.9836342334747314, "text_region": [[422.0, 1185.0], [661.0, 1178.0], [662.0, 1221.0], [423.0, 1228.0]]}, {"text": " Paged Optimizers use the NVIDIA unified memory 3 feature wich does automatic page-to-page", "confidence": 0.9850884079933167, "text_region": [[422.0, 1241.0], [2022.0, 1244.0], [2022.0, 1300.0], [422.0, 1297.0]]}, {"text": "transfers between the CPU and GPU for error-free GPU processing in the scenario where the GPU", "confidence": 0.9955288171768188, "text_region": [[429.0, 1294.0], [2019.0, 1294.0], [2019.0, 1340.0], [429.0, 1340.0]]}, {"text": "occasionally runs out-of-memory. The feature works like regular memory paging between CPU RAM", "confidence": 0.9856641292572021, "text_region": [[426.0, 1333.0], [2022.0, 1330.0], [2022.0, 1386.0], [426.0, 1389.0]]}, {"text": "and the disk. We use this feature to allocate paged memory for the optimizer states which are then", "confidence": 0.9928000569343567, "text_region": [[429.0, 1383.0], [2019.0, 1383.0], [2019.0, 1429.0], [429.0, 1429.0]]}, {"text": "automatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU", "confidence": 0.9956336617469788, "text_region": [[429.0, 1422.0], [2022.0, 1422.0], [2022.0, 1478.0], [429.0, 1478.0]]}, {"text": " memory when the memory is needed in the optimizer update step.", "confidence": 0.9904244542121887, "text_region": [[423.0, 1465.0], [1483.0, 1469.0], [1483.0, 1525.0], [422.0, 1521.0]]}, {"text": "QLoRA. Using the components described above, we define QLoRA for a single linear layer in", "confidence": 0.9928068518638611, "text_region": [[432.0, 1541.0], [2016.0, 1541.0], [2016.0, 1587.0], [432.0, 1587.0]]}], "img_idx": 0, "score": 0.9459380507469177}
{"type": "text", "bbox": [1971, 1701, 2019, 1731], "res": [{"text": "(5)", "confidence": 0.9841808676719666, "text_region": [[1962.0, 1693.0], [2029.0, 1693.0], [2029.0, 1746.0], [1962.0, 1746.0]]}], "img_idx": 0, "score": 0.8871962428092957}
{"type": "text", "bbox": [1970, 1865, 2019, 1898], "res": [{"text": "doubleDequant(CFP32 , C-bit, Wk-bit) = dequant(dequant(CFP32 , C-bit), W4bit) = WBF16,  (6)", "confidence": 0.9394046664237976, "text_region": [[526.0, 1855.0], [2026.0, 1855.0], [2026.0, 1911.0], [526.0, 1911.0]]}], "img_idx": 0, "score": 0.8848541975021362}
{"type": "text", "bbox": [438, 1798, 1900, 1903], "res": [{"text": "where doubleDequant() is defined as:", "confidence": 0.9656615853309631, "text_region": [[429.0, 1782.0], [1041.0, 1782.0], [1041.0, 1828.0], [429.0, 1828.0]]}, {"text": "doubleDequant(CFP32 , C-bit, Wk-bit) = dequant(dequant(CFP32 , C-bit), W4bit) = WBF16,  (6)", "confidence": 0.9394046664237976, "text_region": [[526.0, 1855.0], [2026.0, 1855.0], [2026.0, 1911.0], [526.0, 1911.0]]}], "img_idx": 0, "score": 0.7549406290054321}
{"type": "title", "bbox": [597, 2521, 1045, 2558], "res": [{"text": "4 QLoRA vs. Standard Finetuning", "confidence": 0.9820669889450073, "text_region": [[419.0, 2511.0], [1181.0, 2518.0], [1180.0, 2574.0], [419.0, 2567.0]]}], "img_idx": 0, "score": 0.5603708028793335}
{"type": "footer", "bbox": [1214, 2973, 1240, 2998], "res": [{"text": "5", "confidence": 0.999257504940033, "text_region": [[1211.0, 2973.0], [1241.0, 2973.0], [1241.0, 3003.0], [1211.0, 3003.0]]}], "img_idx": 0, "score": 0.6256061792373657}
{"type": "reference", "bbox": [483, 2854, 1173, 2887], "res": [{"text": "-guid", "confidence": 0.9751802682876587, "text_region": [[1098.0, 2868.0], [1164.0, 2868.0], [1164.0, 2891.0], [1098.0, 2891.0]]}], "img_idx": 0, "score": 0.7657715678215027}
{"type": "equation", "bbox": [653, 1689, 1797, 1739], "res": [{"text": "YBF16 = XBF16doubleDequant(CFP32, C-bi, WNF4) + XBF16LBF16LBF16,", "confidence": 0.9549567699432373, "text_region": [[639.0, 1683.0], [1803.0, 1683.0], [1803.0, 1749.0], [639.0, 1749.0]]}], "img_idx": 0, "score": 0.7852693796157837}
