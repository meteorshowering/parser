{"type": "text", "bbox": [428, 419, 1266, 1105], "res": [{"text": "QLoRA reduces the average memory requirements", "confidence": 0.9989413022994995, "text_region": [[429.0, 412.0], [1267.0, 416.0], [1267.0, 462.0], [429.0, 459.0]]}, {"text": "of finetuning a 65B parameter model from >780GB", "confidence": 0.9943783283233643, "text_region": [[426.0, 455.0], [1264.0, 455.0], [1264.0, 502.0], [426.0, 502.0]]}, {"text": "of GPU memory to <48GB without degrading the", "confidence": 0.9772306084632874, "text_region": [[426.0, 502.0], [1267.0, 502.0], [1267.0, 548.0], [426.0, 548.0]]}, {"text": "runtime or predictive performance compared to a 16-", "confidence": 0.9909625053405762, "text_region": [[419.0, 541.0], [1274.0, 538.0], [1274.0, 594.0], [419.0, 597.0]]}, {"text": "bit fully finetuned baseline. This marks a significant", "confidence": 0.9988563060760498, "text_region": [[426.0, 584.0], [1267.0, 587.0], [1267.0, 637.0], [426.0, 634.0]]}, {"text": "shift in accessibility of LLM finetuning: now the", "confidence": 0.9893702864646912, "text_region": [[429.0, 634.0], [1267.0, 634.0], [1267.0, 680.0], [429.0, 680.0]]}, {"text": "largest publicly available models to date finetunable", "confidence": 0.9931408762931824, "text_region": [[422.0, 673.0], [1270.0, 666.0], [1271.0, 723.0], [423.0, 729.0]]}, {"text": "on a single GPU. Using QLoRA, we train the Gua-", "confidence": 0.9806200265884399, "text_region": [[426.0, 719.0], [1267.0, 719.0], [1267.0, 766.0], [426.0, 766.0]]}, {"text": "naco family of models, with the second best model", "confidence": 0.9988501667976379, "text_region": [[429.0, 766.0], [1267.0, 766.0], [1267.0, 809.0], [429.0, 809.0]]}, {"text": "reaching 97.8% of the performance level of ChatGPT", "confidence": 0.9848512411117554, "text_region": [[422.0, 805.0], [1267.0, 802.0], [1267.0, 848.0], [423.0, 851.0]]}, {"text": "on the Vicuna [10] benchmark, while being trainable", "confidence": 0.9965561628341675, "text_region": [[426.0, 848.0], [1264.0, 848.0], [1264.0, 894.0], [426.0, 894.0]]}, {"text": "in less than 12 hours on a single consumer GPU;", "confidence": 0.9898528456687927, "text_region": [[426.0, 894.0], [1271.0, 894.0], [1271.0, 940.0], [426.0, 940.0]]}, {"text": "using a single professional GPU over 24 hours we", "confidence": 0.9944872260093689, "text_region": [[422.0, 934.0], [1274.0, 931.0], [1274.0, 987.0], [423.0, 990.0]]}, {"text": " achieve 99.3% with our largest model, essentially", "confidence": 0.9889278411865234, "text_region": [[419.0, 973.0], [1271.0, 977.0], [1270.0, 1033.0], [419.0, 1030.0]]}, {"text": "closing the gap to ChatGPT on the Vicuna bench-", "confidence": 0.9880622029304504, "text_region": [[426.0, 1023.0], [1274.0, 1023.0], [1274.0, 1079.0], [426.0, 1079.0]]}, {"text": "mark. When deployed, our smallest Guanaco model", "confidence": 0.9828093647956848, "text_region": [[429.0, 1069.0], [1267.0, 1069.0], [1267.0, 1115.0], [429.0, 1115.0]]}], "img_idx": 0, "score": 0.9908317923545837}
{"type": "text", "bbox": [1302, 286, 2020, 593], "res": [{"text": "Table 1: Elo ratings for a competition between", "confidence": 0.9950698614120483, "text_region": [[1291.0, 271.0], [2022.0, 274.0], [2022.0, 330.0], [1290.0, 327.0]]}, {"text": "models, averaged for 10,000 random initial order-", "confidence": 0.9777536988258362, "text_region": [[1297.0, 320.0], [2019.0, 320.0], [2019.0, 366.0], [1297.0, 366.0]]}, {"text": "ings. The winner of a match is determined by", "confidence": 0.9804641604423523, "text_region": [[1297.0, 360.0], [2022.0, 360.0], [2022.0, 403.0], [1297.0, 403.0]]}, {"text": "GPT-4 which declares which response is better for", "confidence": 0.9868184328079224, "text_region": [[1297.0, 399.0], [2022.0, 399.0], [2022.0, 446.0], [1297.0, 446.0]]}, {"text": "a given prompt of the the Vicuna benchmark. 95%", "confidence": 0.9850075840950012, "text_region": [[1294.0, 439.0], [2022.0, 432.0], [2022.0, 478.0], [1294.0, 485.0]]}, {"text": "confidence intervals are shown (\u00b1). After GPT-", "confidence": 0.9870342016220093, "text_region": [[1297.0, 479.0], [2022.0, 479.0], [2022.0, 525.0], [1297.0, 525.0]]}, {"text": "4, Guanaco 33B and 65B win the most matches,", "confidence": 0.98795485496521, "text_region": [[1294.0, 518.0], [2026.0, 518.0], [2026.0, 564.0], [1294.0, 564.0]]}, {"text": "while Guanaco 13B scores better than Bard.", "confidence": 0.9925211668014526, "text_region": [[1294.0, 558.0], [1939.0, 558.0], [1939.0, 604.0], [1294.0, 604.0]]}], "img_idx": 0, "score": 0.9876263737678528}
{"type": "text", "bbox": [428, 1238, 2019, 1622], "res": [{"text": "QLoRA introduces multiple innovations designed to reduce memory use without sacrificing per-", "confidence": 0.9957588911056519, "text_region": [[429.0, 1231.0], [2022.0, 1234.0], [2022.0, 1280.0], [429.0, 1277.0]]}, {"text": "formance: (1) 4-bit NormalFloat, an information theoretically optimal quantization data type for", "confidence": 0.9949736595153809, "text_region": [[429.0, 1277.0], [2022.0, 1277.0], [2022.0, 1323.0], [429.0, 1323.0]]}, {"text": "normally distributed data that yields better empirical results than 4-bit Integers and 4-bit Floats.", "confidence": 0.989900529384613, "text_region": [[432.0, 1323.0], [2019.0, 1323.0], [2019.0, 1366.0], [432.0, 1366.0]]}, {"text": "(2) Double Quantization, a method that quantizes the quantization constants, saving an average", "confidence": 0.9885936379432678, "text_region": [[426.0, 1360.0], [2019.0, 1366.0], [2019.0, 1412.0], [426.0, 1406.0]]}, {"text": "of about 0.37 bits per parameter (approximately 3 GB for a 65B model). (3) Paged Optimizers,", "confidence": 0.9889041185379028, "text_region": [[432.0, 1409.0], [2019.0, 1409.0], [2019.0, 1455.0], [432.0, 1455.0]]}, {"text": "using NVIDIA unified memory to avoid the gradient checkpointing memory spikes that occur when", "confidence": 0.9949053525924683, "text_region": [[429.0, 1452.0], [2022.0, 1452.0], [2022.0, 1498.0], [429.0, 1498.0]]}, {"text": " processing a mini-batch with a long sequence length. We combine these contributions into a better", "confidence": 0.9892607927322388, "text_region": [[422.0, 1492.0], [2022.0, 1488.0], [2022.0, 1544.0], [422.0, 1548.0]]}, {"text": "tuned LoRA approach that includes adapters at every network layer and thereby avoids almost all of", "confidence": 0.9935407042503357, "text_region": [[426.0, 1534.0], [2026.0, 1534.0], [2026.0, 1591.0], [426.0, 1591.0]]}, {"text": "the accuracy tradeoffs seen in prior work.", "confidence": 0.9831263422966003, "text_region": [[429.0, 1584.0], [1094.0, 1584.0], [1094.0, 1630.0], [429.0, 1630.0]]}], "img_idx": 0, "score": 0.9845494627952576}
{"type": "text", "bbox": [428, 2675, 2014, 2886], "res": [{"text": "We release all model generations with human and GPT-4 annotations to facilitate further study. We", "confidence": 0.9980126619338989, "text_region": [[426.0, 2666.0], [2019.0, 2670.0], [2019.0, 2716.0], [426.0, 2713.0]]}, {"text": "open-source our codebase and CUDA kernels and integrate our methods into the Hugging Face", "confidence": 0.9897388815879822, "text_region": [[426.0, 2713.0], [2022.0, 2713.0], [2022.0, 2769.0], [426.0, 2769.0]]}, {"text": "transformers stack [64], making them easily accessible to all. We release a collection of adapters", "confidence": 0.9976517558097839, "text_region": [[422.0, 2756.0], [2022.0, 2752.0], [2022.0, 2808.0], [422.0, 2812.0]]}, {"text": "for 7/13/33/65B size models, trained on 8 different instruction following datasets, for a total of 32", "confidence": 0.9854575991630554, "text_region": [[429.0, 2802.0], [2019.0, 2802.0], [2019.0, 2848.0], [429.0, 2848.0]]}, {"text": "different open sourced, finetuned models.", "confidence": 0.9874439835548401, "text_region": [[426.0, 2845.0], [1094.0, 2848.0], [1094.0, 2894.0], [426.0, 2891.0]]}], "img_idx": 0, "score": 0.9755523204803467}
{"type": "text", "bbox": [430, 1663, 2019, 2465], "res": [{"text": "QLoRA's efficiency enables us to perform an in-depth study of instruction finetuning and chatbot", "confidence": 0.9955318570137024, "text_region": [[432.0, 1657.0], [2019.0, 1657.0], [2019.0, 1703.0], [432.0, 1703.0]]}, {"text": "performance on model scales that would be impossible using regular finetuning due to memory", "confidence": 0.9976377487182617, "text_region": [[429.0, 1703.0], [2019.0, 1703.0], [2019.0, 1749.0], [429.0, 1749.0]]}, {"text": "overhead. Therefore, we train more than 1,oo0 models across several instruction tuning datasets,", "confidence": 0.991300642490387, "text_region": [[429.0, 1746.0], [2019.0, 1746.0], [2019.0, 1792.0], [429.0, 1792.0]]}, {"text": "model architectures, and sizes between 80M to 65B parameters. In addition to showing that QLoRA", "confidence": 0.9953736066818237, "text_region": [[429.0, 1789.0], [2019.0, 1789.0], [2019.0, 1835.0], [429.0, 1835.0]]}, {"text": "recovers 16-bit performance ($4) and training a state-of-the-art chatbot, Guanaco, ($5), we also", "confidence": 0.9891459345817566, "text_region": [[426.0, 1835.0], [2022.0, 1835.0], [2022.0, 1881.0], [426.0, 1881.0]]}, {"text": "analyze trends in the trained models. First, we find that data quality is far more important than", "confidence": 0.9864360094070435, "text_region": [[426.0, 1878.0], [2019.0, 1878.0], [2019.0, 1924.0], [426.0, 1924.0]]}, {"text": "dataset size, e.g., a 9k sample dataset (OASST1) outperformed a 450k sample dataset (FLAN v2,", "confidence": 0.9847978949546814, "text_region": [[429.0, 1921.0], [2022.0, 1921.0], [2022.0, 1967.0], [429.0, 1967.0]]}, {"text": "subsampled) on chatbot performance, even when both are meant to support instruction following", "confidence": 0.986369788646698, "text_region": [[426.0, 1960.0], [2022.0, 1960.0], [2022.0, 2016.0], [426.0, 2016.0]]}, {"text": " generalization. Second, we show that strong Massive Multitask Language Understanding (MMLU)", "confidence": 0.9868066906929016, "text_region": [[422.0, 2000.0], [2022.0, 2000.0], [2022.0, 2056.0], [422.0, 2056.0]]}, {"text": "benchmark performance does not imply strong Vicuna chatbot benchmark performance and vice", "confidence": 0.9877419471740723, "text_region": [[429.0, 2053.0], [2019.0, 2053.0], [2019.0, 2099.0], [429.0, 2099.0]]}, {"text": "versa\u2014in other words, dataset suitability matters more than size for a given task.", "confidence": 0.9876354336738586, "text_region": [[426.0, 2096.0], [1716.0, 2096.0], [1716.0, 2138.0], [426.0, 2138.0]]}, {"text": "Furthermore, we also provide a extensive analysis of chatbot performance that uses both human", "confidence": 0.9909093976020813, "text_region": [[429.0, 2175.0], [2019.0, 2175.0], [2019.0, 2218.0], [429.0, 2218.0]]}, {"text": "raters and GPT-4 for evaluation. We use tournament-style benchmarking where models compete", "confidence": 0.9885684251785278, "text_region": [[429.0, 2214.0], [2022.0, 2214.0], [2022.0, 2260.0], [429.0, 2260.0]]}, {"text": "against each other in matches to produce the best response for a given prompt. The winner of a", "confidence": 0.9842807054519653, "text_region": [[426.0, 2254.0], [2026.0, 2254.0], [2026.0, 2310.0], [426.0, 2310.0]]}, {"text": "match is judged by either GPT-4 or human annotators. The tournament results are aggregated into", "confidence": 0.9891232848167419, "text_region": [[429.0, 2303.0], [2019.0, 2303.0], [2019.0, 2350.0], [429.0, 2350.0]]}, {"text": "Elo scores [16, 17] which determine the ranking of chatbot performance. We find that GPT-4 and", "confidence": 0.985316812992096, "text_region": [[429.0, 2346.0], [2019.0, 2346.0], [2019.0, 2389.0], [429.0, 2389.0]]}, {"text": "human evaluations largely agree on the rank of model performance in the tournaments, but we also", "confidence": 0.994401752948761, "text_region": [[429.0, 2389.0], [2022.0, 2389.0], [2022.0, 2435.0], [429.0, 2435.0]]}, {"text": "find there are instances of strong disagreement. As such, we highlight that model-based evaluation", "confidence": 0.9944784045219421, "text_region": [[426.0, 2429.0], [2026.0, 2429.0], [2026.0, 2485.0], [426.0, 2485.0]]}], "img_idx": 0, "score": 0.9508838653564453}
{"type": "text", "bbox": [429, 2556, 2016, 2636], "res": [{"text": " We augment our chatbot benchmark results with a qualitative analysis of Guanaco models. Our analy-", "confidence": 0.9843597412109375, "text_region": [[419.0, 2544.0], [2022.0, 2548.0], [2022.0, 2604.0], [419.0, 2600.0]]}, {"text": "sis highlights success and failure cases that were not captured by the quantitative benchmarks.", "confidence": 0.9934419989585876, "text_region": [[429.0, 2597.0], [1903.0, 2597.0], [1903.0, 2643.0], [429.0, 2643.0]]}], "img_idx": 0, "score": 0.9389891624450684}
{"type": "text", "bbox": [430, 297, 1261, 379], "res": [{"text": "that are tuned by backpropagating gradients through", "confidence": 0.9889066219329834, "text_region": [[422.0, 290.0], [1274.0, 290.0], [1274.0, 346.0], [422.0, 346.0]]}, {"text": "the quantized weights.", "confidence": 0.9848791360855103, "text_region": [[426.0, 340.0], [792.0, 340.0], [792.0, 386.0], [426.0, 386.0]]}], "img_idx": 0, "score": 0.9190350770950317}
{"type": "text", "bbox": [434, 1119, 2020, 1195], "res": [{"text": "(7B parameters) requires just 5 GB of memory and outperforms a 26 GB Alpaca model by more than ", "confidence": 0.9878407716751099, "text_region": [[422.0, 1109.0], [2026.0, 1109.0], [2026.0, 1165.0], [422.0, 1165.0]]}, {"text": "20 percentage points on the Vicuna benchmark (Table 6).", "confidence": 0.9840879440307617, "text_region": [[429.0, 1158.0], [1344.0, 1158.0], [1344.0, 1204.0], [429.0, 1204.0]]}], "img_idx": 0, "score": 0.881695032119751}
{"type": "table", "bbox": [1321, 632, 1994, 1074], "res": {"cell_bbox": [[7.263723373413086, 2.3176844120025635, 671.2962646484375, 2.519594430923462, 671.2516479492188, 58.44843292236328, 7.128116607666016, 55.502906799316406], [15.122557640075684, 55.69020462036133, 670.709716796875, 58.57424545288086, 670.61669921875, 358.1195373535156, 14.267156600952148, 354.28662109375]], "html": "<html><body><table><tbody><tr><td>Model Size Elo</td></tr><tr><td>GPT-4 1348 \u00b1 1 Guanaco 65B 41 GB 1022\u00b11 Guanaco 33B 21 GB 992\u00b11 Vicuna 13B 26 GB 974 \u00b1 1 ChatGPT 966 \u00b1 1 Guanaco 13B 10 GB 916 \u00b1 1 Bard 902 \u00b1 1 Guanaco 7B 6 GB 879 \u00b1 1</td></tr></tbody></table></body></html>"}, "img_idx": 0, "score": 0.9484163522720337}
{"type": "footer", "bbox": [1214, 2973, 1240, 2999], "res": [{"text": "2", "confidence": 0.9760555624961853, "text_region": [[1217.0, 2973.0], [1234.0, 2973.0], [1234.0, 2996.0], [1217.0, 2996.0]]}], "img_idx": 0, "score": 0.6147329807281494}
