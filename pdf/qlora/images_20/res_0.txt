{"type": "text", "bbox": [429, 298, 2020, 543], "res": [{"text": "[72] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V.", "confidence": 0.9835649728775024, "text_region": [[419.0, 290.0], [2029.0, 287.0], [2029.0, 343.0], [419.0, 347.0]]}, {"text": "Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068,", "confidence": 0.9962034821510315, "text_region": [[509.0, 337.0], [2022.0, 337.0], [2022.0, 393.0], [509.0, 393.0]]}, {"text": "2022.", "confidence": 0.9958102107048035, "text_region": [[519.0, 386.0], [612.0, 386.0], [612.0, 429.0], [519.0, 429.0]]}, {"text": "[73] R. Zhong, K. Lee, Z. Zhang, and D. Klein. Adapting language models for zero-shot learning by", "confidence": 0.9878928065299988, "text_region": [[429.0, 462.0], [2022.0, 462.0], [2022.0, 508.0], [429.0, 508.0]]}, {"text": " meta-tuning on dataset and prompt collections. arXiv preprint arXiv:2104.04670, 2021.", "confidence": 0.992274284362793, "text_region": [[509.0, 502.0], [1916.0, 502.0], [1916.0, 548.0], [509.0, 548.0]]}], "img_idx": 0, "score": 0.9785283803939819}
{"type": "footer", "bbox": [1201, 2973, 1240, 3002], "res": [{"text": "21", "confidence": 0.9931725263595581, "text_region": [[1201.0, 2967.0], [1247.0, 2967.0], [1247.0, 3010.0], [1201.0, 3010.0]]}], "img_idx": 0, "score": 0.9096555113792419}
