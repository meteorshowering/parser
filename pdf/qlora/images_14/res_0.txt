{"type": "text", "bbox": [427, 1205, 2024, 1546], "res": [{"text": "Chatbots  Many instruction following models are structured as dialogue-based chatbots, often using", "confidence": 0.9888542890548706, "text_region": [[423.0, 1188.0], [2022.0, 1195.0], [2022.0, 1251.0], [422.0, 1244.0]]}, {"text": " Reinforcement Learning from Human Feedback (RLHF) [11] or generating data from an existing", "confidence": 0.9918932318687439, "text_region": [[422.0, 1234.0], [2026.0, 1238.0], [2026.0, 1294.0], [422.0, 1290.0]]}, {"text": " model to train with AI model feedback (RLAIF) [5]. Approaches and datasets include Anthropic-", "confidence": 0.9860635995864868, "text_region": [[419.0, 1277.0], [2026.0, 1280.0], [2026.0, 1337.0], [419.0, 1333.0]]}, {"text": "HH [2, 4], Open Assistant [31], LaMDA [56], and Sparrow [21]. We do not use reinforcement", "confidence": 0.9872474074363708, "text_region": [[426.0, 1330.0], [2022.0, 1330.0], [2022.0, 1376.0], [426.0, 1376.0]]}, {"text": "learning, but our best model, Guanaco, is finetuned on multi-turn chat interactions from the Open", "confidence": 0.9878652691841125, "text_region": [[426.0, 1369.0], [2022.0, 1373.0], [2022.0, 1419.0], [426.0, 1416.0]]}, {"text": "Assistant dataset which was designed to be used for RLHF training [31]. For the evaluation of", "confidence": 0.9913238286972046, "text_region": [[429.0, 1416.0], [2022.0, 1416.0], [2022.0, 1462.0], [429.0, 1462.0]]}, {"text": "chatbots approaches that use GPT-4 instead of costly human annotation have been developed [10, 45].", "confidence": 0.9905727505683899, "text_region": [[426.0, 1455.0], [2026.0, 1455.0], [2026.0, 1511.0], [426.0, 1511.0]]}, {"text": " We improve on such approaches with a focus on an evaluation setup that is more reliable.", "confidence": 0.9842457175254822, "text_region": [[419.0, 1495.0], [1859.0, 1498.0], [1859.0, 1554.0], [419.0, 1551.0]]}], "img_idx": 0, "score": 0.9716542959213257}
{"type": "text", "bbox": [432, 1036, 2015, 1161], "res": [{"text": "MetaTuning [73], InstructGPT [43], FLAN [62, 12], PromptSource [3], Super-NaturalInstructions [61,", "confidence": 0.9908775091171265, "text_region": [[429.0, 1033.0], [2022.0, 1033.0], [2022.0, 1079.0], [429.0, 1079.0]]}, {"text": "50], Self-instruct [59], UnnaturalInstructions [26], OPT-IML [29], UnifiedSKG[67], OIG/Chip2 [32],", "confidence": 0.9981823563575745, "text_region": [[432.0, 1079.0], [2022.0, 1079.0], [2022.0, 1122.0], [432.0, 1122.0]]}, {"text": "Alpaca [55], Vicuna [10], Koala [20], and Self-instruct-GPT-4 [45].", "confidence": 0.9963799118995667, "text_region": [[429.0, 1122.0], [1510.0, 1122.0], [1510.0, 1168.0], [429.0, 1168.0]]}], "img_idx": 0, "score": 0.9046528935432434}
{"type": "text", "bbox": [429, 1681, 2018, 2521], "res": [{"text": "We have shown evidence that our method, QLoRA, can replicate 16-bit full finetuning performance", "confidence": 0.997458279132843, "text_region": [[422.0, 1670.0], [2019.0, 1673.0], [2019.0, 1723.0], [422.0, 1719.0]]}, {"text": "with a 4-bit base model and Low-rank Adapters (LoRA). Despite this evidence, we did not establish", "confidence": 0.9875808358192444, "text_region": [[429.0, 1719.0], [2022.0, 1719.0], [2022.0, 1766.0], [429.0, 1766.0]]}, {"text": "that QLoRA can match full 16-bit finetuning performance at 33B and 65B scales. Due to the", "confidence": 0.9838534593582153, "text_region": [[422.0, 1759.0], [2019.0, 1759.0], [2019.0, 1805.0], [422.0, 1805.0]]}, {"text": "immense resource costs, we leave this study to future work.", "confidence": 0.9924737811088562, "text_region": [[426.0, 1808.0], [1380.0, 1808.0], [1380.0, 1851.0], [426.0, 1851.0]]}, {"text": "Another limitation is the evaluation of instruction finetuning models. While we provide evaluations", "confidence": 0.9946336150169373, "text_region": [[429.0, 1878.0], [2019.0, 1878.0], [2019.0, 1924.0], [429.0, 1924.0]]}, {"text": "on MMLU, the Vicuna benchmark, and the OA benchmark, we did not evaluate on other benchmarks", "confidence": 0.9987983703613281, "text_region": [[426.0, 1917.0], [2016.0, 1917.0], [2016.0, 1960.0], [426.0, 1960.0]]}, {"text": "such as BigBench, RAFT, and HELM, and it is not ensured that our evaluations generalize to these", "confidence": 0.9940070509910583, "text_region": [[429.0, 1967.0], [2019.0, 1967.0], [2019.0, 2013.0], [429.0, 2013.0]]}, {"text": "benchmarks. On the other hand, we perform a very broad study on MMLU and develop new methods ", "confidence": 0.9915814995765686, "text_region": [[422.0, 2003.0], [2026.0, 2003.0], [2026.0, 2059.0], [422.0, 2059.0]]}, {"text": "for evaluating chatbots.", "confidence": 0.9798585772514343, "text_region": [[429.0, 2053.0], [805.0, 2053.0], [805.0, 2099.0], [429.0, 2099.0]]}, {"text": "From the evidence presented, it appears that the performance of these benchmarks likely depends how", "confidence": 0.9912002086639404, "text_region": [[429.0, 2122.0], [2019.0, 2122.0], [2019.0, 2168.0], [429.0, 2168.0]]}, {"text": "similar the finetuning data is to the benchmark dataset. For example, FLAN v2 is similar to MMLU,", "confidence": 0.9919123649597168, "text_region": [[429.0, 2165.0], [2022.0, 2165.0], [2022.0, 2211.0], [429.0, 2211.0]]}, {"text": "but dissimilar to chatbot benchmarks and vice versa for the Chip2 dataset and both models score", "confidence": 0.9983180165290833, "text_region": [[429.0, 2211.0], [2019.0, 2211.0], [2019.0, 2254.0], [429.0, 2254.0]]}, {"text": "accordingly on the MMLU and Vicuna benchmarks. This highlights that not only better benchmarks", "confidence": 0.999018132686615, "text_region": [[426.0, 2254.0], [2019.0, 2254.0], [2019.0, 2300.0], [426.0, 2300.0]]}, {"text": "and evaluation is needed, but that one needs to be careful about what one is evaluating in the first", "confidence": 0.9927780032157898, "text_region": [[426.0, 2297.0], [2022.0, 2297.0], [2022.0, 2340.0], [426.0, 2340.0]]}, {"text": "place. Do we want to create models that do well on classroom highschool and colleague knowledge or", "confidence": 0.9885520339012146, "text_region": [[426.0, 2340.0], [2019.0, 2340.0], [2019.0, 2386.0], [426.0, 2386.0]]}, {"text": " do we want to do well on chatbot conversation ability? Maybe something else? Because it is always", "confidence": 0.982606828212738, "text_region": [[422.0, 2379.0], [2022.0, 2379.0], [2022.0, 2435.0], [422.0, 2435.0]]}, {"text": "easier to evaluate on an existing benchmark compared to creating a new one, certain benchmarks", "confidence": 0.9968193769454956, "text_region": [[429.0, 2429.0], [2019.0, 2429.0], [2019.0, 2475.0], [429.0, 2475.0]]}, {"text": "can steer the community towards a certain direction. We should ensure as a community that the", "confidence": 0.9873685836791992, "text_region": [[429.0, 2472.0], [2022.0, 2472.0], [2022.0, 2518.0], [429.0, 2518.0]]}, {"text": " benchmarks measure what we care about.", "confidence": 0.9878555536270142, "text_region": [[422.0, 2511.0], [1094.0, 2511.0], [1094.0, 2557.0], [422.0, 2557.0]]}], "img_idx": 0, "score": 0.8969840407371521}
{"type": "text", "bbox": [429, 2589, 2019, 2886], "res": [{"text": "While we provide a detailed evaluation for general chatbot performance, another limitation is that we", "confidence": 0.9871000647544861, "text_region": [[422.0, 2581.0], [2019.0, 2581.0], [2019.0, 2637.0], [422.0, 2637.0]]}, {"text": "only do a limited responsible AI evaluation of Guanaco. We evaluate the likelihood of Guanaco-65B", "confidence": 0.9885413646697998, "text_region": [[429.0, 2630.0], [2019.0, 2630.0], [2019.0, 2673.0], [429.0, 2673.0]]}, {"text": "to generate a socially biased sequence of tokens compared to other models in Table 8. We see that the", "confidence": 0.9889457821846008, "text_region": [[426.0, 2673.0], [2019.0, 2673.0], [2019.0, 2719.0], [426.0, 2719.0]]}, {"text": "average score in Guanaco-65B is much lower than other raw pretrained models. As such, it seems that", "confidence": 0.9869802594184875, "text_region": [[426.0, 2719.0], [2022.0, 2713.0], [2022.0, 2759.0], [426.0, 2765.0]]}, {"text": "finetuning on the OASST1 dataset reduces the bias of the LLaMA base model. While these results", "confidence": 0.989402174949646, "text_region": [[426.0, 2756.0], [2022.0, 2756.0], [2022.0, 2812.0], [426.0, 2812.0]]}, {"text": " are encouraging, it is unclear if Guanaco does also well when assessed on other types of biases. We", "confidence": 0.9758247137069702, "text_region": [[422.0, 2802.0], [2025.0, 2795.0], [2026.0, 2851.0], [423.0, 2858.0]]}, {"text": "leave further evaluation of analyzing biases in Guanaco and similar chatbots to future work.", "confidence": 0.9994632005691528, "text_region": [[429.0, 2851.0], [1893.0, 2851.0], [1893.0, 2894.0], [429.0, 2894.0]]}], "img_idx": 0, "score": 0.7170712947845459}
{"type": "title", "bbox": [431, 1599, 1056, 1638], "res": [{"text": "8", "confidence": 0.9997722506523132, "text_region": [[432.0, 1604.0], [466.0, 1604.0], [466.0, 1634.0], [432.0, 1634.0]]}, {"text": "Limitations and Discussion", "confidence": 0.9958021640777588, "text_region": [[452.0, 1597.0], [1061.0, 1597.0], [1061.0, 1643.0], [452.0, 1643.0]]}], "img_idx": 0, "score": 0.9508577585220337}
{"type": "table", "bbox": [539, 398, 1902, 956], "res": {"cell_bbox": [[114.84103393554688, 23.316497802734375, 411.6954345703125, 23.653854370117188, 409.3647155761719, 51.039459228515625, 111.53137969970703, 50.48317337036133], [454.87347412109375, 18.03365707397461, 676.9096069335938, 17.915279388427734, 678.7151489257812, 52.88933563232422, 455.27593994140625, 53.47879409790039], [720.1778564453125, 17.992277145385742, 909.921630859375, 17.901264190673828, 913.6093139648438, 54.82301330566406, 722.5008544921875, 55.258209228515625], [974.2831420898438, 20.34258270263672, 1185.0875244140625, 20.264909744262695, 1186.656494140625, 55.28847122192383, 977.1766967773438, 55.54826354980469], [43.570831298828125, 78.07955932617188, 264.51922607421875, 77.85022735595703, 265.5888977050781, 118.31461334228516, 43.0645866394043, 118.33815002441406], [504.78546142578125, 77.29078674316406, 637.4271850585938, 76.82252502441406, 642.593017578125, 112.6941146850586, 506.08123779296875, 113.30982208251953], [802.5667724609375, 74.01986694335938, 926.0925903320312, 74.16691589355469, 929.443603515625, 110.28680419921875, 805.2046508789062, 110.43797302246094], [1087.4166259765625, 73.70317840576172, 1287.3173828125, 73.52421569824219, 1286.87646484375, 111.11076354980469, 1086.99609375, 111.85379791259766], [56.42198181152344, 122.02778625488281, 283.1688537597656, 121.4723129272461, 283.62005615234375, 155.0627899169922, 56.5462532043457, 155.90916442871094], [497.79571533203125, 121.92575073242188, 637.0993041992188, 120.84391021728516, 640.6168823242188, 154.6174774169922, 498.63348388671875, 155.96151733398438], [793.3994750976562, 120.28019714355469, 909.88525390625, 119.95479583740234, 911.667236328125, 152.57920837402344, 794.7764282226562, 153.3531951904297], [1088.1446533203125, 119.94880676269531, 1267.181884765625, 119.43106079101562, 1266.4759521484375, 154.47274780273438, 1087.109130859375, 155.39190673828125], [53.757938385009766, 164.70826721191406, 305.09130859375, 164.41807556152344, 304.7646484375, 198.6942901611328, 53.624916076660156, 199.23609924316406], [505.78839111328125, 164.43069458007812, 642.6389770507812, 163.63735961914062, 645.1542358398438, 198.71730041503906, 505.7569885253906, 199.7629852294922], [796.4446411132812, 162.92820739746094, 909.948486328125, 162.6482696533203, 911.3698120117188, 196.14791870117188, 797.1124877929688, 196.92286682128906], [1095.763427734375, 162.5102081298828, 1266.9710693359375, 162.0847625732422, 1266.2569580078125, 197.4737091064453, 1094.7374267578125, 198.31570434570312], [46.69587707519531, 206.69949340820312, 361.2265930175781, 206.5902099609375, 360.75933837890625, 242.85853576660156, 46.440364837646484, 243.2425079345703], [508.2032775878906, 206.31227111816406, 635.8067016601562, 205.83847045898438, 637.6262817382812, 241.44296264648438, 507.6376953125, 242.1495819091797], [772.0625, 205.1106414794922, 886.3389282226562, 204.6971893310547, 887.6029052734375, 238.56857299804688, 772.4492797851562, 239.45333862304688], [1057.9451904296875, 204.9376220703125, 1225.9073486328125, 204.42979431152344, 1225.4224853515625, 239.88671875, 1057.310546875, 240.81124877929688], [51.45322036743164, 249.89028930664062, 290.0458068847656, 250.02366638183594, 289.5080261230469, 287.4447021484375, 51.005619049072266, 287.6121826171875], [507.38189697265625, 248.6466827392578, 642.005126953125, 247.97882080078125, 643.3029174804688, 283.755126953125, 506.50262451171875, 284.7100830078125], [783.3552856445312, 247.4179229736328, 895.3583374023438, 246.94244384765625, 896.5953369140625, 281.6612854003906, 783.664794921875, 282.62860107421875], [1081.3040771484375, 247.4921112060547, 1241.9752197265625, 246.90509033203125, 1241.3409423828125, 282.9384460449219, 1080.4473876953125, 283.93084716796875], [57.997215270996094, 295.9952697753906, 303.5904541015625, 296.13623046875, 303.30029296875, 331.9532470703125, 57.637413024902344, 331.9896240234375], [510.7496643066406, 294.0812683105469, 633.8445434570312, 293.8428955078125, 635.7196655273438, 329.68475341796875, 510.5736999511719, 330.1016540527344], [772.166748046875, 292.80377197265625, 887.7302856445312, 292.58367919921875, 889.1331787109375, 327.4168395996094, 772.9641723632812, 327.93853759765625], [1069.7275390625, 293.2619323730469, 1219.1470947265625, 292.9158020019531, 1218.740966796875, 329.0374450683594, 1069.475830078125, 329.6824645996094], [60.711669921875, 339.58319091796875, 269.6922912597656, 339.8759765625, 269.2073059082031, 375.9217834472656, 60.22151184082031, 375.7292785644531], [508.06396484375, 338.3665771484375, 633.8546142578125, 338.30096435546875, 635.2225341796875, 373.57373046875, 507.4263000488281, 373.8955993652344], [776.5556030273438, 337.1670227050781, 891.5956420898438, 337.069580078125, 892.6630249023438, 371.81195068359375, 776.8560180664062, 372.2156982421875], [1086.3416748046875, 338.25946044921875, 1237.0428466796875, 338.0657958984375, 1236.4964599609375, 373.3199462890625, 1085.8580322265625, 373.81988525390625], [62.93230056762695, 381.58544921875, 378.4196472167969, 382.1132507324219, 378.55096435546875, 420.90692138671875, 62.541221618652344, 420.6597595214844], [500.60540771484375, 380.6015625, 622.4598999023438, 380.80096435546875, 623.7205200195312, 418.235107421875, 500.2209777832031, 418.21044921875], [761.6494140625, 380.03741455078125, 886.8719482421875, 380.01556396484375, 888.3216552734375, 417.1669616699219, 762.08056640625, 417.434326171875], [1088.1951904296875, 381.00537109375, 1232.7059326171875, 380.9539794921875, 1232.4608154296875, 417.97308349609375, 1088.0341796875, 418.2807922363281], [55.19944381713867, 424.96160888671875, 384.4549560546875, 425.5992126464844, 385.8498840332031, 466.0981750488281, 54.84451675415039, 465.6468505859375], [501.00592041015625, 425.8018798828125, 626.425048828125, 426.1053466796875, 628.0590209960938, 463.4980163574219, 500.39068603515625, 463.4125061035156], [756.1289672851562, 425.4309387207031, 887.1815185546875, 425.4524230957031, 889.3923950195312, 462.6325988769531, 756.7154541015625, 462.8267822265625], [1095.7799072265625, 426.3512878417969, 1237.0797119140625, 426.3316345214844, 1237.52734375, 462.75927734375, 1096.679931640625, 462.97320556640625], [62.872596740722656, 492.13104248046875, 358.25518798828125, 492.23248291015625, 360.0085754394531, 532.9408569335938, 62.874916076660156, 532.7362060546875], [514.22119140625, 493.16571044921875, 633.9879150390625, 493.6622009277344, 637.182861328125, 529.53173828125, 515.9275512695312, 529.2529296875], [769.5175170898438, 491.622314453125, 912.2149047851562, 491.6082763671875, 915.3104858398438, 528.5115966796875, 771.00439453125, 528.5433349609375], [1115.91259765625, 494.1022644042969, 1247.01123046875, 494.4999694824219, 1248.783203125, 529.42724609375, 1119.37939453125, 529.3126220703125]], "html": "<html><body><table><thead><tr><td></td><td>LLaMA-65B</td><td>GPT-3</td><td>OPT-175B Guanaco-65B</td></tr></thead><tbody><tr><td>Gender</td><td>70.6</td><td>62.6 65.7</td><td>47.5</td></tr><tr><td>Religion</td><td>79.0</td><td>73.3 68.6</td><td>38.7</td></tr><tr><td>Race/Color</td><td>57.0</td><td>64.7 68.6</td><td>45.3</td></tr><tr><td>Sexual orientation</td><td>81.0</td><td>76.2 78.6</td><td>59.1</td></tr><tr><td>Age</td><td>70.1</td><td>64.4 67.8</td><td>36.3</td></tr><tr><td>Nationality</td><td>64.2</td><td>61.6 62.9</td><td>32.4</td></tr><tr><td>Disability</td><td>66.7</td><td>76.7 76.7</td><td>33.9</td></tr><tr><td>Physical appearance</td><td>77.8</td><td>74.6 76.2</td><td>43.1</td></tr><tr><td>Socioeconomic status</td><td>71.5</td><td>73.8 76.2</td><td>55.3</td></tr><tr><td>Average</td><td>66.6</td><td>67.2 69.5</td><td>43.5</td></tr></tbody></table></body></html>"}, "img_idx": 0, "score": 0.9772371649742126}
{"type": "table_caption", "bbox": [443, 317, 2007, 390], "res": [{"text": "Table 8: Evaluation of biases on the CrowS dataset. A lower score indicates lower likelihood of generating", "confidence": 0.9935423135757446, "text_region": [[426.0, 307.0], [2022.0, 314.0], [2022.0, 360.0], [426.0, 353.0]]}, {"text": " biased sequences. Guanaco follows the biased pattern of the LLaMA base model.", "confidence": 0.9958240985870361, "text_region": [[422.0, 353.0], [1600.0, 353.0], [1600.0, 396.0], [422.0, 396.0]]}], "img_idx": 0, "score": 0.9314452409744263}
{"type": "footer", "bbox": [1207, 2974, 1243, 3001], "res": [{"text": "15", "confidence": 0.9999122619628906, "text_region": [[1201.0, 2967.0], [1254.0, 2967.0], [1254.0, 3010.0], [1201.0, 3010.0]]}], "img_idx": 0, "score": 0.8925583362579346}
