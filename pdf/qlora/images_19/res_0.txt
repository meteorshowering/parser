{"type": "footer", "bbox": [1201, 2973, 1245, 3001], "res": [{"text": "20", "confidence": 0.9998712539672852, "text_region": [[1201.0, 2970.0], [1254.0, 2970.0], [1254.0, 3013.0], [1201.0, 3013.0]]}], "img_idx": 0, "score": 0.9124787449836731}
{"type": "reference", "bbox": [435, 297, 2022, 2890], "res": [{"text": "[55] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto.", "confidence": 0.9896447658538818, "text_region": [[429.0, 297.0], [2019.0, 297.0], [2019.0, 343.0], [429.0, 343.0]]}, {"text": "Stanford alpaca: An instruction-following llama model. https : //github. com/tat su-lab/", "confidence": 0.9823369383811951, "text_region": [[516.0, 343.0], [2016.0, 343.0], [2016.0, 389.0], [516.0, 389.0]]}, {"text": "stanford_alpaca, 2023.", "confidence": 0.9976717233657837, "text_region": [[519.0, 389.0], [941.0, 389.0], [941.0, 432.0], [519.0, 432.0]]}, {"text": "[56] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos,", "confidence": 0.9840965270996094, "text_region": [[432.0, 465.0], [2019.0, 465.0], [2019.0, 508.0], [432.0, 508.0]]}, {"text": "L. Baker, Y. Du, et al. Lamda: Language models for dialog applications. arXiv preprint", "confidence": 0.9957116842269897, "text_region": [[506.0, 498.0], [2022.0, 502.0], [2022.0, 558.0], [506.0, 554.0]]}, {"text": "arXiv:2201.08239, 2022.", "confidence": 0.9997361302375793, "text_region": [[516.0, 548.0], [918.0, 551.0], [918.0, 597.0], [515.0, 594.0]]}, {"text": "[57] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal", "confidence": 0.9967404007911682, "text_region": [[426.0, 624.0], [2016.0, 627.0], [2016.0, 673.0], [426.0, 670.0]]}, {"text": " E. Hambro, F. Azhar, et al. Llama: Open and effcient foundation language models. arXiv", "confidence": 0.9850343465805054, "text_region": [[506.0, 663.0], [2022.0, 667.0], [2022.0, 723.0], [506.0, 719.0]]}, {"text": "preprint arXiv:2302.13971, 2023.", "confidence": 0.9984968900680542, "text_region": [[512.0, 716.0], [1058.0, 713.0], [1058.0, 759.0], [512.0, 762.0]]}, {"text": "[58] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi", "confidence": 0.9878350496292114, "text_region": [[426.0, 789.0], [2016.0, 792.0], [2016.0, 838.0], [426.0, 835.0]]}, {"text": "task benchmark and analysis platform for natural language understanding. arXiv preprint", "confidence": 0.9911945462226868, "text_region": [[519.0, 838.0], [2019.0, 838.0], [2019.0, 884.0], [519.0, 884.0]]}, {"text": "arXiv:1804.07461, 2018.", "confidence": 0.9982600212097168, "text_region": [[512.0, 878.0], [915.0, 878.0], [915.0, 921.0], [512.0, 921.0]]}, {"text": "[59] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct:", "confidence": 0.9818329811096191, "text_region": [[426.0, 954.0], [2019.0, 957.0], [2019.0, 1003.0], [426.0, 1000.0]]}, {"text": "Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560,", "confidence": 0.9988147020339966, "text_region": [[519.0, 1003.0], [2019.0, 1003.0], [2019.0, 1049.0], [519.0, 1049.0]]}, {"text": "2022.", "confidence": 0.9997924566268921, "text_region": [[516.0, 1049.0], [609.0, 1049.0], [609.0, 1092.0], [516.0, 1092.0]]}, {"text": "[60] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S.", "confidence": 0.9916172027587891, "text_region": [[429.0, 1122.0], [2019.0, 1122.0], [2019.0, 1168.0], [429.0, 1168.0]]}, {"text": "Dhanasekaran, A. Naik, D. Stap, et al. Super-naturalinstructions:generalization via declarative", "confidence": 0.9875655174255371, "text_region": [[512.0, 1158.0], [2022.0, 1162.0], [2022.0, 1218.0], [512.0, 1214.0]]}, {"text": "instructions on 1600+ tasks. In EMNLP, 2022.", "confidence": 0.9918890595436096, "text_region": [[519.0, 1214.0], [1261.0, 1214.0], [1261.0, 1257.0], [519.0, 1257.0]]}, {"text": "[61] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S.", "confidence": 0.9930422306060791, "text_region": [[429.0, 1287.0], [2019.0, 1287.0], [2019.0, 1333.0], [429.0, 1333.0]]}, {"text": "Dhanasekaran, A. Arunkumar, D. Stap, et al. Super-naturalinstructions: Generalization via", "confidence": 0.9904881119728088, "text_region": [[516.0, 1333.0], [2012.0, 1333.0], [2012.0, 1379.0], [516.0, 1379.0]]}, {"text": "declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical", "confidence": 0.9905916452407837, "text_region": [[519.0, 1376.0], [2019.0, 1376.0], [2019.0, 1422.0], [519.0, 1422.0]]}, {"text": "Methods in Natural Language Processing, pages 5085-5109, 2022.", "confidence": 0.9984711408615112, "text_region": [[509.0, 1412.0], [1593.0, 1416.0], [1593.0, 1472.0], [509.0, 1468.0]]}, {"text": "[62] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le.", "confidence": 0.9905132055282593, "text_region": [[429.0, 1495.0], [2019.0, 1495.0], [2019.0, 1541.0], [429.0, 1541.0]]}, {"text": "Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.", "confidence": 0.993985116481781, "text_region": [[512.0, 1541.0], [1972.0, 1538.0], [1972.0, 1584.0], [512.0, 1587.0]]}, {"text": "[63] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V. Le, D. Zhou, et al.", "confidence": 0.9762052893638611, "text_region": [[426.0, 1614.0], [2019.0, 1617.0], [2019.0, 1663.0], [426.0, 1660.0]]}, {"text": "Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural", "confidence": 0.9861458539962769, "text_region": [[519.0, 1663.0], [2019.0, 1663.0], [2019.0, 1709.0], [519.0, 1709.0]]}, {"text": "Information Processing Systems, 2022.", "confidence": 0.9846612215042114, "text_region": [[516.0, 1709.0], [1138.0, 1709.0], [1138.0, 1752.0], [516.0, 1752.0]]}, {"text": "[64] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,", "confidence": 0.9837316870689392, "text_region": [[422.0, 1775.0], [2022.0, 1779.0], [2022.0, 1835.0], [422.0, 1831.0]]}, {"text": "M. Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing.", "confidence": 0.9966741800308228, "text_region": [[509.0, 1818.0], [2022.0, 1825.0], [2022.0, 1881.0], [509.0, 1874.0]]}, {"text": "arXiv preprint arXiv:1910.03771, 2019.", "confidence": 0.995185375213623, "text_region": [[519.0, 1874.0], [1157.0, 1874.0], [1157.0, 1917.0], [519.0, 1917.0]]}, {"text": "[65] M. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt. Stable and", "confidence": 0.9880499839782715, "text_region": [[432.0, 1950.0], [2016.0, 1950.0], [2016.0, 1993.0], [432.0, 1993.0]]}, {"text": "low-precision training for large-scale vision-language models. arXiv preprint arXiv:2304.13013,", "confidence": 0.9968141317367554, "text_region": [[516.0, 1990.0], [2022.0, 1990.0], [2022.0, 2046.0], [516.0, 2046.0]]}, {"text": "2023.", "confidence": 0.9998002052307129, "text_region": [[519.0, 2039.0], [612.0, 2039.0], [612.0, 2082.0], [519.0, 2082.0]]}, {"text": "[66] G. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han. Smoothquant: Accurate and effcient", "confidence": 0.9835475087165833, "text_region": [[422.0, 2105.0], [2019.0, 2109.0], [2019.0, 2165.0], [422.0, 2161.0]]}, {"text": " post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022.", "confidence": 0.9816566109657288, "text_region": [[509.0, 2155.0], [2019.0, 2152.0], [2019.0, 2208.0], [509.0, 2211.0]]}, {"text": "[67] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu, M. Zhong, P. Yin,", "confidence": 0.984437108039856, "text_region": [[429.0, 2237.0], [2019.0, 2237.0], [2019.0, 2284.0], [429.0, 2284.0]]}, {"text": "S. 1. Wang, et al. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with", "confidence": 0.9840236902236938, "text_region": [[509.0, 2270.0], [2022.0, 2274.0], [2022.0, 2330.0], [509.0, 2326.0]]}, {"text": "text-to-text language models. arXiv preprint arXiv:2201.05966, 2022.", "confidence": 0.9945944547653198, "text_region": [[519.0, 2323.0], [1633.0, 2323.0], [1633.0, 2369.0], [519.0, 2369.0]]}, {"text": "[68] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. Cohen, R. Salakhutdinov, and C. D. Manning. Hotpotqa:", "confidence": 0.9834590554237366, "text_region": [[426.0, 2396.0], [2019.0, 2402.0], [2019.0, 2449.0], [426.0, 2442.0]]}, {"text": "A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018", "confidence": 0.9941365122795105, "text_region": [[512.0, 2439.0], [2022.0, 2435.0], [2022.0, 2491.0], [512.0, 2495.0]]}, {"text": "Conference on Empirical Methods in Natural Language Processing, pages 2369-2380, 2018.", "confidence": 0.9973021149635315, "text_region": [[516.0, 2488.0], [2002.0, 2488.0], [2002.0, 2534.0], [516.0, 2534.0]]}, {"text": "[69] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He. Zeroquant: Efficient and affordable", "confidence": 0.9763780236244202, "text_region": [[426.0, 2561.0], [2022.0, 2557.0], [2022.0, 2614.0], [426.0, 2617.0]]}, {"text": "post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.", "confidence": 0.9915251135826111, "text_region": [[512.0, 2610.0], [2019.0, 2607.0], [2019.0, 2653.0], [512.0, 2657.0]]}, {"text": "[70] E. B. Zaken, S. Ravfogel, and Y. Goldberg. Bitfit: Simple parameter-efficient fine-tuning for", "confidence": 0.9892712831497192, "text_region": [[432.0, 2686.0], [2019.0, 2686.0], [2019.0, 2732.0], [432.0, 2732.0]]}, {"text": "transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021.", "confidence": 0.9952702522277832, "text_region": [[516.0, 2732.0], [1886.0, 2732.0], [1886.0, 2779.0], [516.0, 2779.0]]}, {"text": "[71] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, et al.", "confidence": 0.989953875541687, "text_region": [[432.0, 2808.0], [2022.0, 2808.0], [2022.0, 2851.0], [432.0, 2851.0]]}, {"text": "Glm-130b: An 0pen bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022", "confidence": 0.9878345727920532, "text_region": [[516.0, 2854.0], [1942.0, 2854.0], [1942.0, 2897.0], [516.0, 2897.0]]}], "img_idx": 0, "score": 0.9850230813026428}
