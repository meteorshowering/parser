{"type": "text", "bbox": [427, 1639, 2020, 1894], "res": [{"text": "We use the same preprocessing of the Super-Natural Instruction dataset as Wang et al. [60]. However,", "confidence": 0.9903202056884766, "text_region": [[426.0, 1637.0], [2022.0, 1637.0], [2022.0, 1693.0], [426.0, 1693.0]]}, {"text": "we split the training data in training and validation datasets allowing us to perform more rigorous", "confidence": 0.9833024144172668, "text_region": [[426.0, 1676.0], [2019.0, 1676.0], [2019.0, 1732.0], [426.0, 1732.0]]}, {"text": "hyperparameter tuning and early stopping. We use the same hyperparameters described in the paper", "confidence": 0.9938618540763855, "text_region": [[429.0, 1729.0], [2019.0, 1729.0], [2019.0, 1775.0], [429.0, 1775.0]]}, {"text": "for training the various T5 model sizes on the Super-Natural Instruction data. We use LoRA r = 16", "confidence": 0.9936614632606506, "text_region": [[429.0, 1772.0], [2019.0, 1772.0], [2019.0, 1818.0], [429.0, 1818.0]]}, {"text": "for small, medium, and large T5 models and LoRA r = 64 for T5 xl and xxl models. We also use", "confidence": 0.984734058380127, "text_region": [[429.0, 1815.0], [2019.0, 1815.0], [2019.0, 1861.0], [429.0, 1861.0]]}, {"text": "LoRA \u03b1 = 64 in all our experiments and no LoRA dropout.", "confidence": 0.9828208684921265, "text_region": [[423.0, 1851.0], [1394.0, 1855.0], [1394.0, 1911.0], [422.0, 1907.0]]}], "img_idx": 0, "score": 0.9739720821380615}
{"type": "text", "bbox": [432, 433, 2027, 596], "res": [{"text": "We do a hyperparameter search for LoRA over the following variables: LoRA dropout  0.0, 0.05,", "confidence": 0.9765012264251709, "text_region": [[426.0, 429.0], [2022.0, 429.0], [2022.0, 485.0], [426.0, 485.0]]}, {"text": "0.1], LoRA r { 8, 16, 32, 64, 128, 256], LoRA layers {(key+query, all attention layers, all FFN layers,", "confidence": 0.9672515392303467, "text_region": [[422.0, 469.0], [2022.0, 472.0], [2022.0, 528.0], [422.0, 525.0]]}, {"text": "all layers, attention + FFN output layers}. We keep LoRA o fixed and search the learning rate, since", "confidence": 0.9711350798606873, "text_region": [[429.0, 521.0], [2019.0, 521.0], [2019.0, 568.0], [429.0, 568.0]]}, {"text": "LoRA \u03b1 is always proportional to the learning rate.", "confidence": 0.9881890416145325, "text_region": [[423.0, 554.0], [1254.0, 561.0], [1254.0, 617.0], [422.0, 610.0]]}], "img_idx": 0, "score": 0.9441826343536377}
{"type": "text", "bbox": [424, 2171, 2018, 2886], "res": [{"text": "OASST1  The OpenAssistant dataset [31] was collected via crowd-sourcing. It contains 161,443", "confidence": 0.9869107604026794, "text_region": [[429.0, 2171.0], [2019.0, 2171.0], [2019.0, 2218.0], [429.0, 2218.0]]}, {"text": "unique messages distributed across 66,497 conversations and spanning 35 different languages. The", "confidence": 0.9846170544624329, "text_region": [[426.0, 2208.0], [2022.0, 2211.0], [2022.0, 2267.0], [426.0, 2264.0]]}, {"text": "dataset often contains several ranked replies for each given user question. In our experiments, we", "confidence": 0.993706464767456, "text_region": [[426.0, 2254.0], [2019.0, 2257.0], [2019.0, 2303.0], [426.0, 2300.0]]}, {"text": "only use the top reply at each level in the conversation tree. This limits the dataset to 9,209 examples.", "confidence": 0.9889463782310486, "text_region": [[426.0, 2300.0], [2019.0, 2300.0], [2019.0, 2346.0], [426.0, 2346.0]]}, {"text": "We finetuning our models on the full conversation including the user queries.", "confidence": 0.9857053160667419, "text_region": [[423.0, 2340.0], [1666.0, 2343.0], [1666.0, 2399.0], [422.0, 2396.0]]}, {"text": "HH-RLHF  This is a human preference dataset about helpfulness and harmlessness. Each datapoint", "confidence": 0.9863359332084656, "text_region": [[429.0, 2422.0], [2019.0, 2422.0], [2019.0, 2468.0], [429.0, 2468.0]]}, {"text": "consists of two assistant replies to a user question along with a human preference judgment of the", "confidence": 0.9898394346237183, "text_region": [[432.0, 2468.0], [2019.0, 2468.0], [2019.0, 2515.0], [432.0, 2515.0]]}, {"text": "best reply. The dataset contains 160,800 examples. When finetuning on this dataset, we combine", "confidence": 0.9952278137207031, "text_region": [[429.0, 2508.0], [2019.0, 2508.0], [2019.0, 2554.0], [429.0, 2554.0]]}, {"text": "helpfulness and harmlessness data and only keep the preferred assistant reply.", "confidence": 0.9885905385017395, "text_region": [[423.0, 2548.0], [1673.0, 2551.0], [1673.0, 2607.0], [422.0, 2604.0]]}, {"text": "FLAN v2  The FLAN v2 collection [39] is a collection of 1836 tasks augmented with hundreds", "confidence": 0.9803139567375183, "text_region": [[426.0, 2627.0], [2019.0, 2630.0], [2019.0, 2676.0], [426.0, 2673.0]]}, {"text": "of manually curated templates and rich formatting patterns into over 15M examples. The authors", "confidence": 0.9937198758125305, "text_region": [[429.0, 2676.0], [2019.0, 2676.0], [2019.0, 2722.0], [429.0, 2722.0]]}, {"text": "show that models trained on this collection outperform other public collections including the original", "confidence": 0.9969854354858398, "text_region": [[429.0, 2719.0], [2019.0, 2719.0], [2019.0, 2762.0], [429.0, 2762.0]]}, {"text": "FLAN 2021 [62], T0++ [50], Super-Natural Instructions [60], and OPT-IML [29]. We used the", "confidence": 0.9904957413673401, "text_region": [[429.0, 2762.0], [2019.0, 2762.0], [2019.0, 2808.0], [429.0, 2808.0]]}, {"text": " same task mixtures described by the authors with the exception of some datasets that were not freely", "confidence": 0.975275456905365, "text_region": [[422.0, 2798.0], [2022.0, 2802.0], [2022.0, 2858.0], [422.0, 2854.0]]}, {"text": "available at the time of writing.", "confidence": 0.9948842525482178, "text_region": [[426.0, 2848.0], [928.0, 2855.0], [928.0, 2901.0], [425.0, 2894.0]]}], "img_idx": 0, "score": 0.938698410987854}
{"type": "text", "bbox": [427, 638, 2017, 767], "res": [{"text": "We find that LoRA dropout 0.05 is useful for small models (7B, 13B), but not for larger models (33B.", "confidence": 0.9900469183921814, "text_region": [[429.0, 637.0], [2016.0, 637.0], [2016.0, 683.0], [429.0, 683.0]]}, {"text": "65B). We find LoRA r is unrelated to final performance if LoRA is used on all layers as can be seen", "confidence": 0.9865724444389343, "text_region": [[426.0, 676.0], [2019.0, 680.0], [2019.0, 726.0], [426.0, 723.0]]}, {"text": "in Figure 4", "confidence": 0.9997472167015076, "text_region": [[426.0, 723.0], [612.0, 723.0], [612.0, 769.0], [426.0, 769.0]]}], "img_idx": 0, "score": 0.8784506320953369}
{"type": "text", "bbox": [614, 2093, 1511, 2132], "res": [{"text": "We describe the datasets used for QLoRA finetuning experiments outlined in Section 5.", "confidence": 0.9915001392364502, "text_region": [[422.0, 2089.0], [1843.0, 2089.0], [1843.0, 2145.0], [422.0, 2145.0]]}], "img_idx": 0, "score": 0.5175125002861023}
{"type": "title", "bbox": [433, 295, 1743, 336], "res": [{"text": "A QLoRA vs Standard Finetuning Experimental Setup Details", "confidence": 0.9908624887466431, "text_region": [[423.0, 287.0], [1746.0, 290.0], [1746.0, 347.0], [422.0, 343.0]]}], "img_idx": 0, "score": 0.9357996582984924}
{"type": "title", "bbox": [436, 373, 1048, 411], "res": [{"text": "A.1Hyperparameters for QLoRA", "confidence": 0.9891180992126465, "text_region": [[423.0, 363.0], [1058.0, 366.0], [1058.0, 423.0], [422.0, 419.0]]}], "img_idx": 0, "score": 0.8281620740890503}
{"type": "title", "bbox": [429, 1955, 1807, 2007], "res": [{"text": "BTraining a State-of-the-art Chatbot Experimental Setup Details", "confidence": 0.9926156997680664, "text_region": [[429.0, 1954.0], [1806.0, 1954.0], [1806.0, 2000.0], [429.0, 2000.0]]}], "img_idx": 0, "score": 0.8267731666564941}
{"type": "title", "bbox": [429, 1582, 1468, 1619], "res": [{"text": "A.2 Super-Natural Instructions Experimental Setup Details", "confidence": 0.9759531617164612, "text_region": [[429.0, 1581.0], [1473.0, 1581.0], [1473.0, 1624.0], [429.0, 1624.0]]}], "img_idx": 0, "score": 0.8249362111091614}
{"type": "figure", "bbox": [849, 819, 1577, 1362], "res": [{"text": "55.0", "confidence": 0.8861978054046631, "text_region": [[941.0, 861.0], [971.0, 861.0], [971.0, 881.0], [941.0, 881.0]]}, {"text": "54.", "confidence": 0.941470205783844, "text_region": [[935.0, 942.0], [958.0, 931.0], [969.0, 952.0], [946.0, 963.0]]}, {"text": "LoRA r", "confidence": 0.993040144443512, "text_region": [[1187.0, 1320.0], [1271.0, 1320.0], [1271.0, 1356.0], [1187.0, 1356.0]]}], "img_idx": 0, "score": 0.9199109077453613}
{"type": "figure_caption", "bbox": [430, 1390, 2018, 1504], "res": [{"text": "Figure 4: LoRA r for LLaMA 7B models finetuned on Alpaca. Each dot represents a combination of", "confidence": 0.9858657121658325, "text_region": [[429.0, 1389.0], [2019.0, 1389.0], [2019.0, 1436.0], [429.0, 1436.0]]}, {"text": "hyperparameters and for each LoRA r we run 3 random seed with each hyperparameter combination. The", "confidence": 0.987304151058197, "text_region": [[429.0, 1429.0], [2019.0, 1429.0], [2019.0, 1475.0], [429.0, 1475.0]]}, {"text": "performance of specific LoRA r values appears to be independent of other hyperparameters.", "confidence": 0.9874903559684753, "text_region": [[426.0, 1468.0], [1753.0, 1468.0], [1753.0, 1515.0], [426.0, 1515.0]]}], "img_idx": 0, "score": 0.9460678100585938}
{"type": "footer", "bbox": [1202, 2973, 1244, 3000], "res": [{"text": "22", "confidence": 0.9998414516448975, "text_region": [[1201.0, 2967.0], [1251.0, 2967.0], [1251.0, 3013.0], [1201.0, 3013.0]]}], "img_idx": 0, "score": 0.9079484343528748}
