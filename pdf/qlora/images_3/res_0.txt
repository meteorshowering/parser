{"type": "text", "bbox": [429, 299, 2021, 819], "res": [{"text": "Parameter Efficient Finetuning (PEFT) method, most of the memory footprint for LLM finetuning", "confidence": 0.9874363541603088, "text_region": [[426.0, 294.0], [2016.0, 294.0], [2016.0, 340.0], [426.0, 340.0]]}, {"text": "comes from activation gradients and not from the learned LoRA parameters. For a 7B LLaMA", "confidence": 0.9938746094703674, "text_region": [[429.0, 340.0], [2019.0, 340.0], [2019.0, 386.0], [429.0, 386.0]]}, {"text": "model trained on FLAN v2 with a batch size of 1, with LoRA weights equivalent to commonly used", "confidence": 0.9875717163085938, "text_region": [[426.0, 379.0], [2019.0, 383.0], [2019.0, 429.0], [426.0, 426.0]]}, {"text": "0.2% of the original model weights[28, 37], the LoRA input gradients have a memory footprint", "confidence": 0.9964476227760315, "text_region": [[422.0, 422.0], [2019.0, 426.0], [2019.0, 472.0], [422.0, 469.0]]}, {"text": "of 567 MB while the LoRA parameters take up only 26 MB. With gradient checkpointing [9], the", "confidence": 0.9931999444961548, "text_region": [[432.0, 472.0], [2019.0, 472.0], [2019.0, 518.0], [432.0, 518.0]]}, {"text": "input gradients reduce to an average of 18 MB per sequence making them more memory intensive", "confidence": 0.9941895604133606, "text_region": [[429.0, 515.0], [2019.0, 515.0], [2019.0, 561.0], [429.0, 561.0]]}, {"text": "than all LoRA weights combined. In comparison, the 4-bit base model consumes 5,048 MB of", "confidence": 0.9924766421318054, "text_region": [[429.0, 558.0], [2019.0, 558.0], [2019.0, 604.0], [429.0, 604.0]]}, {"text": "memory. This highlights that gradient checkpointing is important but also that aggressively reducing", "confidence": 0.9870237708091736, "text_region": [[429.0, 604.0], [2019.0, 604.0], [2019.0, 650.0], [429.0, 650.0]]}, {"text": "the amount of LoRA parameter yields only minor memory benefits. This means we can use more", "confidence": 0.9895980954170227, "text_region": [[426.0, 647.0], [2019.0, 647.0], [2019.0, 693.0], [426.0, 693.0]]}, {"text": "adapters without significantly increasing the overall training memory footprint (see Appendix G", "confidence": 0.9904406666755676, "text_region": [[426.0, 686.0], [2022.0, 686.0], [2022.0, 742.0], [426.0, 742.0]]}, {"text": "for a detailed breakdown). As discussed later, this is crucial for recovering full 16-bit precision", "confidence": 0.9967208504676819, "text_region": [[426.0, 733.0], [2019.0, 733.0], [2019.0, 779.0], [426.0, 779.0]]}, {"text": "performance.", "confidence": 0.9997353553771973, "text_region": [[422.0, 779.0], [644.0, 772.0], [646.0, 818.0], [423.0, 826.0]]}], "img_idx": 0, "score": 0.9921404719352722}
{"type": "text", "bbox": [431, 2762, 2017, 2890], "res": [{"text": "where Qx() is the quantile function of the standard normal distribution N(0, 1). A problem for", "confidence": 0.9840484857559204, "text_region": [[422.0, 2756.0], [2022.0, 2752.0], [2022.0, 2808.0], [422.0, 2812.0]]}, {"text": "a symmetric k-bit quantization is that this approach does not have an exact representation of zero,", "confidence": 0.9878356456756592, "text_region": [[429.0, 2805.0], [2019.0, 2805.0], [2019.0, 2851.0], [429.0, 2851.0]]}, {"text": "which is an important property to quantize padding and other zero-valued elements with no error. To", "confidence": 0.9973406791687012, "text_region": [[429.0, 2851.0], [2019.0, 2851.0], [2019.0, 2897.0], [429.0, 2897.0]]}], "img_idx": 0, "score": 0.9673408269882202}
{"type": "text", "bbox": [1970, 2625, 2018, 2658], "res": [{"text": "(4)", "confidence": 0.9781994223594666, "text_region": [[1966.0, 2624.0], [2026.0, 2624.0], [2026.0, 2666.0], [1966.0, 2666.0]]}], "img_idx": 0, "score": 0.9105150103569031}
{"type": "text", "bbox": [427, 916, 2020, 2534], "res": [{"text": "3QLoRA Finetuning", "confidence": 0.998727560043335, "text_region": [[420.0, 854.0], [928.0, 862.0], [927.0, 921.0], [419.0, 914.0]]}, {"text": "QLoRA achieves high-fidelity 4-bit finetuning via two techniques we propose 4-bit NormalFloat", "confidence": 0.9803429245948792, "text_region": [[429.0, 937.0], [2022.0, 937.0], [2022.0, 993.0], [429.0, 993.0]]}, {"text": "(NF4) quantization and Double Quantization. Additionally, we introduce Paged Optimizers, to", "confidence": 0.9926592707633972, "text_region": [[432.0, 987.0], [2022.0, 987.0], [2022.0, 1033.0], [432.0, 1033.0]]}, {"text": " prevent memory spikes during gradient checkpointing from causing out-of-memory errors that have", "confidence": 0.9801874160766602, "text_region": [[422.0, 1026.0], [2026.0, 1023.0], [2026.0, 1079.0], [422.0, 1082.0]]}, {"text": "traditionally made finetuning on a single machine difficult for large models.", "confidence": 0.9954218864440918, "text_region": [[429.0, 1076.0], [1636.0, 1076.0], [1636.0, 1122.0], [429.0, 1122.0]]}, {"text": "QLoRA has one low-precision storage data type, in our case usually 4-bit, and one computation data", "confidence": 0.9961044192314148, "text_region": [[432.0, 1142.0], [2019.0, 1142.0], [2019.0, 1188.0], [432.0, 1188.0]]}, {"text": "type that is usually BFloat16. In practice, this means whenever a QLoRA weight tensor is used, we", "confidence": 0.9892913699150085, "text_region": [[432.0, 1185.0], [2019.0, 1185.0], [2019.0, 1231.0], [432.0, 1231.0]]}, {"text": "dequantize the tensor to BFloat16, and then perform a matrix multiplication in 16-bit.", "confidence": 0.9966852068901062, "text_region": [[426.0, 1228.0], [1796.0, 1224.0], [1796.0, 1270.0], [426.0, 1274.0]]}, {"text": "We now discuss the components of QLoRA followed by a formal definition of QLoRA.", "confidence": 0.9852681159973145, "text_region": [[429.0, 1297.0], [1849.0, 1297.0], [1849.0, 1343.0], [429.0, 1343.0]]}, {"text": "4-bit NormalFloat Quantization The NormalFloat (NF) data type builds on Quantile Quantization", "confidence": 0.9868183135986328, "text_region": [[429.0, 1370.0], [2019.0, 1370.0], [2019.0, 1416.0], [429.0, 1416.0]]}, {"text": "[15] which is an information-theoretically optimal data type that ensures each quantization bin has an", "confidence": 0.9875954389572144, "text_region": [[426.0, 1412.0], [2016.0, 1412.0], [2016.0, 1459.0], [426.0, 1459.0]]}, {"text": " equal number of values assigned from the input tensor. Quantile quantization works by estimating", "confidence": 0.9838642477989197, "text_region": [[422.0, 1449.0], [2022.0, 1452.0], [2022.0, 1508.0], [422.0, 1505.0]]}, {"text": "the quantile of the input tensor through the empirical cumulative distribution function.", "confidence": 0.9956868886947632, "text_region": [[429.0, 1502.0], [1806.0, 1502.0], [1806.0, 1544.0], [429.0, 1544.0]]}, {"text": "The main limitation of quantile quantization is that the process of quantile estimation is expensive.", "confidence": 0.9897398948669434, "text_region": [[426.0, 1561.0], [2022.0, 1561.0], [2022.0, 1617.0], [426.0, 1617.0]]}, {"text": "Therefore fast quantile approximation algorithms, such as SRAM quantiles [15], are used to estimate", "confidence": 0.9848291277885437, "text_region": [[426.0, 1607.0], [2022.0, 1607.0], [2022.0, 1663.0], [426.0, 1663.0]]}, {"text": "them. Due to the approximate nature of these quantile estimation algorithms, the data type has large", "confidence": 0.9778546094894409, "text_region": [[426.0, 1650.0], [2019.0, 1657.0], [2019.0, 1703.0], [426.0, 1696.0]]}, {"text": "quantization errors for outliers, which are often the most important values.", "confidence": 0.9908879995346069, "text_region": [[429.0, 1699.0], [1616.0, 1699.0], [1616.0, 1746.0], [429.0, 1746.0]]}, {"text": "Expensive quantile estimates and approximation errors can be avoided when input tensors come from", "confidence": 0.993899941444397, "text_region": [[426.0, 1766.0], [2019.0, 1766.0], [2019.0, 1812.0], [426.0, 1812.0]]}, {"text": " a distribution fixed up to a quantization constant. In such cases, input tensors have the same quantiles", "confidence": 0.9838019013404846, "text_region": [[422.0, 1805.0], [2019.0, 1808.0], [2019.0, 1855.0], [422.0, 1851.0]]}, {"text": " making exact quantile estimation computationally feasible.", "confidence": 0.9795711040496826, "text_region": [[422.0, 1848.0], [1374.0, 1845.0], [1374.0, 1901.0], [423.0, 1904.0]]}, {"text": "Since pretrained neural network weights usually have a zero-centered normal distribution with", "confidence": 0.992706298828125, "text_region": [[426.0, 1914.0], [2022.0, 1914.0], [2022.0, 1970.0], [426.0, 1970.0]]}, {"text": "standard deviation o (see Appendix F), we can transform all weights to a single fixed distribution by", "confidence": 0.9866480231285095, "text_region": [[429.0, 1963.0], [2019.0, 1963.0], [2019.0, 2010.0], [429.0, 2010.0]]}, {"text": "scaling o such that the distribution fits exactly into the range of our data type. For our data type, we", "confidence": 0.9807450771331787, "text_region": [[422.0, 2000.0], [2022.0, 2003.0], [2022.0, 2059.0], [422.0, 2056.0]]}, {"text": "set the arbitrary range [\u20141, 1]. As such, both the quantiles for the data type and the neural network", "confidence": 0.9936217665672302, "text_region": [[429.0, 2053.0], [2022.0, 2053.0], [2022.0, 2099.0], [429.0, 2099.0]]}, {"text": "weights need to be normalized into this range.", "confidence": 0.9962677955627441, "text_region": [[429.0, 2095.0], [1167.0, 2095.0], [1167.0, 2142.0], [429.0, 2142.0]]}, {"text": "The information theoretically optimal data type for zero-mean normal distributions with arbitrary", "confidence": 0.9904231429100037, "text_region": [[429.0, 2158.0], [2016.0, 2158.0], [2016.0, 2204.0], [429.0, 2204.0]]}, {"text": "standard deviations o in the range [1, 1] is computed as follows: (1) estimate the 2 + 1 quantiles", "confidence": 0.983299970626831, "text_region": [[426.0, 2201.0], [2022.0, 2198.0], [2022.0, 2254.0], [426.0, 2257.0]]}, {"text": "of a theoretical N(o, 1) distribution to obtain a k-bit quantile quantization data type for normal distri", "confidence": 0.9818417429924011, "text_region": [[429.0, 2247.0], [2019.0, 2247.0], [2019.0, 2293.0], [429.0, 2293.0]]}, {"text": "butions, (2) take this data type and normalize its values into the [-1, 1] range, (3) quantize an input", "confidence": 0.9828692078590393, "text_region": [[426.0, 2294.0], [2019.0, 2294.0], [2019.0, 2340.0], [426.0, 2340.0]]}, {"text": "weight tensor by normalizing it into the [-1, 1] range through absolute maximum rescaling.", "confidence": 0.9945016503334045, "text_region": [[429.0, 2336.0], [1893.0, 2336.0], [1893.0, 2383.0], [429.0, 2383.0]]}, {"text": " Once the weight range and data type range match, we can quantize as usual. Step (3) is equivalent to", "confidence": 0.9928085803985596, "text_region": [[422.0, 2396.0], [2022.0, 2399.0], [2022.0, 2455.0], [422.0, 2452.0]]}, {"text": "rescaling the standard deviation of the weight tensor to match the standard deviation of the k-bit data", "confidence": 0.9906447529792786, "text_region": [[429.0, 2449.0], [2019.0, 2449.0], [2019.0, 2491.0], [429.0, 2491.0]]}, {"text": "type. More formally, we estimate the 2k values qi of the data type as follows:", "confidence": 0.9810487627983093, "text_region": [[426.0, 2485.0], [1670.0, 2485.0], [1670.0, 2541.0], [426.0, 2541.0]]}], "img_idx": 0, "score": 0.8911489248275757}
{"type": "title", "bbox": [429, 869, 924, 909], "res": [{"text": "3QLoRA Finetuning", "confidence": 0.998727560043335, "text_region": [[420.0, 854.0], [928.0, 862.0], [927.0, 921.0], [419.0, 914.0]]}], "img_idx": 0, "score": 0.9304971694946289}
{"type": "footer", "bbox": [1212, 2974, 1240, 2998], "res": [{"text": "4", "confidence": 0.9907581806182861, "text_region": [[1203.0, 2978.0], [1231.0, 2967.0], [1243.0, 2996.0], [1215.0, 3007.0]]}], "img_idx": 0, "score": 0.5830690860748291}
{"type": "equation", "bbox": [853, 2594, 1585, 2694], "res": [{"text": "=(x()+x()", "confidence": 0.6781420707702637, "text_region": [[835.0, 2581.0], [1590.0, 2581.0], [1590.0, 2706.0], [835.0, 2706.0]]}], "img_idx": 0, "score": 0.9442166686058044}
