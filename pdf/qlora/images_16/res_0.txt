{"type": "title", "bbox": [434, 294, 654, 329], "res": [{"text": "References", "confidence": 0.9994220733642578, "text_region": [[430.0, 290.0], [662.0, 297.0], [661.0, 344.0], [429.0, 336.0]]}], "img_idx": 0, "score": 0.9168316721916199}
{"type": "footer", "bbox": [1206, 2973, 1243, 3000], "res": [{"text": "17", "confidence": 0.9997876882553101, "text_region": [[1201.0, 2970.0], [1251.0, 2970.0], [1251.0, 3013.0], [1201.0, 3013.0]]}], "img_idx": 0, "score": 0.8874221444129944}
{"type": "reference", "bbox": [439, 361, 2020, 2888], "res": [{"text": "[1] S. An, Y. Li, Z. Lin, Q. Liu, B. Chen, Q. Fu, W. Chen, N. Zheng, and J.-G. Lou. Input-tuning:", "confidence": 0.9961578249931335, "text_region": [[446.0, 363.0], [2019.0, 366.0], [2019.0, 413.0], [446.0, 409.0]]}, {"text": "Adapting unfamiliar inputs to frozen pretrained models. arXiv preprint arXiv:2203.03131,", "confidence": 0.9932134747505188, "text_region": [[522.0, 412.0], [2019.0, 412.0], [2019.0, 459.0], [522.0, 459.0]]}, {"text": "2022.", "confidence": 0.9978551864624023, "text_region": [[519.0, 455.0], [605.0, 455.0], [605.0, 492.0], [519.0, 492.0]]}, {"text": "[2] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann", "confidence": 0.989323079586029, "text_region": [[449.0, 531.0], [2012.0, 531.0], [2012.0, 578.0], [449.0, 578.0]]}, {"text": "arXiv:2112.00861, 2021.", "confidence": 0.9995822310447693, "text_region": [[519.0, 620.0], [918.0, 620.0], [918.0, 663.0], [519.0, 663.0]]}, {"text": "[3] S. H. Bach, V. Sanh, Z.-X. Yong, A. Webson, C. Raffel, N. V. Nayak, A. Sharma, T. Kim, M. S.", "confidence": 0.9854739308357239, "text_region": [[449.0, 696.0], [2019.0, 696.0], [2019.0, 742.0], [449.0, 742.0]]}, {"text": "Bari, T. Fevry, et al. Promptsource: An integrated development environment and repository for", "confidence": 0.9818381667137146, "text_region": [[512.0, 739.0], [2019.0, 743.0], [2019.0, 789.0], [512.0, 785.0]]}, {"text": "natural language prompts. arXiv preprint arXiv:2202.01279, 2022.", "confidence": 0.9925501346588135, "text_region": [[519.0, 789.0], [1583.0, 789.0], [1583.0, 835.0], [519.0, 835.0]]}, {"text": "[4] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli,", "confidence": 0.9927008748054504, "text_region": [[446.0, 861.0], [2019.0, 865.0], [2019.0, 911.0], [446.0, 907.0]]}, {"text": "T. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from", "confidence": 0.9912421107292175, "text_region": [[512.0, 904.0], [2019.0, 908.0], [2019.0, 954.0], [512.0, 950.0]]}, {"text": "human feedback. arXiv preprint arXiv:2204.05862, 2022.", "confidence": 0.9994834661483765, "text_region": [[519.0, 954.0], [1444.0, 954.0], [1444.0, 1000.0], [519.0, 1000.0]]}, {"text": "[5] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho-", "confidence": 0.9941797852516174, "text_region": [[452.0, 1030.0], [2019.0, 1030.0], [2019.0, 1076.0], [452.0, 1076.0]]}, {"text": "seini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint", "confidence": 0.9934026598930359, "text_region": [[519.0, 1076.0], [2019.0, 1076.0], [2019.0, 1122.0], [519.0, 1122.0]]}, {"text": "arXiv:2212.08073, 2022.", "confidence": 0.9900179505348206, "text_region": [[519.0, 1119.0], [918.0, 1119.0], [918.0, 1162.0], [519.0, 1162.0]]}, {"text": "[6] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic", "confidence": 0.9806631207466125, "text_region": [[452.0, 1198.0], [2016.0, 1198.0], [2016.0, 1241.0], [452.0, 1241.0]]}, {"text": "parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on", "confidence": 0.9937828779220581, "text_region": [[516.0, 1244.0], [2019.0, 1244.0], [2019.0, 1287.0], [516.0, 1287.0]]}, {"text": "fairness, accountability, and transparency, pages 610-623, 2021.", "confidence": 0.9918932914733887, "text_region": [[516.0, 1287.0], [1560.0, 1287.0], [1560.0, 1333.0], [516.0, 1333.0]]}, {"text": "[7] S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O'Brien, E. Hallahan, M. A. Khan,", "confidence": 0.9802757501602173, "text_region": [[449.0, 1363.0], [2019.0, 1363.0], [2019.0, 1409.0], [449.0, 1409.0]]}, {"text": "S. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models", "confidence": 0.9860127568244934, "text_region": [[509.0, 1399.0], [2022.0, 1403.0], [2022.0, 1459.0], [509.0, 1455.0]]}, {"text": "across training and scaling. arXiv preprint arXiv:2304.01373, 2023.", "confidence": 0.9988895058631897, "text_region": [[515.0, 1452.0], [1603.0, 1449.0], [1603.0, 1495.0], [516.0, 1498.0]]}, {"text": "[8] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,", "confidence": 0.9843228459358215, "text_region": [[449.0, 1531.0], [2016.0, 1531.0], [2016.0, 1574.0], [449.0, 1574.0]]}, {"text": "J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models", "confidence": 0.9922561049461365, "text_region": [[512.0, 1574.0], [2016.0, 1574.0], [2016.0, 1620.0], [512.0, 1620.0]]}, {"text": "arXiv preprint arXiv:2108.07258, 2021.", "confidence": 0.982772946357727, "text_region": [[515.0, 1620.0], [1157.0, 1614.0], [1158.0, 1656.0], [516.0, 1663.0]]}, {"text": "[9] T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training deep nets with sublinear memory cost.", "confidence": 0.9880678653717041, "text_region": [[452.0, 1696.0], [2019.0, 1696.0], [2019.0, 1742.0], [452.0, 1742.0]]}, {"text": "arXiv preprint arXiv:1604.06174, 2016.", "confidence": 0.9977534413337708, "text_region": [[519.0, 1742.0], [1154.0, 1742.0], [1154.0, 1785.0], [519.0, 1785.0]]}, {"text": "[10] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E.", "confidence": 0.9850276112556458, "text_region": [[429.0, 1818.0], [2019.0, 1818.0], [2019.0, 1864.0], [429.0, 1864.0]]}, {"text": "Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%*", "confidence": 0.9840242266654968, "text_region": [[512.0, 1858.0], [2026.0, 1855.0], [2026.0, 1911.0], [512.0, 1914.0]]}, {"text": "chatgpt quality, March 2023. URL https : //lmsys .org/blog/2023-03-30-vicuna/.", "confidence": 0.9887071251869202, "text_region": [[512.0, 1901.0], [1959.0, 1901.0], [1959.0, 1957.0], [512.0, 1957.0]]}, {"text": "[11] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement", "confidence": 0.9861790537834167, "text_region": [[422.0, 1977.0], [2019.0, 1980.0], [2019.0, 2036.0], [422.0, 2033.0]]}, {"text": "learning from human preferences. Advances in neural information processing systems, 30.", "confidence": 0.9852728247642517, "text_region": [[516.0, 2026.0], [2016.0, 2026.0], [2016.0, 2072.0], [516.0, 2072.0]]}, {"text": "2017.", "confidence": 0.9999414682388306, "text_region": [[519.0, 2076.0], [612.0, 2076.0], [612.0, 2112.0], [519.0, 2112.0]]}, {"text": "[12] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. De-", "confidence": 0.9839411973953247, "text_region": [[432.0, 2152.0], [2016.0, 2152.0], [2016.0, 2198.0], [432.0, 2198.0]]}, {"text": "hghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint", "confidence": 0.9881677031517029, "text_region": [[512.0, 2191.0], [2019.0, 2195.0], [2019.0, 2241.0], [512.0, 2237.0]]}, {"text": "arXiv:2210.11416, 2022.", "confidence": 0.9980436563491821, "text_region": [[512.0, 2238.0], [921.0, 2234.0], [922.0, 2280.0], [512.0, 2284.0]]}, {"text": "[13]  T. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv", "confidence": 0.9735316038131714, "text_region": [[429.0, 2317.0], [2016.0, 2317.0], [2016.0, 2363.0], [429.0, 2363.0]]}, {"text": " preprint arXiv:2212.09720, 2022.", "confidence": 0.9781400561332703, "text_region": [[505.0, 2360.0], [1061.0, 2353.0], [1061.0, 2409.0], [506.0, 2416.0]]}, {"text": "[14] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. LLM.int8O: 8-bit matrix multiplication", "confidence": 0.9675959944725037, "text_region": [[429.0, 2439.0], [2016.0, 2439.0], [2016.0, 2485.0], [429.0, 2485.0]]}, {"text": "for transformers at scale. Advances in Neural Information Processing Systems 35: Annual", "confidence": 0.9994513988494873, "text_region": [[519.0, 2485.0], [2019.0, 2485.0], [2019.0, 2528.0], [519.0, 2528.0]]}, {"text": "Conference on Neural Information Processing Systems 2022, NeurIPS 2022, 2022.", "confidence": 0.9957782626152039, "text_region": [[515.0, 2528.0], [1833.0, 2524.0], [1833.0, 2571.0], [516.0, 2574.0]]}, {"text": "[15] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer. 8-bit optimizers via block-wise", "confidence": 0.9806143641471863, "text_region": [[429.0, 2607.0], [2019.0, 2607.0], [2019.0, 2650.0], [429.0, 2650.0]]}, {"text": "quantization. 9th International Conference on Learning Representations, ICLR, 2022.", "confidence": 0.9961029291152954, "text_region": [[516.0, 2650.0], [1886.0, 2650.0], [1886.0, 2696.0], [516.0, 2696.0]]}, {"text": "[16]  A. E. Elo. The proposed uscf rating system. its development, theory, and applications. Chess", "confidence": 0.9899193644523621, "text_region": [[422.0, 2722.0], [2019.0, 2726.0], [2019.0, 2782.0], [422.0, 2779.0]]}, {"text": "Life, 22(8):242-247, 1967.", "confidence": 0.9909986853599548, "text_region": [[519.0, 2775.0], [948.0, 2775.0], [948.0, 2818.0], [519.0, 2818.0]]}, {"text": "[17] A. E. Elo. The rating of chessplayers, past and present. Arco Pub., 1978.", "confidence": 0.9867091774940491, "text_region": [[429.0, 2851.0], [1686.0, 2848.0], [1686.0, 2894.0], [429.0, 2897.0]]}], "img_idx": 0, "score": 0.985550045967102}
