{"type": "text", "bbox": [429, 1523, 2014, 1739], "res": [{"text": "The memory footpring for QLoRA training with different LLaMA base models can be seen in", "confidence": 0.9872241616249084, "text_region": [[426.0, 1521.0], [2022.0, 1521.0], [2022.0, 1568.0], [426.0, 1568.0]]}, {"text": "Figure 6. We see that the 33B model does not quite fit into a 24 GB and that paged optimizers", "confidence": 0.9898719191551208, "text_region": [[426.0, 1564.0], [2022.0, 1564.0], [2022.0, 1610.0], [426.0, 1610.0]]}, {"text": " are needed to train it. Depicted is also batch size 1 with a sequence length of 512 and gradient", "confidence": 0.9910162091255188, "text_region": [[422.0, 1610.0], [2022.0, 1607.0], [2022.0, 1653.0], [422.0, 1657.0]]}, {"text": "checkpointning. This means, if one uses a larger batch size, or if a long sequence is processed, the", "confidence": 0.9934120178222656, "text_region": [[426.0, 1653.0], [2022.0, 1653.0], [2022.0, 1699.0], [426.0, 1699.0]]}, {"text": " activation gradient might consume a considerable amount of memory.", "confidence": 0.9928174614906311, "text_region": [[419.0, 1690.0], [1550.0, 1693.0], [1550.0, 1749.0], [419.0, 1746.0]]}], "img_idx": 0, "score": 0.9764176607131958}
{"type": "title", "bbox": [431, 1450, 900, 1492], "res": [{"text": "G", "confidence": 0.9989758729934692, "text_region": [[432.0, 1449.0], [482.0, 1449.0], [482.0, 1488.0], [432.0, 1488.0]]}, {"text": "Memory Footprint", "confidence": 0.9801763296127319, "text_region": [[509.0, 1449.0], [905.0, 1449.0], [905.0, 1495.0], [509.0, 1495.0]]}], "img_idx": 0, "score": 0.9630058407783508}
{"type": "figure", "bbox": [645, 370, 1825, 1144], "res": [{"text": "100%", "confidence": 0.999901294708252, "text_region": [[688.0, 383.0], [752.0, 383.0], [752.0, 422.0], [688.0, 422.0]]}, {"text": "1152", "confidence": 0.9998487830162048, "text_region": [[888.0, 416.0], [951.0, 416.0], [951.0, 452.0], [888.0, 452.0]]}, {"text": "3510", "confidence": 0.9996716976165771, "text_region": [[1337.0, 409.0], [1414.0, 409.0], [1414.0, 449.0], [1337.0, 449.0]]}, {"text": "5760", "confidence": 0.9998683929443359, "text_region": [[1563.0, 409.0], [1636.0, 409.0], [1636.0, 449.0], [1563.0, 449.0]]}, {"text": "1800", "confidence": 0.9979773759841919, "text_region": [[1121.0, 419.0], [1174.0, 419.0], [1174.0, 446.0], [1121.0, 446.0]]}, {"text": "1440", "confidence": 0.9990007281303406, "text_region": [[1567.0, 449.0], [1640.0, 449.0], [1640.0, 485.0], [1567.0, 485.0]]}, {"text": "877.5", "confidence": 0.9991105794906616, "text_region": [[1337.0, 459.0], [1417.0, 459.0], [1417.0, 498.0], [1337.0, 498.0]]}, {"text": "450", "confidence": 0.9990580677986145, "text_region": [[1124.0, 485.0], [1174.0, 485.0], [1174.0, 508.0], [1124.0, 508.0]]}, {"text": "288", "confidence": 0.9990032315254211, "text_region": [[898.0, 495.0], [938.0, 495.0], [938.0, 521.0], [898.0, 521.0]]}, {"text": "37074", "confidence": 0.9998787045478821, "text_region": [[1563.0, 528.0], [1643.0, 528.0], [1643.0, 568.0], [1563.0, 568.0]]}, {"text": "75%", "confidence": 0.9997817873954773, "text_region": [[698.0, 541.0], [758.0, 541.0], [758.0, 581.0], [698.0, 581.0]]}, {"text": "19302", "confidence": 0.9992400407791138, "text_region": [[1344.0, 551.0], [1414.0, 551.0], [1414.0, 578.0], [1344.0, 578.0]]}, {"text": "8476", "confidence": 0.9999450445175171, "text_region": [[1108.0, 564.0], [1181.0, 564.0], [1181.0, 604.0], [1108.0, 604.0]]}, {"text": "5046", "confidence": 0.9999525547027588, "text_region": [[885.0, 574.0], [951.0, 574.0], [951.0, 610.0], [885.0, 610.0]]}, {"text": "50%", "confidence": 0.9455330967903137, "text_region": [[702.0, 700.0], [758.0, 700.0], [758.0, 736.0], [702.0, 736.0]]}, {"text": "25%", "confidence": 0.999960720539093, "text_region": [[698.0, 855.0], [762.0, 855.0], [762.0, 894.0], [698.0, 894.0]]}, {"text": "0%", "confidence": 0.9994207620620728, "text_region": [[715.0, 1016.0], [765.0, 1016.0], [765.0, 1049.0], [715.0, 1049.0]]}, {"text": "7B (6.9 GB)", "confidence": 0.994822084903717, "text_region": [[855.0, 1043.0], [988.0, 1043.0], [988.0, 1076.0], [855.0, 1076.0]]}, {"text": "13B (11.3 GB)", "confidence": 0.9656153321266174, "text_region": [[1071.0, 1043.0], [1227.0, 1043.0], [1227.0, 1076.0], [1071.0, 1076.0]]}, {"text": "33B (24.7 GB)", "confidence": 0.9607563018798828, "text_region": [[1294.0, 1043.0], [1454.0, 1043.0], [1454.0, 1076.0], [1294.0, 1076.0]]}, {"text": "65B (45.0 GB)", "confidence": 0.9980204701423645, "text_region": [[1523.0, 1043.0], [1680.0, 1043.0], [1680.0, 1076.0], [1523.0, 1076.0]]}, {"text": " LLaMA model size", "confidence": 0.9762998223304749, "text_region": [[1152.0, 1092.0], [1364.0, 1099.0], [1363.0, 1146.0], [1150.0, 1138.0]]}], "img_idx": 0, "score": 0.9345539808273315}
{"type": "figure_caption", "bbox": [428, 1205, 2018, 1359], "res": [{"text": "Figure 6: Breakdown of the memory footprint of different LLaMA models. The input gradient size is for batch", "confidence": 0.996568500995636, "text_region": [[426.0, 1201.0], [2022.0, 1201.0], [2022.0, 1247.0], [426.0, 1247.0]]}, {"text": "size 1 and sequence length 512 and is estimated only for adapters and the base model weights (no attention).", "confidence": 0.9920043349266052, "text_region": [[426.0, 1238.0], [2022.0, 1238.0], [2022.0, 1284.0], [426.0, 1284.0]]}, {"text": "Numbers on the bars are memory footprint in MB of individual elements of the total footprint. While some", "confidence": 0.9902427196502686, "text_region": [[426.0, 1280.0], [2026.0, 1280.0], [2026.0, 1327.0], [426.0, 1327.0]]}, {"text": " models do not quite ft on certain GPUs, paged optimzier provide enough memory to make these models fit.", "confidence": 0.9871960878372192, "text_region": [[422.0, 1317.0], [1986.0, 1317.0], [1986.0, 1373.0], [422.0, 1373.0]]}], "img_idx": 0, "score": 0.9117797613143921}
{"type": "table", "bbox": [850, 2488, 1598, 2893], "res": {"cell_bbox": [[14.042860984802246, 2.050645351409912, 730.7105712890625, 2.2397525310516357, 730.3710327148438, 61.622249603271484, 13.719698905944824, 59.07718276977539], [16.916563034057617, 50.79886245727539, 743.060302734375, 52.8495979309082, 742.9300537109375, 349.1816101074219, 16.265869140625, 347.4739074707031]], "html": "<html><body><table><tbody><tr><td>Model Params Size</td></tr><tr><td>Guanaco 65B 41 GB Guanaco 33B 21 GB Vicuna 13B 26 GB ChatGPT-3.5 Turbo N/A N/A Bard N/A N/A Guanaco 13B 10 GB Guanaco 7B 5 GB</td></tr></tbody></table></body></html>"}, "img_idx": 0, "score": 0.9556764364242554}
{"type": "table_caption", "bbox": [579, 2448, 1868, 2478], "res": [{"text": "Table 13: The complete ordering induced by pairwise GPT-4 judgments between systems ", "confidence": 0.9878033399581909, "text_region": [[572.0, 2442.0], [1876.0, 2442.0], [1876.0, 2485.0], [572.0, 2485.0]]}], "img_idx": 0, "score": 0.8519898653030396}
{"type": "footer", "bbox": [1201, 2973, 1243, 3001], "res": [{"text": "26", "confidence": 0.9999585151672363, "text_region": [[1201.0, 2967.0], [1254.0, 2967.0], [1254.0, 3010.0], [1201.0, 3010.0]]}], "img_idx": 0, "score": 0.9086973071098328}
