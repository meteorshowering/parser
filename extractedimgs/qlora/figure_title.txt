Figure 6: Breakdown of the memory footprint of different LLaMA models. The input gradient size is for batchsize 1 and sequence length 512 and is estimated only for adapters and the base model weights (no attention).Numbers on the bars are memory footprint in MB of individual elements of the total footprint. While some models do not quite ft on certain GPUs, paged optimzier provide enough memory to make these models fit.
Figure 1: Different finetuning methods and their memory requirements. QLoRA improves over LoRA byquantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes.
Figure 4: LoRA r for LLaMA 7B models finetuned on Alpaca. Each dot represents a combination ofhyperparameters and for each LoRA r we run 3 random seed with each hyperparameter combination. Theperformance of specific LoRA r values appears to be independent of other hyperparameters.
Figure 5: The crowdsourcing form used by human annotators.
Figure 6: Breakdown of the memory footprint of different LLaMA models. The input gradient size is for batchsize 1 and sequence length 512 and is estimated only for adapters and the base model weights (no attention).Numbers on the bars are memory footprint in MB of individual elements of the total footprint. While some models do not quite ft on certain GPUs, paged optimzier provide enough memory to make these models fit.
